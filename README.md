# Sales-Forecasting-Project

 --- 
 
1. Create Master Weather + Store Subcluster + Holiday + Sales.ipynb
- Data cleaning: selecting and renaming columns, ensuring that columns have the correct format and dtypes, filtering out outliers and data not of interest
- Merging spreadsheets to create the target dataset to apply Machine Learning

---

2. Create Public Holiday Final.ipynb
- Loading downloaded html file from website
- Data cleaning: selecting columns, building dataframes (using Pandas), manipulating column names
- Merging data from different years to form final public holiday data

---

3. Daily Sales Supervised ML with Dupe Dates.ipynb (analysing how public holiday, store subcluster, weather and historical sales data influence sales with multiple supervised ML models)
- Loading dataset created from 1.
- Data cleaning: labelling categories and applying labelencoder(), ensuring data in specific columns have the correct dtypes, spliting columns into "features" and "target"
- Implemented supervised ML models (Polynomial regression, XGBoost Regressor, LightGBM, Feed Forward Neural Network)
*used train_test_split() on dataset, fit the models and compared model predictions to testing data -> analysing evaluation metrics, implementing GridSearchCV for hyperparameter tuning -> showing how much it improved the model, plot feature importance table and used it to eliminate zero-importance columns, built an Embedding and Dense Model, implemented regularisation techniques such as ReduceLROnPlateau() and EarlyStopping(), Plotted a Loss Curve Plot that compares training with validation loss

---

4. DAILY Sales Weather Project.ipynb - Calculate daily correlation between sales and weather with a few ML models
- Data cleaning: filtering outliers, manipulating store data, implementing SMOTE (to address 13:1 disproportional data), merging dataframes
- Created a correlation chart with a conclusion
- Experimented with ML models such as XGBoost and a first attempt of a Multi Layer Perceptrons Neural Network (which was clearly underfitting - led me to believe that weather was insufficient to predict sales data)

---

5. Data for LSTM Model FINAL.ipynb - Jupyter notebook to create the data for the LSTM model
- ğŸ”µ Subcluster data: "Master Store Subcluster" in Lakehouse -> AI_Sandbox -> LH_External_Data -> tables
- ğŸ”µ Daily Sales data: "Daily Sales - Store & VM" in Lakehouse -> AI_Sandbox -> LH_External_Data -> tables
- ğŸŸ¢ Weather data: can scrape using "Scrape Weather FINAL.ipynb" in this repo
- ğŸ”µ ğŸ”´ Public Holiday data: "PH for non-PH" in Microsoft Teams Planner "FM OPS BI" under "Sales Forecasting"
- Need to set where does the code keep output file "Data for LSTM model.csv".

---

6. HOURLY Sales Weather Project FINAL.ipynb - Jupyter notebook to calculate daily correlation between sales and weather with a few ML models
- ğŸ”µ Store data: "Master Store" in Lakehouse -> AI_Sandbox -> LH_External_Data -> tables
- ğŸ”µ Hourly Sales data: (I filtered out only May 2025 data from "Hourly Sales - Store & VM" in Lakehouse -> AI_Sandbox -> LH_External_Data -> tables)
- ğŸŸ¢ Weather data: can scrape using "Scrape Weather FINAL.ipynb" in this repo

---

7. LSTM Final Model.ipynb - Jupyter notebook to create LSTM and make forecast more about it below.
- ğŸŸ¢ df: can create using 5. "Data for LSTM Model FINAL.ipynb" in this repo OR ğŸ”µ "Data for LSTM model.csv" in Microsoft Teams Planner "FM OPS BI" under "Sales Forecasting"

---

8. LSTM-tested-features - all of the features that I have tested that did not make the final model
- ğŸŸ¢ df: can create using "5. Data for LSTM Model FINAL.ipynb" in this repo OR ğŸ”µ "Data for LSTM model.csv" in Microsoft Teams Planner "FM OPS BI" under "Sales Forecasting"
- Need to set where does the code keep output file "Sales_Forecasting_LSTM_Model.h5".

---

9. Sales_Forecasting_LSTM_Model.h5 - model that is created when running model in "LSTM Final Model.ipynb"

---

10. Scrape Weather FINAL.ipynb - Jupyter notebook to scrape weather data from the internet
Do not need any data files, but need to set directory - where the code gets "Master Store.csv" file and where the code keeps final "Weather Data Updated.csv" file.

---

11. Time series.ipynb - Jupyter notebook to categories Public Holiday into categories
- ğŸ”µ "Master Weather + Store Subcluster + Holiday + Sales" csv file in Lakehouse -> AI_Sandbox -> LH_External_Data -> files
- ğŸ”µ Daily Sales data: "Daily Sales - Store & VM" in Lakehouse -> AI_Sandbox -> LH_External_Data -> tables

---

12. Running LSTM without retraining model.ipynb - Jupyter notebook to run my current model without retraining another LSTM model (to make predictions/forecasting)
- ğŸ”µ Under 9. "Sales_Forecasting_LSTM_Model.h5" in this repo
- ğŸŸ¢ df: can create using 5. "Data for LSTM Model FINAL.ipynb" in this repo OR ğŸ”µ "Data for LSTM model.csv" in Microsoft Teams Planner "FM OPS BI" under "Sales Forecasting"

---

# 4 Main Projects:

1. [ğŸ“ˆ Sales Forecasting with Multi-Input LSTM](#-Sales-Forecasting-Multi-Input-LSTM)
2. [Daily Sales Supervised ML Studies](#Daily-Sales-Supervised-ML-Studies)
3. [Daily Sales and Weather Studies](#Daily-Sales-and-Weather-Studies)
4. [Hourly Sales and Weather Studies](#Hourly-Sales-and-Weather-Studies)

---

# ğŸ“ˆ Sales Forecasting with Multiâ€‘Input LSTM

> **Accurate 14â€‘day window sales & transaction forecasts for FamilyMart Stores, powered by TensorFlow/Keras, categorical embeddings & residual static context.**

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#lstm-features)
2. [Data](#lstm-data)
3. [Environment/Setup](#lstm-env-setup)
4. [Reproducing the Results](#lstm-reproducing)
5. [Training](#lstm-training)
6. [Evaluation & Metrics](#lstm-eval)
7. [Forecasting & Visualisation](#lstm-forecasting)
8. [Project Structure](#lstm-structure)

---

<a name="lstm-features"></a>

## Features

* **Hybrid input:** numeric weather & sales timeâ€‘series, binary flags **and** categorical embeddings (store no, state and subcluster codes).
* **Residual static context:** 2â€‘layer MLP with skipâ€‘connection broadcasts static store attributes into every LSTM timestep.
* **Timeâ€‘aware split per store:** avoids train/val/test leakage across dates.
* **Permutation feature importance** for both *NetÂ Amount* and *Transaction Count* targets.
* **200â€‘step autoregressive forecasting** & optional 21â€‘day perâ€‘store interactive dashboard (Plotly + ipywidgets).
* **EarlyStopping** + **ReduceLROnPlateau** for robust convergence.
* **Reproducible:** single script / notebook endâ€‘toâ€‘end, explicit random seed control.

<a name="lstm-data"></a>

## Data

* **Source CSV**: `Data for LSTM model.csv`

| Column type                  | Example columns                                                            | Notes                                     |
| ---------------------------- | -------------------------------------------------------------------------- | ----------------------------------------- |
| **Timeâ€‘varying categorical** | `Name`, `Day`, `Month`                                                     | Encoded & *embedded* if >â€¯6 unique values |
| **Static categorical**       | `Store_No`, `State`, `SubCluster`                                          | Embedded, then tiled across timesteps     |
| **Binary flags**             | `Rain?`, `Puasa`, `PublicÂ Holiday`                                         | Mapped to 0/1                             |
| **Numeric**                  | `Net_Amount`, `TC`, `Days_after_Opening`, `AverageÂ DailyÂ Temperatureâ€¯(Â°C)` | Minâ€‘Max scaled 0â€‘1                        |

---

<a name="lstm-env-setup"></a>

## Environment &Â Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

```
pandas>=1.5
numpy>=1.23
matplotlib>=3.7     
scikit-learn>=1.3
tensorflow==2.15.0   
keras-tuner>=1.4.2
plotly>=5.18
ipywidgets>=8.1

```

</details>

---

<a name="lstm-reproducing"></a>

## Reproducing the Results

**Match software versions** â€“ see `requirements.txt`.
**Make sure that the data file is correct**.

---

<a name="lstm-training"></a>

## Training

Key hyperâ€‘parameters:

| Flag             | Default | Description                              |
| ---------------- | ------- | ---------------------------------------- |
| `--window`       | 14      | Lookâ€‘back window length (days)           |
| `--lstm_units`   | 64      | Hidden size of LSTM layer                |
| `--static_dense` | 128     | Width of residual MLP on static features |
| `--dropout`      | 0.25    | Dropout rate everywhere                  |
| `--lr`           | 1eâ€‘3    | Initial learning rate                    |

---

<a name="lstm-eval"></a>

## EvaluationÂ &Â Metrics

After training, the script prints:

* **MAE / RMSE /Â RÂ²** for *Net\_Amount* and *TC*,
  plus intuitive qualitative bands (ğŸ”µÂ Excellent | ğŸŸ¢Â Good | ğŸŸ¡Â Okay | ğŸ”´Â Poor).
* PNG plots: loss curves, parity scatter, firstâ€‘200â€‘window overlay.

---

<a name="lstm-forecasting"></a>

## ForecastingÂ &Â Visualisation

* **200â€‘step autoregressive demo**
* **21â€‘day perâ€‘store dashboard** with Plotly + dropdown widget
* Optional **95â€¯% CI** ribbons (constant Ïƒ derived from residuals)

---

<a name="lstm-structure"></a>

## Project Structure

```
.
â”œâ”€â”€ data/                     # to obtain data for LSTM
â”‚   â””â”€â”€ Data for LSTM Model FINAL.ipynb
â”œâ”€â”€ models/                   # trained model (.h5) saved after training
â”‚   â””â”€â”€ Sales_Forecasting_LSTM_Model.h5
â”œâ”€â”€ notebooks/               
â”‚   â”œâ”€â”€ LSTM Final Model.ipynb    
â”œâ”€â”€ README.md                 
.

```

# Daily Sales Supervised ML studies

> Multimodel experiments (PolynomialÂ Regression, XGBoost, LightGBM & Feedâ€‘ForwardÂ NN) to predict **Daily NetÂ Amount (RM)** and **Transaction Count (TC)** from store data, publicâ€‘holiday data and local weather data.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Data Preparation](#ml-data-preparation)
2. [Baseline: Polynomial Regression](#ml-baseline)
3. [Tree Boosting](#ml-tree-boosting)

   * 3.1 [XGBoost](#ml-xgboost)
   * 3.2 [LightGBM â€“ four drafts](#ml-lightgbm)
4. [Feed-Forward Neural Network](#ml-ffnn)
5. [Training & Hyper-parameters](#ml-training)
6. [Evaluation Metrics](#ml-eval)
7. [Environment / Setup](#ml-env)
8. [Reproducing the Results](#ml-reproducing)
9. [Project Structure](#ml-structure)

---

<a name="ml-data-preparation"></a>

## DataÂ Preparation

* **Source CSV**: `MasterÂ WeatherÂ +Â StoreÂ SubclusterÂ +Â HolidayÂ +Â Sales.csv`

* **Categoricals** (labelâ€‘encoded):

  | Column                       | Description                                       |
  | ---------------------------- | ------------------------------------------------- |
  | `Store_No`                   | Unique store number                               |
  | `Name`                       | Holiday name                                      |
  | `State`                      | Malaysian state                                   |
  | `Day`                        | Dayâ€‘ofâ€‘week string (Monâ€¦Sun)                      |
  | `CODEÂ (subclusterÂ 1)`        | Store subcluster 1                                |
  | `CODEÂ FY26Â 1Â (subclusterÂ 2)` | Store subcluster 2                                |
  | `CODEÂ FY26Â 2Â (subclusterÂ 3)` | Store subcluster 3                                |
  | `Rain?`                      | 1Â if rainfall at any point during the day, elseÂ 0 |
  | `PublicÂ Holiday`             | 1Â if Public Holiday in that state, elseÂ 0         |

* **Numericals** (standardâ€‘scaled where noted):

  | Column                           | Units | Notes                            |
  | -------------------------------- | ----- | -------------------------------- |
  | `Days_after_Opening`             | days  | Days a store has been opened for |
  | `AverageÂ DailyÂ TemperatureÂ (Â°C)` | Â°C    | Different for all store location |
  | `DaysÂ FromÂ Holiday`              | days  | âˆ’veÂ =Â days *before* PH           |
  | `PuasaÂ Count`                    | days  | Day number during Ramadan        |

* **Targets** (not scaled in tree models):

  * `Net_Amount`Â (RM)
  * `TC` â€“ Transaction count

---

<a name="ml-baseline"></a>

## Baseline: PolynomialÂ Regression

Simple degreeâ€‘2 polynomial on standardâ€‘scaled numericals (categoricals left as integer ids). Wrapped in `MultiOutputRegressor` to predict both targets simultaneously.

---

<a name="ml-tree-boosting"></a>

## TreeÂ Boosting

### <a name="ml-xgboost"></a>XGBoost

* Objective: `reg:squarederror`
* 3â€‘fold GridSearch over `max_depth`, `learning_rate`, `n_estimators`, `subsample`, `colsample_bytree`.
* Separate models for **Net\_Amount** and **TC**.

### <a name="ml-lightgbm"></a>LightGBM

Four consecutive drafts investigated featureâ€‘drops & categorical handling:

1. **DraftÂ 1** â€“ full feature set
2. **DraftÂ 2** â€“ converts seven highâ€‘cardinality columns to categorical dtypes
3. **DraftÂ 3** â€“ drops `Store_No`
4. **DraftÂ 4** â€“ additionally drops `Rain?`, `DaysÂ FromÂ Holiday`, `PuasaÂ Count`

Every draft repeats a gridâ€‘search (3â€‘fold) on common hyperâ€‘parameters: `num_leaves`, `max_depth`, `learning_rate`, `n_estimators`, `reg_lambda`, `min_child_samples`.

---

<a name="ml-ffnn"></a>

## Feedâ€‘ForwardÂ NeuralÂ Network

* **Architecture**: shared embeddingâ€‘tower for 9 categoricals + numeric branch â†’ dense trunk â†’ two taskâ€‘specific heads.
* **Embeddings**: size `â£logâ‚‚(cardinality)â¦Â +Â 1`.
* **HiddenÂ layers**: 128â€‘64â€‘32 with `ReLU`, `BatchNorm`, `DropoutÂ 0.3`.
* **Loss**: MSE (separate for each head); **Optimiser**: AdamÂ (lrÂ 5â€¯Ã—â€¯10â»â´).
* **Callbacks**: EarlyStoppingÂ (patienceÂ =Â 3) & ReduceLROnPlateau.
* Training for *max*Â 50Â epochs, batchâ€‘sizeÂ 32.

---

<a name="ml-training"></a>

## TrainingÂ &Â Hyperâ€‘parameters

| Flag / Setting      | Default   | Notes                        |
| ------------------- | --------- | ---------------------------- |
| `test_size`         | 0.20      | `train_test_split` seedÂ =Â 42 |
| `poly_degree`       | 2         | Baseline model               |
| `ffnn_epochs`       | 50        | Earlyâ€‘Stopping \~10â€“15       |
| `ffnn_batch_size`   | 32        |                              |
| `xgb_n_estimators`  | 100â€‘200   | gridâ€‘search                  |
| `lgb_learning_rate` | 0.05â€“0.10 | gridâ€‘search                  |

---

<a name="ml-eval"></a>

## EvaluationÂ &Â Metrics

For each target the notebooks print:

* **RMSE, MAE, RÂ²**
* Ratios vsÂ population `mean` (MAE) and `std` (RMSE) â†’ coloured qualitative bands:

  * ğŸ”µÂ Excellentâ€ƒğŸŸ¢Â Goodâ€ƒğŸŸ¡Â Okayâ€ƒğŸ”´Â Poor

---

<a name="ml-env"></a>

## EnvironmentÂ /Â Setup

<details><summary>requirements.txt</summary>

```
pandas>=1.5
numpy>=1.23
scikit-learn>=1.3
matplotlib>=3.7
seaborn>=0.13
xgboost>=2.0
lightgbm>=4.2

```

</details>

---

<a name="ml-reproducing"></a>

## ReproducingÂ theÂ Results

**Match software versions** â€“ see `requirements.txt`.
**Make sure that the data file is correct**.

---

<a name="ml-structure"></a>

## ProjectÂ Structure

```
.
â”œâ”€â”€ data/                   
â”‚   â””â”€â”€ Create Master Weather + Store Subcluster + Holiday + Sales.ipynb
â”œâ”€â”€ notebooks/               
â”‚   â”œâ”€â”€ Daily Sales Supervised ML with Dupe Dates.ipynb  
â”œâ”€â”€ README.md                 
.
```

# DailyÂ SalesÂ &Â WeatherÂ Studies

> Exploring how **local weather (rain & temperature)** influences storeâ€‘level daily FamilyMart sales in MayÂ 2025, then benchmarking several regressors (XGBoost & Neural Networks) to predict NetÂ Amount (RM) and Transaction Count (TC).

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#dailywx-features)
2. [Data](#dailywx-data)
3. [Environment & Dependencies](#dailywx-env)
4. [Reproducing the Results](#dailywx-reproducing)
5. [Training](#dailywx-training)
6. [Evaluation & Metrics](#dailywx-eval)
7. [Project Structure](#dailywx-structure)

---

<a name="dailywx-features"></a>
## Features

* **Step-by-step method for pulling in raw data, cleaning and organizing it, and getting it ready for analysis**Â â€“ cleans masterâ€‘store, hourly sales & weather CSVs and filters only 24â€‘hour outlets which were open during MayÂ 2025.
* **Daily aggregation**Â â€“ collapses 0â€‘23â€¯h to Total_Daily_Net_Amount (RM) & Total_Daily_TC (transactions).
* **Rain detection**Â â€“ maps weatherâ€‘code sequences â†’ binary Rain? (Yes/No) and **balances classes** with SMOTE for fair modelling.
* **Exploratory stats**Â â€“ Pearson correlation and oneâ€‘hot rain vs sales analysis.
* **Modelling suite**

  * *XGBoost* baselines & 5â€‘fold GridSearch tuning for both targets.
  * *MLP (scikitâ€‘learn)* baseline + gridâ€‘search.
  * *TensorFlow MLP* with earlyâ€‘stopping & LRâ€‘scheduling.
* **Plots & artefacts**Â â€“ featureâ€‘importance bar plots, loss curves.

---

<a name="dailywx-data"></a>
## Data

Source CSV:
   * MasterÂ Store.csv
   * SalesÂ AllÂ StoresÂ MayÂ 2025.csv
   * WeatherÂ DataÂ Updated.csv

| ColumnÂ Type             | Example columns                            | Notes                                               |
| ----------------------- | ------------------------------------------ | --------------------------------------------------- |
| **Identifier**          | Store_No                                   | numeric id, only 24â€¯h stores kept                   |
| **Temporal**            | Date, Hour                                 | UTCâ†’SGT conversion for weather, later aggregated    |
| **Targets**             | Total_Daily_Net_Amount, Total_Daily_TC     | Net amount negated (â€“ve to +ve) then summed perÂ day |
| **Weather numeric**     | AverageÂ DailyÂ TemperatureÂ (Â°C)             | mean of hourly temps                                |
| **Weather categorical** | Rain?                                      | derived Yes/No flag using WMO codes                 |

---

<a name="dailywx-env"></a>
## Environment & Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

pandas>=1.5
numpy>=1.23
matplotlib>=3.7
seaborn>=0.13
scikit-learn>=1.3
statsmodels>=0.14
meteostat>=1.7


</details>

*Tested on WindowsÂ 10 (PythonÂ 3.9, 32â€¯GBÂ RAM)*

---

<a name="dailywx-reproducing"></a>
## Reproducing the Results

1. pip install -r requirements.txt.
2. Load raw CSVs into data/ with exact filenames

---

<a name="dailywx-training"></a>
## Training

Key hyperâ€‘parameters examined:

| Flag / Param                     | Default     | Model      | Description               |
| -------------------------------- | ----------- | ---------- | ------------------------- |
| n_estimators                     | 100Â /Â 300   | XGBoost    | boosting rounds           |
| learning_rate                    | 0.1         | XGBoost    | Î· shrinkage               |
| max_depth                        | 3Â /Â 5Â /Â 7   | XGBoost    | tree depth                |
| subsample / colsample_bytree     | 0.8â€“1.0     | XGBoost    | row / col sampling        |
| hidden_layer_sizes               | (128,64,32) | scikitâ€‘MLP | gridâ€‘searched alt (64,32) |
| alpha                            | 1eâ€‘4        | scikitâ€‘MLP | L2 regulariser            |
| epochs                           | 100         | TFÂ MLP     | earlyâ€‘stops \~20          |
| batch_size                       | 32          | TFÂ MLP     |                           |

---

<a name="dailywx-eval"></a>
## Evaluation & Metrics

Each script/notebook prints for **Net Amount & TC**:

* Mean Absolute Error (MAE)
* Root Mean Square Error (RMSE)
* Coefficient of Determination (RÂ²)
  
---

<a name="dailywx-structure"></a>
## Project Structure 

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ MasterÂ Store.csv
â”‚   â”œâ”€â”€ Salesâ€¯Allâ€¯Storesâ€¯Mayâ€¯2025.csv
â”‚   â””â”€â”€ WeatherÂ DataÂ Updated.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ DAILY Sales Weather Project.ipynb
â””â”€â”€ README.md

```

# Hourly Sales & Weather Studies

> Investigating how temperature and detailed weather codes influence **hourâ€‘byâ€‘hour** Net Amount (RM) and transaction counts (TC) across different FamilyMart stores, then benchmarking linear models and XGBoost.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#hourlywx-features)
2. [Data](#hourlywx-data)
3. [Environment & Dependencies](#hourlywx-env)
4. [Reproducing the Results](#hourlywx-reproducing)
5. [Training](#hourlywx-training)
6. [Evaluation & Metrics](#hourlywx-eval)
7. [Project Structure](#hourlywx-structure)

---

<a name="hourlywx-features"></a>
## Features

* **Step-by-step method for pulling in raw data, cleaning and organizing it, and getting it ready for analysis**

  * Parse masterâ€‘store file, derive Start_h/End_h from Operating_Hours.
  * Filter out closed, notâ€‘openâ€‘yet or nameless stores.
  * Expand each store into every trading hour and map hourly sales rows.
  * Convert UTC weather timestamps â†’ local SG/MY time; extract hour & join on (Date, Hour, Store_No).
* **Exploratory statistics** â€“ Pearson r for sales vs temperature & weather codes; oneâ€‘hot correlation table.
* **Modelling suite**

  * Baseline **Linear / Ridge / Lasso / ElasticNet** models (with optional TempÃ—Hour interaction).
  * **XGBoost** draft & 5â€‘fold GridSearch tuning with featureâ€‘importance plots.
* **Plots & artefacts** â€“ correlation tables, featureâ€‘importance bar charts, trainâ€‘vsâ€‘test RMSE comparison.

---

<a name="hourlywx-data"></a>
## Data

Source CSVs:
   * Master Store.csv
   * Sales All Stores May 2025.csv
   * Weather Data Updated.csv

| Column Type             | Example columns                       | Notes                                    |
| ----------------------- | ------------------------------------- | ---------------------------------------- |
| **Identifiers**         | Store_No, Date, Hour            | Hourly granularity, Date in YYYYâ€‘MMâ€‘DD |
| **Sales targets**       | Net_Amount, TC                    | Hourly; net values negated (â€“ve to +ve)  |
| **Weather numeric**     | Temperature (Â°C)                    | Derived from Temperature (Â°C) in raw   |
| **Weather categorical** | Weather_Code, Weather_Description | WMO code (int) & human string            |
| **Store metadata**      | Start_h, End_h                    | For operatingâ€‘window filtering           |

---

<a name="hourlywx-env"></a>
## Environment & Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

pandas>=1.5
numpy>=1.23
matplotlib>=3.7
scikit-learn>=1.3
statsmodels>=0.14
meteostat>=1.7
pytz


</details>

*Developed on PythonÂ 3.9 (WindowsÂ 10 32â€¯GB RAM).*
TensorFlow is optional (included for future NN experiments).

---

<a name="hourlywx-reproducing"></a>
## Reproducing the Results

1. pip install -r requirements.txt.
2. Load raw CSVs into data/ with exact filenames

---

<a name="hourlywx-training"></a>
## Training

Key hyperâ€‘parameters explored:

| Flag / Param             | Default                | Model      | Description            |
| ------------------------ | ---------------------- | ---------- | ---------------------- |
| n_estimators           | 100/300                | XGBoost    | boosting rounds        |
| learning_rate          | 0.05â€‘0.1               | XGBoost    | Î·                      |
| max_depth              | 3â€‘7                    | XGBoost    | tree depth             |
| subsample/colsample    | 0.8â€‘1.0                | XGBoost    | row/column sampling    |
| reg_alpha / reg_lambda | 0â€‘1 / 1â€‘10             | XGBoost    | L1 / L2 regularisation |
| hidden_layer_sizes     | (128,64,32) vs (64,32) | scikit MLP | gridâ€‘searched          |
| alpha (L2)             | 0.0001â€‘0.001           | scikit MLP |                        |

---

<a name="hourlywx-eval"></a>
## Evaluation & Metrics

For both **Net\_Amount** and **TC** the scripts print:

* **RMSE**, **MAE**, **RÂ²** on holdâ€‘out 20â€¯% test set.

---

<a name="hourlywx-structure"></a>
## Project Structure

```

.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Master Store.csv
â”‚   â”œâ”€â”€ Sales All Stores May 2025.csv
â”‚   â””â”€â”€ Weather Data Updated.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ HOURLY Sales Weather Project FINAL.ipynb
â””â”€â”€ README.md

```
