{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53f4959",
   "metadata": {},
   "source": [
    "remove store number (Store_No), CODE (subcluster 1), CODE FY26 1 (subcluster 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Loading libraries and datasets, and set up data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (Input, LSTM, Embedding, Flatten, Dense, Concatenate, SpatialDropout1D, BatchNormalization, Dropout, Add, RepeatVector)\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Cluster, PH, Weather, Sales data\n",
    "df = pd.read_csv(r\" ... csv\")\n",
    "\n",
    "# Feature columns\n",
    "time_varying_categorical_cols = ['Rain?','Name','Puasa','Public Holiday','Day','Month']  # varies with time\n",
    "static_categorical_cols = ['State','CODE FY26 2 (subcluster 3)']  # won't change depending on time\n",
    "categorical_cols = time_varying_categorical_cols + static_categorical_cols\n",
    "numeric_cols = ['Net_Amount','TC','Days_after_Opening','Average Daily Temperature (°C)']\n",
    "\n",
    "# Filling in spaces to be encoded: 'CODE (subcluster 1)': blank, Name: no PH, Puasa: 0, Public Holiday: 0\n",
    "# Filling empty spaces\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].fillna('blank')\n",
    "df['Name']               = df['Name'].fillna('no PH')\n",
    "df['Puasa']              = df['Puasa'].fillna(0)\n",
    "df['Public Holiday']     = df['Public Holiday'].fillna(0)\n",
    "\n",
    "# Replace any empty‐string entries (''), if they exist\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].replace('', 'blank')\n",
    "df['Name']               = df['Name'].replace('', 'no PH')\n",
    "df['Puasa']              = df['Puasa'].replace('', 0)\n",
    "df['Public Holiday']     = df['Public Holiday'].replace('', 0)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Encoding categorical columns\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Splitting categorical columns into 2 types - to be embedded or be binary\n",
    "embed_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_uniques = df[col].nunique()\n",
    "    if col in static_categorical_cols:\n",
    "        embed_cols.append(col)\n",
    "    elif n_uniques < 3:\n",
    "        pass\n",
    "    elif n_uniques >= 7:\n",
    "        embed_cols.append(col)\n",
    "    else:\n",
    "        print(f\"Column {col} has {n_uniques} categories: choose manually.\")\n",
    "\n",
    "# Encoding embed columns\n",
    "encoders = {}\n",
    "for col in embed_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_enc'] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Binary columns - Puasa and Public Holiday are already in 0s and 1s\n",
    "df['Rain?'] = df['Rain?'].map({'Yes': 1, ' No': 0})\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Scaling continuous features\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Building sequences for LSTM\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# purely numeric or binary (already scaled / mapped 0/1)\n",
    "time_numeric_cols = [\n",
    "    'Net_Amount',\n",
    "    'TC',\n",
    "    'Days_after_Opening',\n",
    "    'Average Daily Temperature (°C)',\n",
    "    'Rain?',          # make sure this is 0/1\n",
    "    'Puasa',          # 0/1\n",
    "    'Public Holiday'  # 0/1\n",
    "]\n",
    "\n",
    "# select window size 7-30 days\n",
    "window = 14\n",
    "\n",
    "X_num, X_name, X_day, X_month, X_stat, y = [], [], [], [], [], []\n",
    "\n",
    "static_cols = [col + '_enc' for col in static_categorical_cols if col in embed_cols]\n",
    "\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T   = len(grp)\n",
    "    if T <= window: \n",
    "        continue\n",
    "\n",
    "    # raw arrays\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(T - window):\n",
    "        X_num.append(   arr_num[i : i+window]     )  # → (window, n_num_feats)\n",
    "        X_name.append(  arr_name[i : i+window]    )  # → (window,)\n",
    "        X_day.append(   arr_day[i : i+window]     )\n",
    "        X_month.append(arr_month[i : i+window]    )\n",
    "        X_stat.append(  arr_static[i]             )  # → (n_static_feats,)\n",
    "        y.append(       arr_target[i]             )  # → scalar\n",
    "\n",
    "# stack into arrays\n",
    "X_num    = np.stack(X_num)    # (N, window, n_num_feats)\n",
    "X_name   = np.stack(X_name)   # (N, window)\n",
    "X_day    = np.stack(X_day)    # (N, window)\n",
    "X_month  = np.stack(X_month)  # (N, window)\n",
    "X_stat   = np.stack(X_stat)   # (N, n_static_feats)\n",
    "y        = np.array(y)        # (N,)\n",
    "\n",
    "print(X_num.shape, X_name.shape, X_day.shape, X_month.shape, X_stat.shape, y.shape)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Time-aware train-validation data split\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Initialize empty Python lists to hold your training vs. validation sequences:\n",
    "X_num_train, X_num_val, \\\n",
    "X_name_train, X_name_val, \\\n",
    "X_day_train,  X_day_val,  \\\n",
    "X_month_train,X_month_val,\\\n",
    "X_stat_train, X_stat_val, \\\n",
    "y_train,      y_val = ([] for _ in range(12))\n",
    "\n",
    "window = 14\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T = len(grp)\n",
    "    n_windows = T - window\n",
    "    if n_windows <= 0: continue\n",
    "\n",
    "    # how many windows to train on for this store\n",
    "    split_store = int(0.8 * n_windows)\n",
    "\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        num_seq  = arr_num[i : i+window]\n",
    "        name_seq = arr_name[i : i+window]\n",
    "        day_seq  = arr_day[i : i+window]\n",
    "        month_seq = arr_month[i : i+window]\n",
    "        static = arr_static[i]\n",
    "        target   = arr_target[i]\n",
    "        if i < split_store:\n",
    "            X_num_train.append(num_seq)\n",
    "            X_name_train.append(name_seq)\n",
    "            X_day_train.append(day_seq)\n",
    "            X_month_train.append(month_seq)\n",
    "            X_stat_train.append(static)\n",
    "            y_train.append(target)\n",
    "        else:\n",
    "            X_num_val.append(num_seq)\n",
    "            X_name_val.append(name_seq)\n",
    "            X_day_val.append(day_seq)\n",
    "            X_month_val.append(month_seq)\n",
    "            X_stat_val.append(static)\n",
    "            y_val.append(target)\n",
    "\n",
    "# finally stack\n",
    "X_num_train = np.stack(X_num_train)\n",
    "X_name_train = np.stack(X_name_train)\n",
    "X_day_train = np.stack(X_day_train)\n",
    "X_month_train = np.stack(X_month_train)\n",
    "X_stat_train = np.stack(X_stat_train)\n",
    "y_train     = np.stack(y_train)\n",
    "\n",
    "X_stat_train = np.array(X_stat_train)\n",
    "X_stat_val   = np.array(X_stat_val)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Build, sanity‑check & train the Keras model (with embeddings)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Building the model\n",
    "def build_sales_lstm(\n",
    "        W, F,\n",
    "        time_cardinalities,        # dict: {'name':n_name,'day':n_day,'month':n_month}\n",
    "        static_cardinalities,      # list of (col_name, vocab_size)\n",
    "        lstm_units     = 48,\n",
    "        dropout_rate   = 0.2,\n",
    "        static_dense   = 64,\n",
    "        learning_rate  = 0.001\n",
    "    ):\n",
    "    # — time‑series inputs —\n",
    "    num_in       = Input(shape=(W, F), name='num_in')\n",
    "    name_seq_in  = Input(shape=(W,), dtype='int32', name='name_seq_in')\n",
    "    day_seq_in   = Input(shape=(W,), dtype='int32', name='day_seq_in')\n",
    "    month_seq_in = Input(shape=(W,), dtype='int32', name='month_seq_in')\n",
    "\n",
    "    # — time‑varying embeddings —\n",
    "    n_name, n_day, n_month = (\n",
    "        time_cardinalities['name'],\n",
    "        time_cardinalities['day'],\n",
    "        time_cardinalities['month'],\n",
    "    )\n",
    "    dim = lambda n: min(100, n//2 + 5)\n",
    "\n",
    "    name_emb  = SpatialDropout1D(dropout_rate)(Embedding(n_name, dim(n_name))(name_seq_in))\n",
    "    day_emb   = SpatialDropout1D(dropout_rate)(Embedding(n_day,   dim(n_day))(day_seq_in))\n",
    "    month_emb = SpatialDropout1D(dropout_rate)(Embedding(n_month, dim(n_month))(month_seq_in))\n",
    "\n",
    "    # concatenate all time-varying features\n",
    "    time_concat = Concatenate(axis=-1)([num_in, name_emb, day_emb, month_emb])  # (batch, W, ...)\n",
    "\n",
    "    # — static inputs and embeddings —\n",
    "    static_inputs, static_vecs = [], []\n",
    "    for base, vocab in static_cardinalities:\n",
    "        inp = Input(shape=(1,), dtype='int32', name=f\"{base}_in\")\n",
    "        emb = Embedding(vocab, dim(vocab))(inp)\n",
    "        emb = Flatten()(emb)\n",
    "        emb = BatchNormalization()(emb)\n",
    "        emb = Dropout(dropout_rate)(emb)\n",
    "        static_inputs.append(inp)\n",
    "        static_vecs.append(emb)\n",
    "\n",
    "    # combine static embeddings\n",
    "    static_cat = Concatenate()(static_vecs)\n",
    "    static_cat = Dense(128, activation='relu')(static_cat)\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    static_cat = Dense(static_dense, activation='relu')(static_cat)\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    # residual block\n",
    "    skip = static_cat\n",
    "    static_cat = Dense(static_dense, activation='relu')(static_cat)\n",
    "    static_cat = Add()([static_cat, skip])\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    # — tile static features across time and append to time series features —\n",
    "    static_tile = RepeatVector(W)(static_cat)  # shape (batch, W, static_dense)\n",
    "    time_concat = Concatenate(axis=-1)([time_concat, static_tile])\n",
    "\n",
    "    # — generate initial LSTM states from static embedding —\n",
    "    init_h = Dense(lstm_units, activation='tanh', name='init_h')(static_cat)\n",
    "    init_c = Dense(lstm_units, activation='tanh', name='init_c')(static_cat)\n",
    "\n",
    "    # — LSTM branch with static-initialized state —\n",
    "    x = LSTM(lstm_units, dropout=0.0)(time_concat, initial_state=[init_h, init_c])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # — combine LSTM output with static for final prediction —\n",
    "    merged = Concatenate()([x, static_cat])\n",
    "    out = Dense(2, activation='linear', name='sales_out')(merged)\n",
    "\n",
    "    # build model\n",
    "    model = Model(\n",
    "        inputs=[num_in, name_seq_in, day_seq_in, month_seq_in] + static_inputs,\n",
    "        outputs=out,\n",
    "        name='Sales_LSTM_StaticInit'\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Prepare cardinalities for embeddings\n",
    "n_name  = df['Name_enc' ].nunique()\n",
    "n_day   = df['Day_enc'  ].nunique()\n",
    "n_month = df['Month_enc'].nunique()\n",
    "time_cardinalities = {'name': n_name, 'day': n_day, 'month': n_month}\n",
    "\n",
    "static_cardinalities = []\n",
    "for enc_col in static_cols:                      # e.g. \"Store_No_enc\"\n",
    "    base  = enc_col.replace('_enc','')           # \"Store_No\"\n",
    "    vocab = df[enc_col].nunique()\n",
    "    static_cardinalities.append((base, vocab))\n",
    "\n",
    "# window & feature count\n",
    "_, W, F = X_num.shape\n",
    "\n",
    "# Cast *all* arrays to proper dtype and shape\n",
    "X_num_train   = np.asarray(X_num_train,   dtype=np.float32)\n",
    "X_num_val     = np.asarray(X_num_val,     dtype=np.float32)\n",
    "\n",
    "def to_int2d(arr): return np.asarray(arr, dtype=np.int32)\n",
    "X_name_train  = to_int2d(X_name_train)\n",
    "X_day_train   = to_int2d(X_day_train)\n",
    "X_month_train = to_int2d(X_month_train)\n",
    "X_name_val    = to_int2d(X_name_val)\n",
    "X_day_val     = to_int2d(X_day_val)\n",
    "X_month_val   = to_int2d(X_month_val)\n",
    "\n",
    "X_stat_train  = np.asarray(X_stat_train, dtype=np.int32)\n",
    "X_stat_val    = np.asarray(X_stat_val,   dtype=np.int32)\n",
    "y_train       = np.asarray(y_train,      dtype=np.float32)\n",
    "y_val         = np.asarray(y_val,        dtype=np.float32)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Fit Model\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Build input dicts (by name ─ safest)\n",
    "train_inputs = {\n",
    "    'num_in':       X_num_train,\n",
    "    'name_seq_in':  X_name_train,\n",
    "    'day_seq_in':   X_day_train,\n",
    "    'month_seq_in': X_month_train,\n",
    "}\n",
    "val_inputs   = {\n",
    "    'num_in':       X_num_val,\n",
    "    'name_seq_in':  X_name_val,\n",
    "    'day_seq_in':   X_day_val,\n",
    "    'month_seq_in': X_month_val,\n",
    "}\n",
    "\n",
    "# add static columns (need reshape (N,1))\n",
    "for i, (base, _) in enumerate(static_cardinalities):\n",
    "    train_inputs[f'{base}_in'] = X_stat_train[:, i].reshape(-1,1)\n",
    "    val_inputs  [f'{base}_in'] = X_stat_val[:,   i].reshape(-1,1)\n",
    "\n",
    "# Sanity check: print expected vs actual - make sure ValueError: Functional() don't pop up\n",
    "print(\"\\n── EXPECTED  vs  ACTUAL per‑sample shapes ──\")\n",
    "tmp_model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "for inp in tmp_model.inputs:\n",
    "    k   = inp.name.split(':')[0]\n",
    "    exp = tuple(inp.shape[1:])\n",
    "    act = tuple(train_inputs[k].shape[1:])\n",
    "    ok  = \"✅\" if exp == act else \"❌\"\n",
    "    print(f\"{k:25s} expected={str(exp):12s} actual={str(act):12s} {ok}\")\n",
    "print(\"────────────────────────────────────────────\\n\")\n",
    "\n",
    "# Instantiate final model & train\n",
    "model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "es   = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, rlrp],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.plot(history.history['loss'],     label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8.  Evaluate on a hold‑out test set and generate forecasts\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "# Build a fresh 70/15/15 split Train/Val/Test (time‑aware per store) \n",
    "def time_aware_split_by_store(df, window, time_numeric_cols, static_cols):\n",
    "    X_num_tr, X_num_va, X_num_te = [], [], []\n",
    "    X_name_tr, X_name_va, X_name_te = [], [], []\n",
    "    X_day_tr,  X_day_va,  X_day_te  = [], [], []\n",
    "    X_month_tr,X_month_va,X_month_te= [], [], []\n",
    "    X_stat_tr, X_stat_va, X_stat_te = [], [], []\n",
    "    y_tr,      y_va,      y_te      = [], [], []\n",
    "\n",
    "    for _, grp in df.groupby('Store_No'):\n",
    "        grp = grp.sort_values('Date')\n",
    "        T = len(grp)\n",
    "        n_windows = T - window\n",
    "        if n_windows <= 0: continue\n",
    "\n",
    "        n_train = int(0.70 * n_windows)\n",
    "        n_val   = int(0.15 * n_windows)\n",
    "        # remainder → test\n",
    "        n_test  = n_windows - n_train - n_val\n",
    "\n",
    "        arr_num    = grp[time_numeric_cols].values\n",
    "        arr_name   = grp['Name_enc' ].values\n",
    "        arr_day    = grp['Day_enc'  ].values\n",
    "        arr_month  = grp['Month_enc'].values\n",
    "        arr_static = grp[static_cols].iloc[window:].values\n",
    "        arr_target = grp[['Net_Amount','TC']].values[window:]\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            seq_num   = arr_num[i:i+window]\n",
    "            seq_name  = arr_name[i:i+window]\n",
    "            seq_day   = arr_day[i:i+window]\n",
    "            seq_month = arr_month[i:i+window]\n",
    "            stat_vec  = arr_static[i]\n",
    "            target    = arr_target[i]\n",
    "\n",
    "            # buckets\n",
    "            if i < n_train:\n",
    "                bucket = (X_num_tr,  X_name_tr,  X_day_tr,  X_month_tr,  X_stat_tr,  y_tr)\n",
    "            elif i < n_train + n_val:\n",
    "                bucket = (X_num_va,  X_name_va,  X_day_va,  X_month_va,  X_stat_va,  y_va)\n",
    "            else:\n",
    "                bucket = (X_num_te,  X_name_te,  X_day_te,  X_month_te,  X_stat_te,  y_te)\n",
    "\n",
    "            bucket[0].append(seq_num)\n",
    "            bucket[1].append(seq_name)\n",
    "            bucket[2].append(seq_day)\n",
    "            bucket[3].append(seq_month)\n",
    "            bucket[4].append(stat_vec)\n",
    "            bucket[5].append(target)\n",
    "\n",
    "    # stack to np arrays\n",
    "    def _stack(lst): return np.stack(lst) if lst and isinstance(lst[0], np.ndarray) else np.array(lst)\n",
    "    return tuple(map(_stack,\n",
    "        [X_num_tr,X_num_va,X_num_te,\n",
    "         X_name_tr,X_name_va,X_name_te,\n",
    "         X_day_tr,X_day_va,X_day_te,\n",
    "         X_month_tr,X_month_va,X_month_te,\n",
    "         X_stat_tr,X_stat_va,X_stat_te,\n",
    "         y_tr,y_va,y_te]))\n",
    "\n",
    "# build split\n",
    "(split_X_num_tr, split_X_num_va, split_X_num_te,\n",
    " split_X_name_tr, split_X_name_va, split_X_name_te,\n",
    " split_X_day_tr,  split_X_day_va,  split_X_day_te,\n",
    " split_X_month_tr,split_X_month_va,split_X_month_te,\n",
    " split_X_stat_tr, split_X_stat_va, split_X_stat_te,\n",
    " split_y_tr,      split_y_va,      split_y_te) = time_aware_split_by_store(\n",
    "    df, window, time_numeric_cols, static_cols\n",
    ")\n",
    "\n",
    "# Assemble TEST input dict \n",
    "def build_input_dict(X_num,X_name,X_day,X_month,X_stat):\n",
    "    d = {\n",
    "        'num_in':       X_num.astype(np.float32),\n",
    "        'name_seq_in':  X_name.astype(np.int32),\n",
    "        'day_seq_in':   X_day.astype(np.int32),\n",
    "        'month_seq_in': X_month.astype(np.int32)\n",
    "    }\n",
    "    for i,(base,_) in enumerate(static_cardinalities):\n",
    "        d[f'{base}_in'] = X_stat[:,i].reshape(-1,1).astype(np.int32)\n",
    "    return d\n",
    "\n",
    "test_inputs = build_input_dict(\n",
    "    split_X_num_te, split_X_name_te, split_X_day_te,\n",
    "    split_X_month_te, split_X_stat_te)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(test_inputs, verbose=0)\n",
    "\n",
    "# Inverse‑scale Net_Amount & TC\n",
    "# remember scaler was fit on numeric_cols, where Net_Amount is col 0 and TC is col 1\n",
    "# we’ll invert only those two columns\n",
    "net_idx = numeric_cols.index('Net_Amount')\n",
    "tc_idx  = numeric_cols.index('TC')\n",
    "\n",
    "def inverse_scale(scaled_vec, col_idx):\n",
    "    tmp      = np.zeros((len(scaled_vec), len(numeric_cols)))\n",
    "    tmp[:,col_idx] = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:,col_idx]\n",
    "\n",
    "net_pred = inverse_scale(y_pred_scaled[:,0], net_idx)\n",
    "tc_pred  = inverse_scale(y_pred_scaled[:,1], tc_idx)\n",
    "\n",
    "net_true = inverse_scale(split_y_te[:,0], net_idx)\n",
    "tc_true  = inverse_scale(split_y_te[:,1], tc_idx)\n",
    "\n",
    "# Metrics & Interpretation\n",
    "def interpret(true, pred, name):\n",
    "    # core metrics\n",
    "    mae  = mean_absolute_error(true, pred)\n",
    "    rmse = math.sqrt(mean_squared_error(true, pred))\n",
    "    r2   = r2_score(true, pred)\n",
    "\n",
    "    # reference stats\n",
    "    mean_y = np.mean(true)\n",
    "    std_y  = np.std(true)\n",
    "\n",
    "    # relative ratios\n",
    "    mae_ratio  = mae  / mean_y  if mean_y else float('nan')\n",
    "    rmse_ratio = rmse / std_y   if std_y  else float('nan')\n",
    "\n",
    "    # header\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\" MAE : {mae:8.2f}   (mean = {mean_y:8.2f}, MAE/mean = {mae_ratio:.2f})\")\n",
    "    print(f\" RMSE: {rmse:8.2f}   (std  = {std_y:8.2f}, RMSE/std  = {rmse_ratio:.2f})\")\n",
    "    print(f\" R²  : {r2:6.3f}\")\n",
    "\n",
    "    # interpret MAE‑to‑mean\n",
    "    if mae_ratio < 0.10:\n",
    "        print(\"   🔵 Excellent MAE (<10% of mean)\")\n",
    "    elif mae_ratio < 0.20:\n",
    "        print(\"   🟢 Good MAE (<20% of mean)\")\n",
    "    elif mae_ratio < 0.30:\n",
    "        print(\"   🟡 Acceptable MAE (<30% of mean)\")\n",
    "    else:\n",
    "        print(\"   🔴 Poor MAE (>30% of mean)\")\n",
    "\n",
    "    # interpret RMSE‑to‑std\n",
    "    if rmse_ratio < 0.50:\n",
    "        print(\"   🔵 Excellent RMSE (<0.5 σ)\")\n",
    "    elif rmse_ratio < 0.75:\n",
    "        print(\"   🟢 Good RMSE (<0.75 σ)\")\n",
    "    elif rmse_ratio < 1.00:\n",
    "        print(\"   🟡 Acceptable RMSE (<1.0 σ)\")\n",
    "    else:\n",
    "        print(\"   🔴 Poor RMSE (>1.0 σ)\")\n",
    "\n",
    "    # interpret R²\n",
    "    if r2 >= 0.90:\n",
    "        print(\"   🔵 Excellent R² (≥0.90)\")\n",
    "    elif r2 >= 0.75:\n",
    "        print(\"   🟢 Good R² (0.75–0.90)\")\n",
    "    elif r2 >= 0.50:\n",
    "        print(\"   🟡 Acceptable R² (0.50–0.75)\")\n",
    "    else:\n",
    "        print(\"   🔴 Weak R² (<0.50)\")\n",
    "\n",
    "# run it for both targets\n",
    "interpret(net_true, net_pred, \"Net_Amount\")\n",
    "interpret(tc_true,  tc_pred,  \"TC\")\n",
    "\n",
    "# Visual check for Net_Amount\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(net_true, net_pred, alpha=.4)\n",
    "ax[0].plot([net_true.min(), net_true.max()],\n",
    "           [net_true.min(), net_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True Net_Amount\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"Net_Amount – scatter\")\n",
    "\n",
    "ax[1].plot(net_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(net_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"Net_Amount – first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Visual check for TC\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(tc_true, tc_pred, alpha=.4)\n",
    "ax[0].plot([tc_true.min(), tc_true.max()],\n",
    "           [tc_true.min(), tc_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True TC\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"TC – scatter\")\n",
    "\n",
    "ax[1].plot(tc_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(tc_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"TC – first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759485ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Helper: inverse-scale a single column (we already defined earlier)\n",
    "def inverse_scale(scaled_vec, col_idx):\n",
    "    tmp              = np.zeros((len(scaled_vec), len(numeric_cols)))\n",
    "    tmp[:, col_idx]  = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:, col_idx]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Baseline error on Net_Amount (choose whichever target you care about)\n",
    "base_pred = model.predict(test_inputs, verbose=0)[:, 0]      # column 0 = Net_Amount\n",
    "base_pred = inverse_scale(base_pred, numeric_cols.index('Net_Amount'))\n",
    "base_true = inverse_scale(split_y_te[:, 0], numeric_cols.index('Net_Amount'))\n",
    "base_mse  = mean_squared_error(base_true, base_pred)\n",
    "\n",
    "print(f\"Baseline RMSE = {np.sqrt(base_mse):.2f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Define groups of columns that belong to one logical feature\n",
    "# build a dict: { \"display name\": list[ key(s) in test_inputs ] }\n",
    "groups = OrderedDict({\n",
    "    # time-series numeric (whole sequence counts as one logical feature)\n",
    "    \"Net_Amount_lags\" : [\"num_in\"],   # but we'll permute only the Net column inside num_in\n",
    "    \"TC_lags\"         : [\"num_in\"],\n",
    "    \"Days_after_Opening\": [\"num_in\"],\n",
    "    \"Avg_Temp_lags\"   : [\"num_in\"],\n",
    "    \"Rain?\"           : [\"num_in\"],\n",
    "    \"Puasa\"           : [\"num_in\"],\n",
    "    \"Public_Holiday\"  : [\"num_in\"],\n",
    "\n",
    "    # time-categorical\n",
    "    \"Name_enc_lags\"   : [\"name_seq_in\"],\n",
    "    \"Day_enc_lags\"    : [\"day_seq_in\"],\n",
    "    \"Month_enc_lags\"  : [\"month_seq_in\"],\n",
    "\n",
    "    # static categoricals – each has its own input tensor\n",
    "})\n",
    "for base, _ in static_cardinalities:\n",
    "    groups[base] = [f\"{base}_in\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Permutation importance loop\n",
    "importances = OrderedDict()\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for gname, keys in groups.items():\n",
    "    # make a deep copy of the original dict\n",
    "    perturbed = {k: v.copy() for k, v in test_inputs.items()}\n",
    "\n",
    "    for k in keys:\n",
    "        arr = perturbed[k]\n",
    "        # permute **independently for each timestep** if 3-D, else shuffle axis-0\n",
    "        if arr.ndim == 3:          # (N, window, features)\n",
    "            flat = arr.reshape(arr.shape[0], -1)\n",
    "            rng.shuffle(flat, axis=0)\n",
    "            perturbed[k] = flat.reshape(arr.shape)\n",
    "        else:                      # (N,1) or (N,window)\n",
    "            rng.shuffle(arr, axis=0)\n",
    "            perturbed[k] = arr\n",
    "\n",
    "    # predict with the corrupted feature(s)\n",
    "    y_perm = model.predict(perturbed, verbose=0)[:, 0]\n",
    "    y_perm = inverse_scale(y_perm, numeric_cols.index('Net_Amount'))\n",
    "    perm_mse = mean_squared_error(base_true, y_perm)\n",
    "\n",
    "    importances[gname] = np.sqrt(perm_mse) - np.sqrt(base_mse)  # Δ RMSE\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Plot\n",
    "names  = list(importances.keys())\n",
    "scores = np.array(list(importances.values()))\n",
    "order  = np.argsort(scores)[::-1]         # highest impact first\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(names)[order], scores[order])\n",
    "plt.xlabel(\"Increase in RMSE when feature is permuted\")\n",
    "plt.title(\"Permutation Feature Importance – Net_Amount\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6b. Permutation importance for TC (column 1)\n",
    "# ------------------------------------------------------------\n",
    "# 6b.1  Baseline TC predictions & RMSE\n",
    "base_pred_tc = model.predict(test_inputs, verbose=0)[:, 1]  # pick the TC column\n",
    "base_pred_tc = inverse_scale(base_pred_tc, numeric_cols.index('TC'))\n",
    "base_true_tc = inverse_scale(split_y_te[:, 1], numeric_cols.index('TC'))\n",
    "base_mse_tc  = mean_squared_error(base_true_tc, base_pred_tc)\n",
    "print(f\"Baseline TC RMSE = {np.sqrt(base_mse_tc):.2f}\")\n",
    "\n",
    "# 6b.2  Same groups dictionary from before\n",
    "#         (we assume 'groups' is still in scope from your Net_Amount run)\n",
    "\n",
    "importances_tc = OrderedDict()\n",
    "for gname, keys in groups.items():\n",
    "    perturbed = {k: v.copy() for k, v in test_inputs.items()}\n",
    "    for k in keys:\n",
    "        arr = perturbed[k]\n",
    "        if arr.ndim == 3:\n",
    "            flat = arr.reshape(arr.shape[0], -1)\n",
    "            rng.shuffle(flat, axis=0)\n",
    "            perturbed[k] = flat.reshape(arr.shape)\n",
    "        else:\n",
    "            rng.shuffle(arr, axis=0)\n",
    "            perturbed[k] = arr\n",
    "\n",
    "    # predict on permuted data, then inverse-scale TC\n",
    "    y_p_tc = model.predict(perturbed, verbose=0)[:, 1]\n",
    "    y_p_tc = inverse_scale(y_p_tc, numeric_cols.index('TC'))\n",
    "    imp_mse = mean_squared_error(base_true_tc, y_p_tc)\n",
    "\n",
    "    importances_tc[gname] = np.sqrt(imp_mse) - np.sqrt(base_mse_tc)\n",
    "\n",
    "# 6b.3  Plot TC importances\n",
    "names_tc  = list(importances_tc.keys())\n",
    "scores_tc = np.array(list(importances_tc.values()))\n",
    "order_tc  = np.argsort(scores_tc)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(names_tc)[order_tc], scores_tc[order_tc])\n",
    "plt.xlabel(\"Increase in TC-RMSE when feature is permuted\")\n",
    "plt.title(\"Permutation Feature Importance – TC\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
