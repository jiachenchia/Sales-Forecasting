{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a4967a",
   "metadata": {},
   "source": [
    "Sales Forecast (not time series) + weather data updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1dc91",
   "metadata": {},
   "source": [
    "Start Modelling: Linear, Polynomial, XGBOOST, NEURAL NETWORKS, then need manipulate data (no sales duplicate data), then can do time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(r\" ... Master Weather + Store Subcluster + Holiday + Sales.csv\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) Preparing data for regression\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# List of categorical columns you want to encode\n",
    "cat_cols = ['Store_No','Name', 'State', 'Day', 'CODE (subcluster 1)',\n",
    "            'CODE FY26 1 (subcluster 2)', 'CODE FY26 2 (subcluster 3)', 'Rain?','Public Holiday']\n",
    "\n",
    "# Fill missing values in these columns only\n",
    "df[cat_cols] = df[cat_cols].fillna(\"Missing\")\n",
    "\n",
    "# Encode\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Making sure PH, Days from Holiday, Puasa count are all integers\n",
    "object_cols = [\"Public Holiday\", \"Days From Holiday\",\"Puasa Count\"]\n",
    "for cols in object_cols:\n",
    "    df[cols] = df[col].astype(int)\n",
    "\n",
    "# Using a numeric placeholder instead of string\n",
    "df[['Net_Amount', 'TC']] = df[['Net_Amount', 'TC']].replace(\"No Sales\", 0).astype(float)\n",
    "\n",
    "# Dropping datetime columns\n",
    "df = df.drop(columns=['Date','Opening_Date'])  # if not useful\n",
    "\n",
    "# Splitting into features and target\n",
    "X = df.drop(['Net_Amount', 'TC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) Implementing Polynomial Regression\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y = df[['Net_Amount','TC']]\n",
    "\n",
    "# Identify continuous columns\n",
    "numeric_cols = [\n",
    "    'Average Daily Temperature (Â°C)',\n",
    "    'Days From Holiday',\n",
    "    'Puasa Count',\n",
    "    'Days_after_Opening'\n",
    "]\n",
    "\n",
    "# 2) Split before scaling so you donâ€™t leak info\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3a) StandardScaler: center @0, std=1\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = X_train.copy()\n",
    "X_test_std  = X_test.copy()\n",
    "\n",
    "X_train_std[numeric_cols] = std_scaler.fit_transform( X_train[numeric_cols] )\n",
    "X_test_std [numeric_cols] = std_scaler .transform( X_test [numeric_cols] )\n",
    "\n",
    "# Creating a polynomial regression pipeline\n",
    "degree = 2  # You can tune this\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "multi_model = MultiOutputRegressor(poly_model)\n",
    "\n",
    "# Fit the model\n",
    "multi_model.fit(X_train, y_train)\n",
    "\n",
    "# Performance\n",
    "y_pred = multi_model.predict(X_test)\n",
    "\n",
    "for i, col in enumerate(y.columns):\n",
    "    y_true = y_test.iloc[:, i]\n",
    "    y_hat  = y_pred[:,    i]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_hat))\n",
    "    r2   = r2_score(             y_true, y_hat)      # â† define r2 here\n",
    "\n",
    "    print(f\"{col} â†’ RMSE: {rmse:.2f} | RÂ²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "# my laptop can't run gridsearchcv cuz it takes too much memory\n",
    "# multi_base = MultiOutputRegressor(base_pipeline)\n",
    "\n",
    "# # 3) Define a negâ€RMSE scorer (so lower RMSE is â€œbetterâ€ for GridSearch)\n",
    "# rmse_scorer = make_scorer(\n",
    "#     lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)),\n",
    "#     greater_is_better=False\n",
    "# )\n",
    "\n",
    "# # 4) Parameter grid: tuning degree of polynomial (and intercept if you like)\n",
    "# param_grid = {\n",
    "#     'estimator__polynomialfeatures__degree':       [1, 2, 3, 4],\n",
    "#     'estimator__linearregression__fit_intercept':  [True, False]\n",
    "# }\n",
    "\n",
    "# # 5) Set up GridSearchCV\n",
    "# grid = GridSearchCV(\n",
    "#     estimator = multi_base,\n",
    "#     param_grid = param_grid,\n",
    "#     scoring    = {'rmse': rmse_scorer, 'r2': 'r2'},\n",
    "#     refit      = 'rmse',   # after CV, refit on whole train set using best RMSE\n",
    "#     cv         = 5,\n",
    "#     verbose    = 1,\n",
    "#     n_jobs     = -1\n",
    "# )\n",
    "\n",
    "# # 6) Run grid search\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # 7) Evaluate best model on the test set\n",
    "# best_model = grid.best_estimator_\n",
    "# y_pred_gs  = best_model.predict(X_test)\n",
    "\n",
    "# print(\"Best parameters:\", grid.best_params_)\n",
    "# print(\"\\n=== Performance on TEST set ===\")\n",
    "# for i, col in enumerate(y.columns):\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], y_pred_gs[:, i]))\n",
    "#     r2   = r2_score(y_test.iloc[:, i],       y_pred_gs[:, i])\n",
    "#     print(f\"{col:12s} â†’ RMSE: {rmse:8.2f} | RÂ²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) XGBOOST Regressor for Net_Amount\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y = df['Net_Amount']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initial model\n",
    "model_net = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "model_net.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model_net.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] RMSE: {rmse}\")\n",
    "print(f\"[Net_Amount] R-squared: {r2}\")\n",
    "print(f\"[Net_Amount] MAE: {mae}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1],\n",
    "    'colsample_bytree': [0.8, 1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(xgb.XGBRegressor(objective='reg:squarederror'), params, scoring='neg_root_mean_squared_error', cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] Tuned RMSE: {rmse}\")\n",
    "print(f\"[Net_Amount] Tuned RÂ²: {r2}\")\n",
    "print(f\"[Net_Amount] Tuned MAE: {mae}\")\n",
    "print(f\"[Net_Amount] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio_sd = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    "\n",
    "print(f\"[Net_Amount] RMSE / SD ratio: {ratio_sd:.2f}\")\n",
    "print(f\"[Net_Amount] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio_sd < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio_sd < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio_sd <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) XGBOOST Regressor for TC\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "y = df['TC']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initial model\n",
    "model_tc = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "model_tc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model_tc.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f\"[TC] RMSE: {rmse}\")\n",
    "print(f\"[TC] R-squared: {r2}\")\n",
    "print(f\"[TC] MAE: {mae}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "grid = GridSearchCV(xgb.XGBRegressor(objective='reg:squarederror'), params, scoring='neg_root_mean_squared_error', cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_net = grid.best_estimator_\n",
    "y_pred = best_net.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f\"[TC] Tuned RMSE: {rmse}\")\n",
    "print(f\"[TC] Tuned RÂ²: {r2}\")\n",
    "print(f\"[TC] Tuned MAE: {mae}\")\n",
    "print(f\"[TC] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[TC] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[TC] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) LightGBM for Net Amount (1st)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# FEATURES: set Store_No, CODE FY26 1 2 3, Name, state, day as categories\n",
    "\n",
    "\n",
    "y = df['Net_Amount']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7)\n",
    "model_net.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(LGBMRegressor(), param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_net = grid.best_estimator_\n",
    "y_pred = best_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] Best Params: {grid.best_params_}\")\n",
    "print(f\"[Net_Amount] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[Net_Amount] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[Net_Amount] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[Net_Amount] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_net.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plotbest\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - Net_Amount', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c270f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) LightGBM for Net Amount (2nd)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y = df['Net_Amount']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [\n",
    "    'Store_No', \n",
    "    'CODE (subcluster 1)', \n",
    "    'CODE FY26 1 (subcluster 2)', \n",
    "    'CODE FY26 2 (subcluster 3)', \n",
    "    'Name', \n",
    "    'State', \n",
    "    'Day'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(LGBMRegressor(), param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_net = grid.best_estimator_\n",
    "y_pred = best_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] Best Params: {grid.best_params_}\")\n",
    "print(f\"[Net_Amount] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[Net_Amount] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[Net_Amount] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[Net_Amount] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_net.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - Net_Amount', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d785c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) LightGBM for Net Amount drop Store_No (3rd)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "df.columns = df.columns.str.replace(r'[^\\w]+', '_', regex=True)\n",
    "X = df.drop(['Net_Amount', 'TC','Store_No'], axis=1)\n",
    "y = df['Net_Amount']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cat_cols = ['CODE_subcluster_1', 'CODE_FY26_1_subcluster_2_', 'CODE_FY26_2_subcluster_3_']\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [col for col in X.columns if col in ['Name', 'State', 'Day', 'CODE_subcluster_1_', \n",
    "                                                'CODE_FY26_1_subcluster_2_','CODE_FY26_2_subcluster_3_']]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "    X_test[col] = X_test[col].cat.set_categories(X_train[col].cat.categories)\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "base_model = LGBMRegressor()\n",
    "base_model.set_params(categorical_feature=cat_cols)\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_net = grid.best_estimator_\n",
    "y_pred = best_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] Best Params: {grid.best_params_}\")\n",
    "print(f\"[Net_Amount] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[Net_Amount] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[Net_Amount] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[Net_Amount] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_net.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - Net_Amount', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) LightGBM for Net Amount drop Store_No, Rain?, Days From Holiday, Puasa Count (4th)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "df.columns = df.columns.str.replace(r'[^\\w]+', '_', regex=True)\n",
    "X = df.drop(['Net_Amount', 'TC','Store_No','Rain_', 'Days_From_Holiday','Puasa_Count'], axis=1)\n",
    "y = df['Net_Amount']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cat_cols = ['CODE_subcluster_1', 'CODE_FY26_1_subcluster_2_', ...]\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [col for col in X.columns if col in ['Name', 'State', 'Day', 'CODE_subcluster_1_', \n",
    "                                                'CODE_FY26_1_subcluster_2_','CODE_FY26_2_subcluster_3_']]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "    X_test[col] = X_test[col].cat.set_categories(X_train[col].cat.categories)\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "base_model = LGBMRegressor()\n",
    "base_model.set_params(categorical_feature=cat_cols)\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_net = grid.best_estimator_\n",
    "y_pred = best_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] Best Params: {grid.best_params_}\")\n",
    "print(f\"[Net_Amount] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[Net_Amount] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[Net_Amount] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[Net_Amount] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_net.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - Net_Amount', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6adfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little note: need to rerun 1) since I removed Store No for draft 3\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) LightGBM for TC (1st)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y = df['TC']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7)\n",
    "model_net.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] MAE: {mae:.2f}\")\n",
    "print(f\"[TC] RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(LGBMRegressor(), param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_tc = grid.best_estimator_\n",
    "y_pred = best_tc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] Best Params: {grid.best_params_}\")\n",
    "print(f\"[TC] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[TC] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[TC] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[TC] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[TC] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature importance\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_tc.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - TC', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 12) LightGBM for TC (2nd)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y = df['TC']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [\n",
    "    'Store_No', \n",
    "    'CODE (subcluster 1)', \n",
    "    'CODE FY26 1 (subcluster 2)', \n",
    "    'CODE FY26 2 (subcluster 3)', \n",
    "    'Name', \n",
    "    'State', \n",
    "    'Day'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[Net_Amount] MAE: {mae:.2f}\")\n",
    "print(f\"[Net_Amount] RMSE: {rmse:.2f}\")\n",
    "print(f\"[Net_Amount] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(LGBMRegressor(), param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_tc = grid.best_estimator_\n",
    "y_pred = best_tc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] Best Params: {grid.best_params_}\")\n",
    "print(f\"[TC] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[TC] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[TC] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[TC] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[TC] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_tc.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - TC', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd09e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 12) LightGBM for TC drop Store_No (3rd)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "df.columns = df.columns.str.replace(r'[^\\w]+', '_', regex=True)\n",
    "X = df.drop(['Net_Amount', 'TC','Store_No'], axis=1)\n",
    "y = df['TC']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cat_cols = ['CODE_subcluster_1', 'CODE_FY26_1_subcluster_2_', 'CODE_FY26_2_subcluster_3_']\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [col for col in X.columns if col in ['Name', 'State', 'Day', 'CODE_subcluster_1_', \n",
    "                                                'CODE_FY26_1_subcluster_2_','CODE_FY26_2_subcluster_3_']]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "    X_test[col] = X_test[col].cat.set_categories(X_train[col].cat.categories)\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] MAE: {mae:.2f}\")\n",
    "print(f\"[TC] RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "base_model = LGBMRegressor()\n",
    "base_model.set_params(categorical_feature=cat_cols)\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_tc = grid.best_estimator_\n",
    "y_pred = best_tc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] Best Params: {grid.best_params_}\")\n",
    "print(f\"[TC] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[TC] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[TC] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[TC] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[TC] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_tc.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - TC', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 12) LightGBM for TC drop Store_No, Rain?, Days From Holiday, Puasa Count (4th)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "df.columns = df.columns.str.replace(r'[^\\w]+', '_', regex=True)\n",
    "X = df.drop(['Net_Amount', 'TC','Store_No','Rain_', 'Days_From_Holiday','Puasa_Count'], axis=1)\n",
    "y = df['TC']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cat_cols = ['CODE_subcluster_1', 'CODE_FY26_1_subcluster_2_', ...]\n",
    "\n",
    "# Change some variables to category type so that categorical_features can detect and use those categories\n",
    "cat_cols = [col for col in X.columns if col in ['Name', 'State', 'Day', 'CODE_subcluster_1_', \n",
    "                                                'CODE_FY26_1_subcluster_2_','CODE_FY26_2_subcluster_3_']]\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "    X_test[col] = X_test[col].cat.set_categories(X_train[col].cat.categories)\n",
    "\n",
    "categorical_features = [f\"{col}\" for col in cat_cols]\n",
    "\n",
    "# Model\n",
    "model_net = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=7) # try to change this also\n",
    "model_net.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = model_net.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] MAE: {mae:.2f}\")\n",
    "print(f\"[TC] RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] RÂ²: {r2:.2f}\")\n",
    "\n",
    "# Param grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "base_model = LGBMRegressor()\n",
    "base_model.set_params(categorical_feature=cat_cols)\n",
    "grid = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_tc = grid.best_estimator_\n",
    "y_pred = best_tc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"[TC] Best Params: {grid.best_params_}\")\n",
    "print(f\"[TC] Tuned MAE: {mae:.2f}\")\n",
    "print(f\"[TC] Tuned RMSE: {rmse:.2f}\")\n",
    "print(f\"[TC] Tuned RÂ²: {r2:.2f}\")\n",
    "\n",
    "print(f\"[TC] Target Std Dev: {y.std()}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_dev = y_test.std()\n",
    "ratio = rmse / std_dev\n",
    "ratio_mean = mae / y.mean()\n",
    " \n",
    "print(f\"[TC] RMSE / SD ratio: {ratio:.2f}\")\n",
    "print(f\"[TC] MAE / mean ratio: {ratio_mean:.2f}\")\n",
    "\n",
    "# Interpretation for RMSE\n",
    "if ratio < 0.5:\n",
    "    print(\"ğŸŸ¢ Excellent RMSE\")\n",
    "elif 0.5 <= ratio < 0.75:\n",
    "    print(\"ğŸŸ¢ Good RMSE\")\n",
    "elif 0.75 <= ratio <= 1.0:\n",
    "    print(\"ğŸŸ¡ Acceptable RMSE, but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor RMSE â€“ model performs worse than mean prediction\")\n",
    "\n",
    "# Interpretation for R-squared\n",
    "if r2 >= 0.9:\n",
    "    print(\"ğŸŸ¢ Excellent RÂ²\")\n",
    "elif 0.75 <= r2 < 0.9:\n",
    "    print(\"ğŸŸ¢ Good RÂ²\")\n",
    "elif 0.5 <= r2 < 0.75:\n",
    "    print(\"ğŸŸ¡ Acceptable RÂ², but can be improved\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Weak RÂ² â€“ model needs improvement\")\n",
    "\n",
    "# Interpretation for MAE\n",
    "if ratio_mean < 0.05:\n",
    "    print(\"ğŸŸ¢ Excellent MAE\")\n",
    "elif 0.05 <= ratio_mean < 0.10:\n",
    "    print(\"ğŸŸ¢ Good MAE\")\n",
    "elif 0.10 <= ratio_mean < 0.20:\n",
    "    print(\"ğŸŸ¡ Acceptable MAE, but can be improved\")\n",
    "elif 0.20 <= ratio_mean <= 0.30:\n",
    "    print(\"ğŸ”´ Weak MAE, high prediction error\")\n",
    "else:\n",
    "    print(\"ğŸ”´ Poor MAE â€“ likely underfitting/noisy data\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sorted DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_tc.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Sorted Feature Importance - TC', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 13) Neural Networks (Feed Forward)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import re\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ### Data preprocessing\n",
    "    y = df[['Net_Amount','TC']].values#\n",
    "\n",
    "    # Label-encode categoricals in place\n",
    "    cat_cols = [\n",
    "        'Store_No','Name','State','Day',\n",
    "        'CODE (subcluster 1)','CODE FY26 1 (subcluster 2)',\n",
    "        'CODE FY26 2 (subcluster 3)','Rain?','Public Holiday'\n",
    "    ]\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"Missing\")\n",
    "    for col in cat_cols:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "    # Define numeric cols (still in df)\n",
    "    numeric_cols = ['Days From Holiday','Puasa Count',\n",
    "        'Days_after_Opening','Average Daily Temperature (Â°C)'\n",
    "    ]\n",
    "\n",
    "    # Split the *full* DataFrame so you keep both cat & num\n",
    "    train_df, test_df, y_train, y_test = train_test_split(\n",
    "        df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit scaler on training numerics, transform both\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train_df[numeric_cols])\n",
    "    X_test_num  = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "    # Extract each categorical column as an array\n",
    "    X_train_cat = [train_df[col].values for col in cat_cols]\n",
    "    X_test_cat  = [test_df[col].values  for col in cat_cols]\n",
    "\n",
    "    # Combine into the list your model expects:\n",
    "    X_train_input = X_train_cat + [X_train_num]\n",
    "    X_test_input  = X_test_cat  + [X_test_num]\n",
    "\n",
    "    ### Building the Embedding + Dense Model\n",
    "    # ========== Define Inputs ==========\n",
    "    n_uniques = {col: df[col].nunique() for col in cat_cols}\n",
    "\n",
    "    def clean(col):\n",
    "        # replace any character that is not Aâ€“Z, aâ€“z, 0â€“9 or underscore with underscore\n",
    "        return re.sub(r'[^A-Za-z0-9_]+', '_', col)             # to change Rain?embed into Rain_embed\n",
    "\n",
    "    cat_inputs = []\n",
    "    cat_embeds = []\n",
    "\n",
    "    for col in cat_cols:\n",
    "        safe = clean(col)\n",
    "        inp = Input(shape=(1,), name=f\"{safe}_in\")\n",
    "        \n",
    "        vocab_size = n_uniques[col]\n",
    "        embed_dim = int(np.log2(vocab_size)) + 1  # adaptive size\n",
    "\n",
    "        emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            name=f\"{safe}_emb\"\n",
    "        )(inp)\n",
    "\n",
    "        flat = Flatten(name=f\"{safe}_flat\")(emb)\n",
    "        cat_inputs.append(inp)\n",
    "        cat_embeds.append(flat)\n",
    "\n",
    "    # Numeric Input\n",
    "    numeric_input = Input(\n",
    "        shape=(X_train_num.shape[1],), \n",
    "        name='numeric_in'\n",
    "    )\n",
    "\n",
    "    # Combine all Keras Inputs\n",
    "    all_inputs = cat_inputs + [numeric_input]\n",
    "\n",
    "    # ========== Concatenate Embeddings + Numeric Inputs ==========\n",
    "    x = Concatenate()(cat_embeds + [numeric_input])\n",
    "\n",
    "    # ========== Updated Network Architecture ==========\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(2)(x)  # Predict Net_Amount and TC\n",
    "\n",
    "    # ========== Build Model ==========\n",
    "    model = Model(inputs=all_inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate= 0.0005),  # Reduced from default 1e-3 if needed\n",
    "        loss='mse',\n",
    "        metrics=['mae'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,       # reduce by half\n",
    "        patience=3,       # wait 3 epochs of no improvement\n",
    "        min_lr=1e-6,      # don't go below this\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ========== Add EarlyStopping ==========\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',         # Watch validation loss\n",
    "        patience=5,                # Stop after 5 bad epochs (no improvement)\n",
    "        min_delta=0.0001,\n",
    "        mode='min',                 \n",
    "        restore_best_weights=True  # Roll back to the best weights seen\n",
    "    )\n",
    "\n",
    "    # Train and capture history for plotting\n",
    "    history = model.fit(\n",
    "        x=X_train_input,\n",
    "        y=y_train,\n",
    "        validation_data=(X_test_input, y_test),\n",
    "        epochs=50,          \n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    ### Evaluation\n",
    "    # 1) Raw evaluation\n",
    "    loss, mae = model.evaluate(X_test_input, y_test, verbose=0)\n",
    "    print(f\"\\nTest MSE: {loss:.2f}, Test MAE: {mae:.2f}\")\n",
    "\n",
    "    # 2) Predictions\n",
    "    predictions = model.predict(X_test_input)\n",
    "    true_net, true_tc = y_test[:, 0], y_test[:, 1]\n",
    "    pred_net, pred_tc = predictions[:, 0], predictions[:, 1]\n",
    "\n",
    "    # 3) Metric helper\n",
    "    def summarize(y_true, y_pred, name):\n",
    "        mse  = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_true, y_pred)\n",
    "        r2   = r2_score(y_true, y_pred)\n",
    "        std  = np.std(y_true)\n",
    "        mean = np.mean(y_true)\n",
    "        print(f\"\\nâ€” {name} â€”\")\n",
    "        print(f\"RMSE : {rmse:.2f} (vs STD {std:.2f})\")\n",
    "        print(f\"MAE  : {mae:.2f} (vs Mean{mean:.2f})\")\n",
    "        print(f\"RÂ²   : {r2:.3f}\")\n",
    "\n",
    "    summarize(true_net, pred_net, \"Net_Amount\")\n",
    "    summarize(true_tc,  pred_tc,  \"TC\")\n",
    "\n",
    "\n",
    "    ### 4) Loss Curve Plot ###\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val   Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118458dc",
   "metadata": {},
   "source": [
    "Changes:\n",
    "1. Putting Net_Amount and TC on the same scale so that MSE loss won't be dominated by larger-magnitude target (Net_Amount)\n",
    "2. Setting two task-specific heads - after a shared trunk the network branches, letting each target learn features that matter only to it, reducing negative interference\n",
    "3. L2 weight regularisation - penalising large weights, discouraging complex co-adaptations and reducing over-fitting\n",
    "4. Lower dropout (0.30) + tighter patience (3) - a good amount of regularisation but allows the model to converge faster + earlier early-stop\n",
    "5. Quicker ReduceLROnPlateau - halves the learning rate after just 2 stagnant epochs, helping the optimiser settle into a better minimum once progress plateaus\n",
    "6. Interpretation - for easy interpretation of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90024ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13) Neural Networks (Feed-Forward) â€“ Improved version with interpretation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (Input, Embedding, Flatten, Concatenate,\n",
    "                                     Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€ 1) Data preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "y = df[['Net_Amount', 'TC']].values\n",
    "\n",
    "cat_cols = ['Store_No','Name','State','Day',\n",
    "            'CODE (subcluster 1)','CODE FY26 1 (subcluster 2)',\n",
    "            'CODE FY26 2 (subcluster 3)','Rain?','Public Holiday']\n",
    "\n",
    "df[cat_cols] = df[cat_cols].fillna(\"Missing\")\n",
    "for c in cat_cols:\n",
    "    df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n",
    "\n",
    "numeric_cols = ['Days From Holiday','Puasa Count',\n",
    "                'Days_after_Opening','Average Daily Temperature (Â°C)']\n",
    "\n",
    "train_df, test_df, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_num = scaler_X.fit_transform(train_df[numeric_cols])\n",
    "X_test_num  = scaler_X.transform(test_df[numeric_cols])\n",
    "\n",
    "X_train_cat = [train_df[c].values for c in cat_cols]\n",
    "X_test_cat  = [test_df[c].values  for c in cat_cols]\n",
    "\n",
    "scaler_y       = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled  = scaler_y.transform(y_test)\n",
    "\n",
    "# â”€â”€ 2) Model definition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def clean(col): return re.sub(r'[^A-Za-z0-9_]+', '_', col)\n",
    "\n",
    "cat_inputs, cat_embeds = [], []\n",
    "n_uniques = {c: df[c].nunique() for c in cat_cols}\n",
    "\n",
    "for col in cat_cols:\n",
    "    safe     = clean(col)\n",
    "    inp      = Input(shape=(1,), name=f\"{safe}_in\")\n",
    "    emb_dim  = int(np.log2(n_uniques[col])) + 1\n",
    "    emb_layer= Embedding(input_dim=n_uniques[col],\n",
    "                         output_dim=emb_dim,\n",
    "                         name=f\"{safe}_emb\")(inp)\n",
    "    cat_inputs.append(inp)\n",
    "    cat_embeds.append(Flatten(name=f\"{safe}_flat\")(emb_layer))\n",
    "\n",
    "numeric_input = Input(shape=(X_train_num.shape[1],), name='numeric_in')\n",
    "all_inputs    = cat_inputs + [numeric_input]\n",
    "x = Concatenate()(cat_embeds + [numeric_input])\n",
    "\n",
    "# shared trunk\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.30)(x)\n",
    "x = Dense(64, activation='relu',  kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "# â”€â”€ split into two taskâ€specific heads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "h1      = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "out_net = Dense(1, name='net')(h1)\n",
    "h2      = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "out_tc  = Dense(1, name='tc')(h2)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=[out_net, out_tc])\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-4),\n",
    "    loss   = ['mse','mse'],    # one per output\n",
    "    metrics= ['mae','mae']\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor   ='val_loss',\n",
    "    factor    =0.5,\n",
    "    patience  =2,\n",
    "    min_lr    =1e-6,\n",
    "    verbose   =1\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor            ='val_loss',\n",
    "    patience           =3,\n",
    "    restore_best_weights=True,\n",
    "    verbose            =1\n",
    ")\n",
    "\n",
    "# â”€â”€ 3) Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train_input = X_train_cat + [X_train_num]\n",
    "X_test_input  = X_test_cat  + [X_test_num]\n",
    "\n",
    "history = model.fit(\n",
    "    x             = X_train_input,\n",
    "    y             = {'net': y_train_scaled[:,0], 'tc': y_train_scaled[:,1]},\n",
    "    validation_data=(X_test_input,\n",
    "                     {'net': y_test_scaled[:,0], 'tc': y_test_scaled[:,1]}),\n",
    "    epochs        = 50,\n",
    "    batch_size    = 32,\n",
    "    callbacks     = [early_stop, reduce_lr],\n",
    "    verbose       = 1\n",
    ")\n",
    "\n",
    "# â”€â”€ 4) Predictions & inverseâ€scale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaled_preds  = model.predict(X_test_input)\n",
    "preds_matrix  = np.hstack(scaled_preds)\n",
    "predictions   = scaler_y.inverse_transform(preds_matrix)\n",
    "true_net, true_tc = y_test[:,0], y_test[:,1]\n",
    "pred_net, pred_tc = predictions[:,0], predictions[:,1]\n",
    "\n",
    "# â”€â”€ 5) Summary + interpretation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def interpret_and_print(y_true, y_pred, name):\n",
    "    mse         = mean_squared_error(y_true, y_pred)\n",
    "    rmse        = np.sqrt(mse)\n",
    "    mae         = mean_absolute_error(y_true, y_pred)\n",
    "    r2          = r2_score(y_true, y_pred)\n",
    "    std_y, mean = np.std(y_true), np.mean(y_true)\n",
    "    mae_ratio   = mae/mean\n",
    "    rmse_ratio  = rmse/std_y\n",
    "\n",
    "    # Header\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"MAE  : {mae:8.2f}   (mean = {mean:8.2f}, MAE/mean = {mae_ratio:.2f})\")\n",
    "    print(f\"RMSE : {rmse:8.2f}   (std  = {std_y:8.2f}, RMSE/std  = {rmse_ratio:.2f})\")\n",
    "    print(f\"RÂ²   : {r2:8.3f}\")\n",
    "\n",
    "    # MAEâ€toâ€mean\n",
    "    if   mae_ratio < 0.10: print(\"ğŸ”µ Excellent MAE (<10% of mean)\")\n",
    "    elif mae_ratio < 0.20: print(\"ğŸŸ¢ Good MAE     (<20% of mean)\")\n",
    "    elif mae_ratio < 0.30: print(\"ğŸŸ¡ Acceptable MAE(<30% of mean)\")\n",
    "    else:                  print(\"ğŸ”´ Poor MAE     (>30% of mean)\")\n",
    "\n",
    "    # RMSEâ€toâ€std\n",
    "    if   rmse_ratio < 0.50: print(\"ğŸ”µ Excellent RMSE (<0.5 Ïƒ)\")\n",
    "    elif rmse_ratio < 0.75: print(\"ğŸŸ¢ Good RMSE     (<0.75 Ïƒ)\")\n",
    "    elif rmse_ratio < 1.00: print(\"ğŸŸ¡ Acceptable RMSE(<1.0 Ïƒ)\")\n",
    "    else:                  print(\"ğŸ”´ Poor RMSE     (>1.0 Ïƒ)\")\n",
    "\n",
    "    # RÂ²\n",
    "    if   r2 >= 0.90:        print(\"ğŸ”µ Excellent RÂ² (â‰¥0.90)\")\n",
    "    elif r2 >= 0.75:        print(\"ğŸŸ¢ Good RÂ²      (0.75â€“0.90)\")\n",
    "    elif r2 >= 0.50:        print(\"ğŸŸ¡ Acceptable RÂ²(0.50â€“0.75)\")\n",
    "    else:                   print(\"ğŸ”´ Weak RÂ²      (<0.50)\")\n",
    "\n",
    "# Run for each target\n",
    "interpret_and_print(true_net, pred_net, \"Net_Amount\")\n",
    "interpret_and_print(true_tc,  pred_tc,  \"TC\")\n",
    "\n",
    "# â”€â”€ 6) Lossâ€curve plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.plot(history.history['loss'],  label='Train Loss')\n",
    "plt.plot(history.history['val_loss'],label='Val   Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Scaled MSE Loss')\n",
    "plt.legend(); plt.title('Training vs Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
