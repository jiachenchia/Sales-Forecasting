{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2656b86a",
   "metadata": {},
   "source": [
    "All variables in input + MLP (which is my best model - but changing my best model parameters to Optuna 2 (without all variables in input without MLP) params \n",
    "                              since better than Optuna 1 and almost as good as all variables in input with MLP)\n",
    "\n",
    "{'window': 7, 'n_layers': 4, 'lstm_units': 128, 'dropout_rate': 0.2, 'static_dense': 128, 'learning_rate': 0.0005728715834481826, 'batch_size': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Loading libraries and datasets, and set up data\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (Input, LSTM, Embedding, Flatten, Dense, Concatenate, SpatialDropout1D, BatchNormalization, Dropout, Add, RepeatVector)\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Cluster, PH, Weather, Sales data\n",
    "df = pd.read_csv(r\" ... csv\")\n",
    "\n",
    "# Feature columns\n",
    "time_varying_categorical_cols = ['Rain?','Name','Puasa','Public Holiday','Day','Month']  # varies with time\n",
    "static_categorical_cols = ['Store_No','State','CODE (subcluster 1)','CODE FY26 1 (subcluster 2)','CODE FY26 2 (subcluster 3)']  # won't change depending on time\n",
    "categorical_cols = time_varying_categorical_cols + static_categorical_cols\n",
    "numeric_cols = ['Net_Amount','TC','Days_after_Opening','Average Daily Temperature (Â°C)']\n",
    "\n",
    "# Filling in spaces to be encoded: 'CODE (subcluster 1)': blank, Name: no PH, Puasa: 0, Public Holiday: 0\n",
    "# Filling empty spaces\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].fillna('blank')\n",
    "df['Name']               = df['Name'].fillna('no PH')\n",
    "df['Puasa']              = df['Puasa'].fillna(0)\n",
    "df['Public Holiday']     = df['Public Holiday'].fillna(0)\n",
    "\n",
    "# Replace any emptyâ€string entries (''), if they exist\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].replace('', 'blank')\n",
    "df['Name']               = df['Name'].replace('', 'no PH')\n",
    "df['Puasa']              = df['Puasa'].replace('', 0)\n",
    "df['Public Holiday']     = df['Public Holiday'].replace('', 0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Encoding categorical columns\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Splitting categorical columns into 2 types - to be embedded or be binary\n",
    "embed_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_uniques = df[col].nunique()\n",
    "    if col in static_categorical_cols:\n",
    "        embed_cols.append(col)\n",
    "    elif n_uniques < 3:\n",
    "        pass\n",
    "    elif n_uniques >= 7:\n",
    "        embed_cols.append(col)\n",
    "    else:\n",
    "        print(f\"Column {col} has {n_uniques} categories: choose manually.\")\n",
    "\n",
    "# Encoding embed columns\n",
    "encoders = {}\n",
    "for col in embed_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_enc'] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Binary columns - Puasa and Public Holiday are already in 0s and 1s\n",
    "df['Rain?'] = df['Rain?'].map({'Yes': 1, ' No': 0})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Scaling continuous features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Building sequences for LSTM\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# purely numeric or binary (already scaled / mapped 0/1)\n",
    "time_numeric_cols = [\n",
    "    'Net_Amount',\n",
    "    'TC',\n",
    "    'Days_after_Opening',\n",
    "    'Average Daily Temperature (Â°C)',\n",
    "    'Rain?',          # make sure this is 0/1\n",
    "    'Puasa',          # 0/1\n",
    "    'Public Holiday'  # 0/1\n",
    "]\n",
    "\n",
    "# select window size 7-30 days\n",
    "window = 7\n",
    "\n",
    "X_num, X_name, X_day, X_month, X_stat, y = [], [], [], [], [], []\n",
    "\n",
    "static_cols = [col + '_enc' for col in static_categorical_cols if col in embed_cols]\n",
    "\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T   = len(grp)\n",
    "    if T <= window: \n",
    "        continue\n",
    "\n",
    "    # raw arrays\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(T - window):\n",
    "        X_num.append(   arr_num[i : i+window]     )  # â†’ (window, n_num_feats)\n",
    "        X_name.append(  arr_name[i : i+window]    )  # â†’ (window,)\n",
    "        X_day.append(   arr_day[i : i+window]     )\n",
    "        X_month.append(arr_month[i : i+window]    )\n",
    "        X_stat.append(  arr_static[i]             )  # â†’ (n_static_feats,)\n",
    "        y.append(       arr_target[i]             )  # â†’ scalar\n",
    "\n",
    "# stack into arrays\n",
    "X_num    = np.stack(X_num)    # (N, window, n_num_feats)\n",
    "X_name   = np.stack(X_name)   # (N, window)\n",
    "X_day    = np.stack(X_day)    # (N, window)\n",
    "X_month  = np.stack(X_month)  # (N, window)\n",
    "X_stat   = np.stack(X_stat)   # (N, n_static_feats)\n",
    "y        = np.array(y)        # (N,)\n",
    "\n",
    "print(X_num.shape, X_name.shape, X_day.shape, X_month.shape, X_stat.shape, y.shape)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Time-aware train-validation data split\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Initialize empty Python lists to hold your training vs. validation sequences:\n",
    "X_num_train, X_num_val, \\\n",
    "X_name_train, X_name_val, \\\n",
    "X_day_train,  X_day_val,  \\\n",
    "X_month_train,X_month_val,\\\n",
    "X_stat_train, X_stat_val, \\\n",
    "y_train,      y_val = ([] for _ in range(12))\n",
    "\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T = len(grp)\n",
    "    n_windows = T - window\n",
    "    if n_windows <= 0: continue\n",
    "\n",
    "    # how many windows to train on for this store\n",
    "    split_store = int(0.8 * n_windows)\n",
    "\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        num_seq  = arr_num[i : i+window]\n",
    "        name_seq = arr_name[i : i+window]\n",
    "        day_seq  = arr_day[i : i+window]\n",
    "        month_seq = arr_month[i : i+window]\n",
    "        static = arr_static[i]\n",
    "        target   = arr_target[i]\n",
    "        if i < split_store:\n",
    "            X_num_train.append(num_seq)\n",
    "            X_name_train.append(name_seq)\n",
    "            X_day_train.append(day_seq)\n",
    "            X_month_train.append(month_seq)\n",
    "            X_stat_train.append(static)\n",
    "            y_train.append(target)\n",
    "        else:\n",
    "            X_num_val.append(num_seq)\n",
    "            X_name_val.append(name_seq)\n",
    "            X_day_val.append(day_seq)\n",
    "            X_month_val.append(month_seq)\n",
    "            X_stat_val.append(static)\n",
    "            y_val.append(target)\n",
    "\n",
    "# finally stack\n",
    "X_num_train = np.stack(X_num_train)\n",
    "X_name_train = np.stack(X_name_train)\n",
    "X_day_train = np.stack(X_day_train)\n",
    "X_month_train = np.stack(X_month_train)\n",
    "X_stat_train = np.stack(X_stat_train)\n",
    "y_train     = np.stack(y_train)\n",
    "\n",
    "X_stat_train = np.array(X_stat_train)\n",
    "X_stat_val   = np.array(X_stat_val)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6.2  Same as (1) but with a 2-layer residual MLP on statics\n",
    "# ------------------------------------------------------------\n",
    "def build_sales_lstm(\n",
    "        W, F, time_cardinalities, static_cardinalities,\n",
    "        lstm_units   = 128,         # you can tune these\n",
    "        dropout_rate = 0.2,\n",
    "        static_dense = 128,\n",
    "        learning_rate= 0.005728715834481826,\n",
    "):\n",
    "    \n",
    "# --- time-series inputs ---------------------------------------------------\n",
    "    num_in       = Input(shape=(W, F),           name='num_in')\n",
    "    name_seq_in  = Input(shape=(W,), dtype='int32', name='name_seq_in')\n",
    "    day_seq_in   = Input(shape=(W,), dtype='int32', name='day_seq_in')\n",
    "    month_seq_in = Input(shape=(W,), dtype='int32', name='month_seq_in')\n",
    "\n",
    "    dim = lambda n: min(100, n // 2 + 5)\n",
    "\n",
    "    name_emb  = SpatialDropout1D(dropout_rate)(\n",
    "                   Embedding(time_cardinalities['name'],  dim(time_cardinalities['name']))(name_seq_in))\n",
    "    day_emb   = SpatialDropout1D(dropout_rate)(\n",
    "                   Embedding(time_cardinalities['day'],   dim(time_cardinalities['day']))(day_seq_in))\n",
    "    month_emb = SpatialDropout1D(dropout_rate)(\n",
    "                   Embedding(time_cardinalities['month'], dim(time_cardinalities['month']))(month_seq_in))\n",
    "\n",
    "    time_concat = Concatenate(axis=-1)([num_in, name_emb, day_emb, month_emb])\n",
    "\n",
    "    # --- static branch: **deeper residual** ----------------------------------\n",
    "    static_inputs, static_vecs = [], []\n",
    "    for base, vocab in static_cardinalities:\n",
    "        s_in = Input(shape=(1,), dtype='int32', name=f'{base}_in')  # âœ… correct\n",
    "        s_emb = Embedding(vocab, dim(vocab))(s_in)\n",
    "        s_emb = Flatten()(s_emb)\n",
    "        static_inputs.append(s_in)\n",
    "        static_vecs.append(s_emb)\n",
    "\n",
    "    static_ctx = Concatenate()(static_vecs)             # concat raw embeddings\n",
    "\n",
    "    # MLP block 1\n",
    "    h = Dense(static_dense, 'relu')(static_ctx)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Dropout(dropout_rate)(h)\n",
    "    # MLP block 2 with RESIDUAL skip\n",
    "    h2 = Dense(static_dense, 'relu')(h)\n",
    "    h2 = BatchNormalization()(h2)\n",
    "    h2 = Dropout(dropout_rate)(h2)\n",
    "    h  = Add()([h, h2])                                # residual connection\n",
    "\n",
    "    # final compressed static context\n",
    "    static_ctx = Dense(static_dense//2, 'relu')(h)\n",
    "    static_ctx = BatchNormalization()(static_ctx)\n",
    "\n",
    "    # ---- tile & state-init exactly like 6.1 ---------------------------------\n",
    "    static_tile = RepeatVector(W)(static_ctx)\n",
    "    time_concat = Concatenate(-1)([time_concat, static_tile])\n",
    "\n",
    "    init_h = Dense(lstm_units, 'tanh')(static_ctx)\n",
    "    init_c = Dense(lstm_units, 'tanh')(static_ctx)\n",
    "\n",
    "    static_tile = RepeatVector(W)(static_ctx)\n",
    "    time_concat = Concatenate(-1)([time_concat, static_tile])\n",
    "\n",
    "    init_h = Dense(lstm_units, 'tanh')(static_ctx)\n",
    "    init_c = Dense(lstm_units, 'tanh')(static_ctx)\n",
    "\n",
    "    # â€”â€” now a 4-layer LSTM stack â€”â€” \n",
    "    # Layer 1, with initial state:\n",
    "    x = LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        dropout=dropout_rate,\n",
    "        )(time_concat, initial_state=[init_h, init_c])\n",
    "\n",
    "    # Layer 2\n",
    "    x = LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        dropout=dropout_rate,\n",
    "        )(x)\n",
    "\n",
    "    # Layer 3\n",
    "    x = LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        dropout=dropout_rate,\n",
    "        )(x)\n",
    "\n",
    "    # Layer 4 (final), no return_sequences so it collapses to (batch, units)\n",
    "    x = LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=False,\n",
    "        dropout=dropout_rate,\n",
    "        )(x)\n",
    "\n",
    "    # now your normalization, dropout, dense, etc.\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    out = Dense(2, 'linear')(x) \n",
    "\n",
    "    model = Model(\n",
    "        inputs=[num_in, name_seq_in, day_seq_in, month_seq_in] + static_inputs,\n",
    "        outputs=out, name='Sales_LSTM_TiledResidual')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare cardinalities for embeddings\n",
    "n_name  = df['Name_enc' ].nunique()\n",
    "n_day   = df['Day_enc'  ].nunique()\n",
    "n_month = df['Month_enc'].nunique()\n",
    "time_cardinalities = {'name': n_name, 'day': n_day, 'month': n_month}\n",
    "\n",
    "static_cardinalities = []\n",
    "for enc_col in static_cols:                      # e.g. \"Store_No_enc\"\n",
    "    base  = enc_col.replace('_enc','')           # \"Store_No\"\n",
    "    vocab = df[enc_col].nunique()\n",
    "    static_cardinalities.append((base, vocab))\n",
    "\n",
    "# window & feature count\n",
    "_, W, F = X_num.shape\n",
    "\n",
    "# Cast *all* arrays to proper dtype and shape\n",
    "X_num_train   = np.asarray(X_num_train,   dtype=np.float32)\n",
    "X_num_val     = np.asarray(X_num_val,     dtype=np.float32)\n",
    "\n",
    "def to_int2d(arr): return np.asarray(arr, dtype=np.int32)\n",
    "X_name_train  = to_int2d(X_name_train)\n",
    "X_day_train   = to_int2d(X_day_train)\n",
    "X_month_train = to_int2d(X_month_train)\n",
    "X_name_val    = to_int2d(X_name_val)\n",
    "X_day_val     = to_int2d(X_day_val)\n",
    "X_month_val   = to_int2d(X_month_val)\n",
    "\n",
    "X_stat_train  = np.asarray(X_stat_train, dtype=np.int32)\n",
    "X_stat_val    = np.asarray(X_stat_val,   dtype=np.int32)\n",
    "y_train       = np.asarray(y_train,      dtype=np.float32)\n",
    "y_val         = np.asarray(y_val,        dtype=np.float32)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Fit Model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Build input dicts (by name â”€ safest)\n",
    "train_inputs = {\n",
    "    'num_in':       X_num_train,\n",
    "    'name_seq_in':  X_name_train,\n",
    "    'day_seq_in':   X_day_train,\n",
    "    'month_seq_in': X_month_train,\n",
    "}\n",
    "val_inputs   = {\n",
    "    'num_in':       X_num_val,\n",
    "    'name_seq_in':  X_name_val,\n",
    "    'day_seq_in':   X_day_val,\n",
    "    'month_seq_in': X_month_val,\n",
    "}\n",
    "\n",
    "# add static columns (need reshapeÂ (N,1))\n",
    "for i, (base, _) in enumerate(static_cardinalities):\n",
    "    train_inputs[f'{base}_in'] = X_stat_train[:, i].reshape(-1,1)\n",
    "    val_inputs  [f'{base}_in'] = X_stat_val[:,   i].reshape(-1,1)\n",
    "\n",
    "# Sanity check: print expected vs actual - make sure ValueError: Functional() don't pop up\n",
    "print(\"\\nâ”€â”€ EXPECTED  vs  ACTUAL perâ€‘sample shapes â”€â”€\")\n",
    "tmp_model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "for inp in tmp_model.inputs:\n",
    "    k   = inp.name.split(':')[0]\n",
    "    exp = tuple(inp.shape[1:])\n",
    "    act = tuple(train_inputs[k].shape[1:])\n",
    "    ok  = \"âœ…\" if exp == act else \"âŒ\"\n",
    "    print(f\"{k:25s} expected={str(exp):12s} actual={str(act):12s} {ok}\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")\n",
    "\n",
    "# Instantiate final model & train\n",
    "model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "es   = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, rlrp],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.plot(history.history['loss'],     label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8.  Evaluate on a holdâ€‘out test set and generate forecasts\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "# Build a fresh 70/15/15 split Train/Val/Test (timeâ€‘aware per store) \n",
    "def time_aware_split_by_store(df, window, time_numeric_cols, static_cols):\n",
    "    X_num_tr, X_num_va, X_num_te = [], [], []\n",
    "    X_name_tr, X_name_va, X_name_te = [], [], []\n",
    "    X_day_tr,  X_day_va,  X_day_te  = [], [], []\n",
    "    X_month_tr,X_month_va,X_month_te= [], [], []\n",
    "    X_stat_tr, X_stat_va, X_stat_te = [], [], []\n",
    "    y_tr,      y_va,      y_te      = [], [], []\n",
    "\n",
    "    for _, grp in df.groupby('Store_No'):\n",
    "        grp = grp.sort_values('Date')\n",
    "        T = len(grp)\n",
    "        n_windows = T - window\n",
    "        if n_windows <= 0: continue\n",
    "\n",
    "        n_train = int(0.70 * n_windows)\n",
    "        n_val   = int(0.15 * n_windows)\n",
    "        # remainder â†’ test\n",
    "        n_test  = n_windows - n_train - n_val\n",
    "\n",
    "        arr_num    = grp[time_numeric_cols].values\n",
    "        arr_name   = grp['Name_enc' ].values\n",
    "        arr_day    = grp['Day_enc'  ].values\n",
    "        arr_month  = grp['Month_enc'].values\n",
    "        arr_static = grp[static_cols].iloc[window:].values\n",
    "        arr_target = grp[['Net_Amount','TC']].values[window:]\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            seq_num   = arr_num[i:i+window]\n",
    "            seq_name  = arr_name[i:i+window]\n",
    "            seq_day   = arr_day[i:i+window]\n",
    "            seq_month = arr_month[i:i+window]\n",
    "            stat_vec  = arr_static[i]\n",
    "            target    = arr_target[i]\n",
    "\n",
    "            # buckets\n",
    "            if i < n_train:\n",
    "                bucket = (X_num_tr,  X_name_tr,  X_day_tr,  X_month_tr,  X_stat_tr,  y_tr)\n",
    "            elif i < n_train + n_val:\n",
    "                bucket = (X_num_va,  X_name_va,  X_day_va,  X_month_va,  X_stat_va,  y_va)\n",
    "            else:\n",
    "                bucket = (X_num_te,  X_name_te,  X_day_te,  X_month_te,  X_stat_te,  y_te)\n",
    "\n",
    "            bucket[0].append(seq_num)\n",
    "            bucket[1].append(seq_name)\n",
    "            bucket[2].append(seq_day)\n",
    "            bucket[3].append(seq_month)\n",
    "            bucket[4].append(stat_vec)\n",
    "            bucket[5].append(target)\n",
    "\n",
    "    # stack to np arrays\n",
    "    def _stack(lst): return np.stack(lst) if lst and isinstance(lst[0], np.ndarray) else np.array(lst)\n",
    "    return tuple(map(_stack,\n",
    "        [X_num_tr,X_num_va,X_num_te,\n",
    "         X_name_tr,X_name_va,X_name_te,\n",
    "         X_day_tr,X_day_va,X_day_te,\n",
    "         X_month_tr,X_month_va,X_month_te,\n",
    "         X_stat_tr,X_stat_va,X_stat_te,\n",
    "         y_tr,y_va,y_te]))\n",
    "\n",
    "# build split\n",
    "(split_X_num_tr, split_X_num_va, split_X_num_te,\n",
    " split_X_name_tr, split_X_name_va, split_X_name_te,\n",
    " split_X_day_tr,  split_X_day_va,  split_X_day_te,\n",
    " split_X_month_tr,split_X_month_va,split_X_month_te,\n",
    " split_X_stat_tr, split_X_stat_va, split_X_stat_te,\n",
    " split_y_tr,      split_y_va,      split_y_te) = time_aware_split_by_store(\n",
    "    df, window, time_numeric_cols, static_cols\n",
    ")\n",
    "\n",
    "# Assemble TEST input dict \n",
    "def build_input_dict(X_num,X_name,X_day,X_month,X_stat):\n",
    "    d = {\n",
    "        'num_in':       X_num.astype(np.float32),\n",
    "        'name_seq_in':  X_name.astype(np.int32),\n",
    "        'day_seq_in':   X_day.astype(np.int32),\n",
    "        'month_seq_in': X_month.astype(np.int32)\n",
    "    }\n",
    "    for i,(base,_) in enumerate(static_cardinalities):\n",
    "        d[f'{base}_in'] = X_stat[:,i].reshape(-1,1).astype(np.int32)\n",
    "    return d\n",
    "\n",
    "test_inputs = build_input_dict(\n",
    "    split_X_num_te, split_X_name_te, split_X_day_te,\n",
    "    split_X_month_te, split_X_stat_te)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(test_inputs, verbose=0)\n",
    "\n",
    "# Inverseâ€‘scale Net_Amount & TC\n",
    "# remember scaler was fit on numeric_cols, where Net_Amount is col 0 and TC is col 1\n",
    "# weâ€™ll invert only those two columns\n",
    "net_idx = numeric_cols.index('Net_Amount')\n",
    "tc_idx  = numeric_cols.index('TC')\n",
    "\n",
    "def inverse_scale(scaled_vec, col_idx):\n",
    "    tmp      = np.zeros((len(scaled_vec), len(numeric_cols)))\n",
    "    tmp[:,col_idx] = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:,col_idx]\n",
    "\n",
    "net_pred = inverse_scale(y_pred_scaled[:,0], net_idx)\n",
    "tc_pred  = inverse_scale(y_pred_scaled[:,1], tc_idx)\n",
    "\n",
    "net_true = inverse_scale(split_y_te[:,0], net_idx)\n",
    "tc_true  = inverse_scale(split_y_te[:,1], tc_idx)\n",
    "\n",
    "# Metrics & Interpretation\n",
    "def interpret(true, pred, name):\n",
    "    # core metrics\n",
    "    mae  = mean_absolute_error(true, pred)\n",
    "    rmse = math.sqrt(mean_squared_error(true, pred))\n",
    "    r2   = r2_score(true, pred)\n",
    "\n",
    "    # reference stats\n",
    "    mean_y = np.mean(true)\n",
    "    std_y  = np.std(true)\n",
    "\n",
    "    # relative ratios\n",
    "    mae_ratio  = mae  / mean_y  if mean_y else float('nan')\n",
    "    rmse_ratio = rmse / std_y   if std_y  else float('nan')\n",
    "\n",
    "    # header\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\" MAE : {mae:8.2f}   (mean = {mean_y:8.2f}, MAE/mean = {mae_ratio:.2f})\")\n",
    "    print(f\" RMSE: {rmse:8.2f}   (std  = {std_y:8.2f}, RMSE/std  = {rmse_ratio:.2f})\")\n",
    "    print(f\" RÂ²  : {r2:6.3f}\")\n",
    "\n",
    "    # interpret MAEâ€‘toâ€‘mean\n",
    "    if mae_ratio < 0.10:\n",
    "        print(\"   ğŸ”µ Excellent MAE (<10% of mean)\")\n",
    "    elif mae_ratio < 0.20:\n",
    "        print(\"   ğŸŸ¢ Good MAE (<20% of mean)\")\n",
    "    elif mae_ratio < 0.30:\n",
    "        print(\"   ğŸŸ¡ Acceptable MAE (<30% of mean)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Poor MAE (>30% of mean)\")\n",
    "\n",
    "    # interpret RMSEâ€‘toâ€‘std\n",
    "    if rmse_ratio < 0.50:\n",
    "        print(\"   ğŸ”µ Excellent RMSE (<0.5â€¯Ïƒ)\")\n",
    "    elif rmse_ratio < 0.75:\n",
    "        print(\"   ğŸŸ¢ Good RMSE (<0.75â€¯Ïƒ)\")\n",
    "    elif rmse_ratio < 1.00:\n",
    "        print(\"   ğŸŸ¡ Acceptable RMSE (<1.0â€¯Ïƒ)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Poor RMSE (>1.0â€¯Ïƒ)\")\n",
    "\n",
    "    # interpret RÂ²\n",
    "    if r2 >= 0.90:\n",
    "        print(\"   ğŸ”µ Excellent RÂ² (â‰¥0.90)\")\n",
    "    elif r2 >= 0.75:\n",
    "        print(\"   ğŸŸ¢ Good RÂ² (0.75â€“0.90)\")\n",
    "    elif r2 >= 0.50:\n",
    "        print(\"   ğŸŸ¡ Acceptable RÂ² (0.50â€“0.75)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Weak RÂ² (<0.50)\")\n",
    "\n",
    "# run it for both targets\n",
    "interpret(net_true, net_pred, \"Net_Amount\")\n",
    "interpret(tc_true,  tc_pred,  \"TC\")\n",
    "\n",
    "# Visual check for Net_Amount\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(net_true, net_pred, alpha=.4)\n",
    "ax[0].plot([net_true.min(), net_true.max()],\n",
    "           [net_true.min(), net_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True Net_Amount\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"Net_Amount â€“ scatter\")\n",
    "\n",
    "ax[1].plot(net_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(net_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"Net_Amount â€“ first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Visual check for TC\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(tc_true, tc_pred, alpha=.4)\n",
    "ax[0].plot([tc_true.min(), tc_true.max()],\n",
    "           [tc_true.min(), tc_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True TC\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"TC â€“ scatter\")\n",
    "\n",
    "ax[1].plot(tc_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(tc_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"TC â€“ first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
