# Sales-Forecasting-Project

 --- 

# 4 Main Projects and Takeaways:

1. [ğŸ“ˆ Sales Forecasting with Multi-Input LSTM](#sales-forecasting-lstm)
2. [Daily Sales Supervised ML Studies](#daily-sales-supervised-ml-studies)
3. [Daily Sales and Weather Studies](#daily-sales-weather)
4. [Hourly Sales & Weather Studies](#hourly-sales-weather)
5. [What I Built & Learned (Summary)](#summary)

---

<a id="sales-forecasting-lstm"></a>
# ğŸ“ˆ Sales Forecasting with Multi-Input LSTM

> **Accurate 14â€‘day window sales & transaction forecasts for stores, powered by TensorFlow/Keras, categorical embeddings & residual static context.**

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#lstm-features)
2. [Data](#lstm-data)
3. [Environment/Setup](#lstm-env-setup)
4. [Reproducing the Results](#lstm-reproducing)
5. [Training](#lstm-training)
6. [Evaluation & Metrics](#lstm-eval)
7. [Forecasting & Visualisation](#lstm-forecasting)
8. [Project Structure](#lstm-structure)

---

<a name="lstm-features"></a>

## Features

* **Hybrid input:** numeric weather & sales timeâ€‘series, binary flags **and** categorical embeddings (store no, state and subcluster codes).
* **Residual static context:** 2â€‘layer MLP with skipâ€‘connection broadcasts static store attributes into every LSTM timestep.
* **Timeâ€‘aware split per store:** avoids train/val/test leakage across dates.
* **Permutation feature importance** for both *NetÂ Amount* and *Transaction Count* targets.
* **200â€‘step autoregressive forecasting** & optional 21â€‘day perâ€‘store interactive dashboard (Plotly + ipywidgets).
* **EarlyStopping** + **ReduceLROnPlateau** for robust convergence.
* **Reproducible:** single script / notebook endâ€‘toâ€‘end, explicit random seed control.

<a name="lstm-data"></a>

## Data

| Column type                  | Example columns                                                            | Notes                                     |
| ---------------------------- | -------------------------------------------------------------------------- | ----------------------------------------- |
| **Timeâ€‘varying categorical** | `Name`, `Day`, `Month`                                                     | Encoded & *embedded* if >â€¯6 unique values |
| **Static categorical**       | `Store_No`, `State`, `SubCluster`                                          | Embedded, then tiled across timesteps     |
| **Binary flags**             | `Rain?`, `Puasa`, `PublicÂ Holiday`                                         | Mapped to 0/1                             |
| **Numeric**                  | `Net_Amount`, `TC`, `Days_after_Opening`, `AverageÂ DailyÂ Temperatureâ€¯(Â°C)` | Minâ€‘Max scaled 0â€‘1                        |

---

<a name="lstm-env-setup"></a>

## Environment &Â Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

```
pandas>=1.5
numpy>=1.23
matplotlib>=3.7     
scikit-learn>=1.3
tensorflow==2.15.0   
keras-tuner>=1.4.2
plotly>=5.18
ipywidgets>=8.1

```

</details>

---

<a name="lstm-reproducing"></a>

## Reproducing the Results

**Match software versions** â€“ see `requirements.txt`.
**Make sure that the data file is correct**.

---

<a name="lstm-training"></a>

## Training

Key hyperâ€‘parameters:

| Flag             | Default | Description                              |
| ---------------- | ------- | ---------------------------------------- |
| `--window`       | 14      | Lookâ€‘back window length (days)           |
| `--lstm_units`   | 64      | Hidden size of LSTM layer                |
| `--static_dense` | 128     | Width of residual MLP on static features |
| `--dropout`      | 0.25    | Dropout rate everywhere                  |
| `--lr`           | 1eâ€‘3    | Initial learning rate                    |

---

<a name="lstm-eval"></a>

## EvaluationÂ &Â Metrics

After training, the script prints:

* **MAE / RMSE /Â RÂ²** for *Net\_Amount* and *TC*,
  plus intuitive qualitative bands (ğŸ”µÂ Excellent | ğŸŸ¢Â Good | ğŸŸ¡Â Okay | ğŸ”´Â Poor).
* PNG plots: loss curves, parity scatter, firstâ€‘200â€‘window overlay.

---

<a name="lstm-forecasting"></a>

## ForecastingÂ &Â Visualisation

* **200â€‘step autoregressive demo**
* **21â€‘day perâ€‘store dashboard** with Plotly + dropdown widget
* Optional **95â€¯% CI** ribbons (constant Ïƒ derived from residuals)

---

<a name="lstm-structure"></a>

## Project Structure

```
.
â”œâ”€â”€ data/                     # to obtain data for LSTM
â”‚   â””â”€â”€ Data for LSTM Model FINAL.ipynb
â”œâ”€â”€ models/                   # trained model (.h5) saved after training
â”‚   â””â”€â”€ Sales_Forecasting_LSTM_Model.h5
â”œâ”€â”€ notebooks/               
â”‚   â”œâ”€â”€ LSTM Final Model.ipynb    
â”œâ”€â”€ README.md                 
.

```

<a id="daily-sales-supervised-ml-studies"></a>
# Daily Sales Supervised ML Studies

> Multimodel experiments (PolynomialÂ Regression, XGBoost, LightGBM & Feedâ€‘ForwardÂ NN) to predict **Daily NetÂ Amount (RM)** and **Transaction Count (TC)** from store data, publicâ€‘holiday data and local weather data.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Data Preparation](#ml-data-preparation)
2. [Baseline: Polynomial Regression](#ml-baseline)
3. [Tree Boosting](#ml-tree-boosting)

   * 3.1 [XGBoost](#ml-xgboost)
   * 3.2 [LightGBM â€“ four drafts](#ml-lightgbm)
4. [Feed-Forward Neural Network](#ml-ffnn)
5. [Training & Hyper-parameters](#ml-training)
6. [Evaluation Metrics](#ml-eval)
7. [Environment / Setup](#ml-env)
8. [Reproducing the Results](#ml-reproducing)
9. [Project Structure](#ml-structure)

---

<a name="ml-data-preparation"></a>

## DataÂ Preparation

* **Categoricals** (labelâ€‘encoded):

  | Column                       | Description                                       |
  | ---------------------------- | ------------------------------------------------- |
  | `Store_No`                   | Unique store number                               |
  | `Name`                       | Holiday name                                      |
  | `State`                      | Malaysian state                                   |
  | `Day`                        | Dayâ€‘ofâ€‘week string (Monâ€¦Sun)                      |
  | `CODEÂ (subclusterÂ 1)`        | Store subcluster 1                                |
  | `CODEÂ FY26Â 1Â (subclusterÂ 2)` | Store subcluster 2                                |
  | `CODEÂ FY26Â 2Â (subclusterÂ 3)` | Store subcluster 3                                |
  | `Rain?`                      | 1Â if rainfall at any point during the day, elseÂ 0 |
  | `PublicÂ Holiday`             | 1Â if Public Holiday in that state, elseÂ 0         |

* **Numericals** (standardâ€‘scaled where noted):

  | Column                           | Units | Notes                            |
  | -------------------------------- | ----- | -------------------------------- |
  | `Days_after_Opening`             | days  | Days a store has been opened for |
  | `AverageÂ DailyÂ TemperatureÂ (Â°C)` | Â°C    | Different for all store location |
  | `DaysÂ FromÂ Holiday`              | days  | âˆ’veÂ =Â days *before* PH           |
  | `PuasaÂ Count`                    | days  | Day number during Ramadan        |

* **Targets** (not scaled in tree models):

  * `Net_Amount`Â (RM)
  * `TC` â€“ Transaction count

---

<a name="ml-baseline"></a>

## Baseline: PolynomialÂ Regression

Simple degreeâ€‘2 polynomial on standardâ€‘scaled numericals (categoricals left as integer ids). Wrapped in `MultiOutputRegressor` to predict both targets simultaneously.

---

<a name="ml-tree-boosting"></a>

## TreeÂ Boosting

### <a name="ml-xgboost"></a>XGBoost

* Objective: `reg:squarederror`
* 3â€‘fold GridSearch over `max_depth`, `learning_rate`, `n_estimators`, `subsample`, `colsample_bytree`.
* Separate models for **Net\_Amount** and **TC**.

### <a name="ml-lightgbm"></a>LightGBM

Four consecutive drafts investigated featureâ€‘drops & categorical handling:

1. **DraftÂ 1** â€“ full feature set
2. **DraftÂ 2** â€“ converts seven highâ€‘cardinality columns to categorical dtypes
3. **DraftÂ 3** â€“ drops `Store_No`
4. **DraftÂ 4** â€“ additionally drops `Rain?`, `DaysÂ FromÂ Holiday`, `PuasaÂ Count`

Every draft repeats a gridâ€‘search (3â€‘fold) on common hyperâ€‘parameters: `num_leaves`, `max_depth`, `learning_rate`, `n_estimators`, `reg_lambda`, `min_child_samples`.

---

<a name="ml-ffnn"></a>

## Feedâ€‘ForwardÂ NeuralÂ Network

* **Architecture**: shared embeddingâ€‘tower for 9 categoricals + numeric branch â†’ dense trunk â†’ two taskâ€‘specific heads.
* **Embeddings**: size `â£logâ‚‚(cardinality)â¦Â +Â 1`.
* **HiddenÂ layers**: 128â€‘64â€‘32 with `ReLU`, `BatchNorm`, `DropoutÂ 0.3`.
* **Loss**: MSE (separate for each head); **Optimiser**: AdamÂ (lrÂ 5â€¯Ã—â€¯10â»â´).
* **Callbacks**: EarlyStoppingÂ (patienceÂ =Â 3) & ReduceLROnPlateau.
* Training for *max*Â 50Â epochs, batchâ€‘sizeÂ 32.

---

<a name="ml-training"></a>

## TrainingÂ &Â Hyperâ€‘parameters

| Flag / Setting      | Default   | Notes                        |
| ------------------- | --------- | ---------------------------- |
| `test_size`         | 0.20      | `train_test_split` seedÂ =Â 42 |
| `poly_degree`       | 2         | Baseline model               |
| `ffnn_epochs`       | 50        | Earlyâ€‘Stopping \~10â€“15       |
| `ffnn_batch_size`   | 32        |                              |
| `xgb_n_estimators`  | 100â€‘200   | gridâ€‘search                  |
| `lgb_learning_rate` | 0.05â€“0.10 | gridâ€‘search                  |

---

<a name="ml-eval"></a>

## EvaluationÂ &Â Metrics

For each target the notebooks print:

* **RMSE, MAE, RÂ²**
* Ratios vsÂ population `mean` (MAE) and `std` (RMSE) â†’ coloured qualitative bands:

  * ğŸ”µÂ Excellentâ€ƒğŸŸ¢Â Goodâ€ƒğŸŸ¡Â Okayâ€ƒğŸ”´Â Poor

---

<a name="ml-env"></a>

## EnvironmentÂ /Â Setup

<details><summary>requirements.txt</summary>

```
pandas>=1.5
numpy>=1.23
scikit-learn>=1.3
matplotlib>=3.7
seaborn>=0.13
xgboost>=2.0
lightgbm>=4.2

```

</details>

---

<a name="ml-reproducing"></a>

## ReproducingÂ theÂ Results

**Match software versions** â€“ see `requirements.txt`.
**Make sure that the data file is correct**.

---

<a name="ml-structure"></a>

## ProjectÂ Structure

```
.
â”œâ”€â”€ data/                   
â”‚   â””â”€â”€ Create Master Weather + Store Subcluster + Holiday + Sales.ipynb
â”œâ”€â”€ notebooks/               
â”‚   â”œâ”€â”€ Daily Sales Supervised ML with Dupe Dates.ipynb  
â”œâ”€â”€ README.md                 
.
```
<a id="daily-sales-weather"></a>
# Daily Sales and Weather Studies

> Exploring how **local weather (rain & temperature)** influences storeâ€‘level daily sales in MayÂ 2025, then benchmarking several regressors (XGBoost & Neural Networks) to predict NetÂ Amount (RM) and Transaction Count (TC).

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#dailywx-features)
2. [Data](#dailywx-data)
3. [Environment & Dependencies](#dailywx-env)
4. [Reproducing the Results](#dailywx-reproducing)
5. [Training](#dailywx-training)
6. [Evaluation & Metrics](#dailywx-eval)
7. [Project Structure](#dailywx-structure)

---

<a name="dailywx-features"></a>
## Features

* **Step-by-step method for pulling in raw data, cleaning and organizing it, and getting it ready for analysis**Â â€“ cleans store, hourly sales & weather CSVs and filters only 24â€‘hour outlets which were open during MayÂ 2025.
* **Daily aggregation**Â â€“ collapses 0â€‘23â€¯h to Total_Daily_Net_Amount (RM) & Total_Daily_TC (transactions).
* **Rain detection**Â â€“ maps weatherâ€‘code sequences â†’ binary Rain? (Yes/No) and **balances classes** with SMOTE for fair modelling.
* **Exploratory stats**Â â€“ Pearson correlation and oneâ€‘hot rain vs sales analysis.
* **Modelling suite**

  * *XGBoost* baselines & 5â€‘fold GridSearch tuning for both targets.
  * *MLP (scikitâ€‘learn)* baseline + gridâ€‘search.
  * *TensorFlow MLP* with earlyâ€‘stopping & LRâ€‘scheduling.
* **Plots & artefacts**Â â€“ featureâ€‘importance bar plots, loss curves.

---

<a name="dailywx-data"></a>
## Data

| ColumnÂ Type             | Example columns                            | Notes                                               |
| ----------------------- | ------------------------------------------ | --------------------------------------------------- |
| **Identifier**          | Store_No                                   | numeric id, only 24â€¯h stores kept                   |
| **Temporal**            | Date, Hour                                 | UTCâ†’SGT conversion for weather, later aggregated    |
| **Targets**             | Total_Daily_Net_Amount, Total_Daily_TC     | Net amount negated (â€“ve to +ve) then summed perÂ day |
| **Weather numeric**     | AverageÂ DailyÂ TemperatureÂ (Â°C)             | mean of hourly temps                                |
| **Weather categorical** | Rain?                                      | derived Yes/No flag using WMO codes                 |

---

<a name="dailywx-env"></a>
## Environment & Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

pandas>=1.5
numpy>=1.23
matplotlib>=3.7
seaborn>=0.13
scikit-learn>=1.3
statsmodels>=0.14
meteostat>=1.7


</details>

*Tested on WindowsÂ 10 (PythonÂ 3.9, 32â€¯GBÂ RAM)*

---

<a name="dailywx-reproducing"></a>
## Reproducing the Results

1. pip install -r requirements.txt.
2. Load raw CSVs into data/ with exact filenames

---

<a name="dailywx-training"></a>
## Training

Key hyperâ€‘parameters examined:

| Flag / Param                     | Default     | Model      | Description               |
| -------------------------------- | ----------- | ---------- | ------------------------- |
| n_estimators                     | 100Â /Â 300   | XGBoost    | boosting rounds           |
| learning_rate                    | 0.1         | XGBoost    | Î· shrinkage               |
| max_depth                        | 3Â /Â 5Â /Â 7   | XGBoost    | tree depth                |
| subsample / colsample_bytree     | 0.8â€“1.0     | XGBoost    | row / col sampling        |
| hidden_layer_sizes               | (128,64,32) | scikitâ€‘MLP | gridâ€‘searched alt (64,32) |
| alpha                            | 1eâ€‘4        | scikitâ€‘MLP | L2 regulariser            |
| epochs                           | 100         | TFÂ MLP     | earlyâ€‘stops \~20          |
| batch_size                       | 32          | TFÂ MLP     |                           |

---

<a name="dailywx-eval"></a>
## Evaluation & Metrics

Each script/notebook prints for **Net Amount & TC**:

* Mean Absolute Error (MAE)
* Root Mean Square Error (RMSE)
* Coefficient of Determination (RÂ²)
  
---

<a name="dailywx-structure"></a>
## Project Structure 

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ store.csv
â”‚   â”œâ”€â”€ sales.csv
â”‚   â””â”€â”€ WeatherÂ DataÂ Updated.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ DAILY Sales Weather Project.ipynb
â””â”€â”€ README.md

```

<a id="hourly-sales-weather"></a>
# Hourly Sales & Weather Studies

> Investigating how temperature and detailed weather codes influence **hourâ€‘byâ€‘hour** Net Amount (RM) and transaction counts (TC) across different stores, then benchmarking linear models and XGBoost.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)

---

## Table of Contents

1. [Features](#hourlywx-features)
2. [Data](#hourlywx-data)
3. [Environment & Dependencies](#hourlywx-env)
4. [Reproducing the Results](#hourlywx-reproducing)
5. [Training](#hourlywx-training)
6. [Evaluation & Metrics](#hourlywx-eval)
7. [Project Structure](#hourlywx-structure)

---

<a name="hourlywx-features"></a>
## Features

* **Step-by-step method for pulling in raw data, cleaning and organizing it, and getting it ready for analysis**

  * Parse store file, derive Start_h/End_h from Operating_Hours.
  * Filter out closed, notâ€‘openâ€‘yet or nameless stores.
  * Expand each store into every trading hour and map hourly sales rows.
  * Convert UTC weather timestamps â†’ local SG/MY time; extract hour & join on (Date, Hour, Store_No).
* **Exploratory statistics** â€“ Pearson r for sales vs temperature & weather codes; oneâ€‘hot correlation table.
* **Modelling suite**

  * Baseline **Linear / Ridge / Lasso / ElasticNet** models (with optional TempÃ—Hour interaction).
  * **XGBoost** draft & 5â€‘fold GridSearch tuning with featureâ€‘importance plots.
* **Plots & artefacts** â€“ correlation tables, featureâ€‘importance bar charts, trainâ€‘vsâ€‘test RMSE comparison.

---

<a name="hourlywx-data"></a>
## Data

| Column Type             | Example columns                       | Notes                                    |
| ----------------------- | ------------------------------------- | ---------------------------------------- |
| **Identifiers**         | Store_No, Date, Hour            | Hourly granularity, Date in YYYYâ€‘MMâ€‘DD |
| **Sales targets**       | Net_Amount, TC                    | Hourly; net values negated (â€“ve to +ve)  |
| **Weather numeric**     | Temperature (Â°C)                    | Derived from Temperature (Â°C) in raw   |
| **Weather categorical** | Weather_Code, Weather_Description | WMO code (int) & human string            |
| **Store metadata**      | Start_h, End_h                    | For operatingâ€‘window filtering           |

---

<a name="hourlywx-env"></a>
## Environment & Dependencies

<details>
<summary>Click to view <code>requirements.txt</code></summary>

pandas>=1.5
numpy>=1.23
matplotlib>=3.7
scikit-learn>=1.3
statsmodels>=0.14
meteostat>=1.7
pytz


</details>

*Developed on PythonÂ 3.9 (WindowsÂ 10 32â€¯GB RAM).*
TensorFlow is optional (included for future NN experiments).

---

<a name="hourlywx-reproducing"></a>
## Reproducing the Results

1. pip install -r requirements.txt.
2. Load raw CSVs into data/ with exact filenames

---

<a name="hourlywx-training"></a>
## Training

Key hyperâ€‘parameters explored:

| Flag / Param             | Default                | Model      | Description            |
| ------------------------ | ---------------------- | ---------- | ---------------------- |
| n_estimators           | 100/300                | XGBoost    | boosting rounds        |
| learning_rate          | 0.05â€‘0.1               | XGBoost    | Î·                      |
| max_depth              | 3â€‘7                    | XGBoost    | tree depth             |
| subsample/colsample    | 0.8â€‘1.0                | XGBoost    | row/column sampling    |
| reg_alpha / reg_lambda | 0â€‘1 / 1â€‘10             | XGBoost    | L1 / L2 regularisation |
| hidden_layer_sizes     | (128,64,32) vs (64,32) | scikit MLP | gridâ€‘searched          |
| alpha (L2)             | 0.0001â€‘0.001           | scikit MLP |                        |

---

<a name="hourlywx-eval"></a>
## Evaluation & Metrics

For both **Net\_Amount** and **TC** the scripts print:

* **RMSE**, **MAE**, **RÂ²** on holdâ€‘out 20â€¯% test set.

---

<a name="hourlywx-structure"></a>
## Project Structure

```

.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ store.csv
â”‚   â”œâ”€â”€ sales.csv
â”‚   â””â”€â”€ Weather Data Updated.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ HOURLY Sales Weather Project FINAL.ipynb
â””â”€â”€ README.md

```

---

<a id="summary"></a>
# What I Built & Learned (Summary)

> **End-to-end retail forecasting**: integrated sales, store attributes, public holidays, and Open-Meteo weather to deliver **store-level 3-week forecasts**, baselines, documented model variants, and reproducible data pipelines.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-lightgrey)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3%2B-orange)
![XGBoost](https://img.shields.io/badge/XGBoost-2.x-brightgreen)
![LightGBM](https://img.shields.io/badge/LightGBM-4.x-yellowgreen)

---

## Highlights (Outcomes First)

- **Data integration & cleaning** â€“ Built an ML-ready table by merging sales, store data, public holidays, and **hourly weather**; enforced dtypes, handled outliers, standardized schemas (pandas).
- **Reproducible external data** â€“ Implemented an **Open-Meteo** pipeline with on-disk caching + retries; mapped WMO weather codes to readable labels; generated **timezone-aware** per-store hourly series.
- **Exploration (daily & hourly)** â€“ Quantified salesâ€“weather relationships; handled **class imbalance** (SMOTE 13:1); produced clear correlation visuals and concluded **weather alone is insufficient** to explain sales variance.
- **Supervised baselines & tuning** â€“ Trained Polynomial/Linear models, **XGBoost**, **LightGBM**, and a Feed-Forward NN; `train_test_split`, `GridSearchCV`, regularization (**ReduceLROnPlateau**, **EarlyStopping**); pruned features via importance; established baseline metrics (*numbers redacted for privacy*).
- **Sequence feature design** â€“ Partitioned features into **time-varying categoricals**, **static categoricals (entity embeddings)**, and **continuous**; applied `StandardScaler`; engineered a **14-day input window** with a **time-aware** train/validation split.
- **LSTM forecasting (flagship)** â€“ LSTM + 2-layer MLP for static context; **200-step autoregressive** forecasts; tracked **MAE/RMSE/RÂ²**; delivered **store-level 3-week forecasts** with interactive **Plotly + ipywidgets** dashboards and **permutation feature importance** for interpretability.
- **Different model variants & negative results (rigor)** â€“ Optuna HPO, categorical-as-numeric trials, PFI-guided column removals, **tiled static embeddings across time**, and **static-initialized LSTM state** â€” documented when they **did not** beat the tuned baseline.
- **Inference & handoff** â€“ Saved a deployable Keras model (`.h5`) and an **inference-only** notebook to generate forecasts without retraining; clearly separated private vs. reproducible steps.

---

## Techniques & Tooling

- **ML/DL**: LSTM (TensorFlow/Keras), FFNN, XGBoost, LightGBM, Linear/Ridge/Lasso/ElasticNet, **Permutation Feature Importance**
- **Feature engineering**: categorical **embeddings**, one-hot weather codes, static vs time-varying partitions, binary event flags, sliding-window sequences
- **Training**: `EarlyStopping`, `ReduceLROnPlateau`, `GridSearchCV`, Optuna trials
- **Data**: pandas pipelines, dtype enforcement, outlier filtering, **SMOTE** (imbalance), timezone conversion
- **Viz & UX**: Plotly charts, interactive **ipywidgets** dashboard

---

## Reproducibility & Data Access

- **Public & reproducible**: full **weather scraping** pipeline (Open-Meteo) and preprocessing logic, downloaded HTML of public holiday data available online
- **Private data**: sales/store data are excluded; notebooks retain code paths with placeholders so reviewers can audit engineering steps.
- **Environment**: versions pinned in `requirements.txt` (see project sections above).

---

## Lessons & Decisions

- Weather improves models but **cannot** explain most variance alone â†’ multi-modal features required.
- Entity-aware LSTM with static embeddings + residual MLP is robust; several static-aware tweaks and HPO **did not** surpass the tuned baseline.
- Clear separation of **training vs inference** simplifies handoff and reproducibility.

---

