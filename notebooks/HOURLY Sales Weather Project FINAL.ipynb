{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly Sales\n",
    "## Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# sales_weather_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import time\n",
    "import pytz\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) CONFIGURE YOUR LOCAL FILE PATHS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "MASTER_STORE_CSV = r\" ... csv\"\n",
    "SALES_CSV        = r\" ... csv\"\n",
    "WEATHER_CSV      = r\" ... csv\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) LOAD & PARSE MASTER STORE (Operating_Hours → Start_h, End_h)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "m = pd.read_csv(MASTER_STORE_CSV, dtype={\"Store_No\": str})\n",
    "# split \"6:00 AM - 12:00 AM\"\n",
    "oh = m[\"Operating_Hours\"].str.split(\" - \", expand=True)\n",
    "m[\"Start_h\"] = pd.to_datetime(oh[0], format=\"%I:%M %p\").dt.hour\n",
    "m[\"End_h\"]   = pd.to_datetime(oh[1], format=\"%I:%M %p\").dt.hour\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) FILTER OUT OUTLIERS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Filtering outliers\n",
    "m[\"Opening_Date\"] = pd.to_datetime(m[\"Opening_Date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "# 1. Closed stores: where CLOSEDATE is NOT null (i.e. already closed)\n",
    "closed_stores = m[m[\"CLOSEDATE\"].notna()][\"Store_No\"].astype(str)\n",
    "\n",
    "# 2. Not open yet: where Opening_Date is in the future\n",
    "today = pd.Timestamp.today().normalize()\n",
    "not_open_yet = m[m[\"Opening_Date\"] > today][\"Store_No\"].astype(str)\n",
    "\n",
    "# 3. Empty Name: where Name is missing or blank\n",
    "empty_name = m[m[\"Name\"].isna() | (m[\"Name\"].str.strip() == \"\")][\"Store_No\"].astype(str)\n",
    "\n",
    "# Combine all outlier Store_Nos and changing to integer\n",
    "outliers = pd.concat([closed_stores, not_open_yet, empty_name]).unique().tolist()\n",
    "outliers_int = set(int(x) for x in outliers)\n",
    "m = m[~m[\"Store_No\"].astype(int).isin(outliers_int)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) EXPAND EACH STORE INTO ITS OPERATING HOURS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "m.loc[m[\"End_h\"] == 0.0, \"End_h\"] = 24.0\n",
    "\n",
    "rows = []\n",
    "\n",
    "m = m.dropna(subset=[\"Start_h\", \"End_h\"])\n",
    "for _, row in m.iterrows():\n",
    "    s = int(row[\"Store_No\"])\n",
    "    start = int(row[\"Start_h\"])\n",
    "    end = int(row[\"End_h\"])\n",
    "\n",
    "    if start < end:\n",
    "        hours = range(start, end + 1)  # normal hours, e.g. 8 to 17\n",
    "    else:\n",
    "        # overnight hours, e.g. 20 to 5 → 20, 21, 22, 23, 0, 1, ..., 5\n",
    "        hours = list(range(start, 24)) + list(range(0, end + 1))\n",
    "\n",
    "    for h in hours:\n",
    "        rows.append({\n",
    "            \"Store_No\": s,\n",
    "            \"Hour\": h,\n",
    "            \"Start_h\": start,\n",
    "            \"End_h\": end\n",
    "        })\n",
    "\n",
    "map_df = pd.DataFrame(rows)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) LOAD & FILTER SALES (May 2025)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "sales = pd.read_csv(SALES_CSV, parse_dates=[\"Hourly Sales - Store & VM[Date]\"])\n",
    "sales.rename(columns={\n",
    "    \"Hourly Sales - Store & VM[Date]\": \"Date\",\n",
    "    \"Hourly Sales - Store & VM[Store No]\": \"Store_No\",\n",
    "    \"Hourly Sales - Store & VM[Hour]\": \"Hour\",\n",
    "    \"Hourly Sales - Store & VM[Net Amount]\": \"Net_Amount\",\n",
    "    \"Hourly Sales - Store & VM[TC]\": \"TC\"\n",
    "}, inplace=True)\n",
    "\n",
    "#print(\"sales: \",len(sales))  # correct - 334565\n",
    "\n",
    "## Keep only rows where Store_No is purely numeric - V is for vending machines\n",
    "# Clean, then filter\n",
    "sales[\"Store_No\"] = sales[\"Store_No\"].astype(str).str.strip()\n",
    "\n",
    "# Filter purely numeric store numbers\n",
    "sales_stores = sales[sales[\"Store_No\"].str.fullmatch(r\"\\d+\")].copy()\n",
    "\n",
    "#print(\"Unique Store_No (after numeric filter):\", sales_stores[\"Store_No\"].nunique())        #correct - 450 \n",
    "\n",
    "# Convert to integer after filtering\n",
    "sales_stores[\"Store_No\"] = sales_stores[\"Store_No\"].astype(int) \n",
    "\n",
    "#print(\"Unique Store_No (after int cast):\", sales_stores[\"Store_No\"].nunique()) #correct - 450 \n",
    "\n",
    "# Convert to datetime objects\n",
    "sales_stores[\"Date\"] = pd.to_datetime(sales_stores[\"Date\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Change columns to desired object types\n",
    "sales_stores[\"Hour\"] = sales_stores[\"Hour\"].astype(int)\n",
    "sales_stores[\"Net_Amount\"] = sales_stores[\"Net_Amount\"].astype(float)\n",
    "sales_stores[\"TC\"] = sales_stores[\"TC\"].astype(int)\n",
    "\n",
    "# Obtain only May 2025 data\n",
    "sales_stores = sales_stores[\n",
    "    (sales_stores[\"Date\"] >= \"2025-05-01\") &\n",
    "    (sales_stores[\"Date\"] <  \"2025-06-01\")\n",
    "].copy()\n",
    "\n",
    "#print(\"Unique Store_No (May only):\", sales_stores[\"Store_No\"].nunique())  #correct - 450\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6) JOIN SALES → HOURS & FILTER OUT-OF-WINDOW\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "sales_stores = sales_stores.merge(map_df, on=[\"Store_No\", \"Hour\"], how=\"left\")\n",
    "\n",
    "#print(\"sales_stores 1: \",len(sales_stores)) # correct - 267606\n",
    "\n",
    "# filter out datapoints not in operating hours\n",
    "def in_operating_hours(row):\n",
    "    hour = row[\"Hour\"]\n",
    "    start = row[\"Start_h\"]\n",
    "    end = row[\"End_h\"]\n",
    "    \n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        return False  # exclude if no mapping\n",
    "\n",
    "    if start < end:\n",
    "        return (start <= hour < end)  # exclude values before start and at/after end\n",
    "    else:\n",
    "        return (hour >= start or hour < end)  # wrap-around case\n",
    "\n",
    "# Apply row-wise\n",
    "sales_stores_filtered = sales_stores[sales_stores.apply(in_operating_hours, axis=1)].copy()\n",
    "#print(\"Filtered in-window rows:\", len(sales_stores_filtered))       #correct 260229\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7) LOAD & PARSE WEATHER (to local SG time → Date, Hour)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "wx = pd.read_csv(WEATHER_CSV)\n",
    "wx[\"ts_utc\"] = pd.to_datetime(wx[\"Timestamp\"].str.strip(), format=\"%Y-%m-%d %H:%M:%S%z\", utc=True)\n",
    "wx[\"ts_sg\"]  = wx[\"ts_utc\"].dt.tz_convert(\"Asia/Singapore\")\n",
    "wx[\"Date\"]   = wx[\"ts_sg\"].dt.date\n",
    "wx[\"Hour\"]   = wx[\"ts_sg\"].dt.hour\n",
    "wx[\"Store_No\"] = wx[\"Store No\"].astype(int)\n",
    "wx = wx[[\"Date\",\"Hour\",\"Store_No\",\"Weather Code\",\"Weather Description\",\"Temperature (°C)\"]]\n",
    "wx = wx[\n",
    "    (wx[\"Date\"] >= pd.to_datetime(\"2025-05-01\").date()) &\n",
    "    (wx[\"Date\"] <  pd.to_datetime(\"2025-06-01\").date())\n",
    "].copy()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8) MERGE SALES ↔ WEATHER\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "sales_stores_filtered[\"Date\"] = pd.to_datetime(sales_stores[\"Date\"])\n",
    "wx[\"Date\"] = pd.to_datetime(wx[\"Date\"])\n",
    "\n",
    "df = sales_stores_filtered.merge(\n",
    "    wx,\n",
    "    on=[\"Date\",\"Store_No\",\"Hour\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\n",
    "    \"Net Amount\": \"Net_Amount\",\n",
    "    \"Weather Code\":\"Weather_Code\",\n",
    "    \"Weather Description\":\"Weather_Description\",\n",
    "    \"Temperature (°C)\":\"Temperature\"\n",
    "})\n",
    "df[\"Net_Amount\"] *= -1\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 9) CORRELATION\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# numeric only: Net_Amount, TC, Weather_Code, Temperature\n",
    "corr_pairs = [\n",
    "    (\"Net_Amount\",\"Weather_Code\"),\n",
    "    (\"Net_Amount\",\"Temperature\"),\n",
    "    (\"TC\",\"Weather_Code\"),\n",
    "    (\"TC\",\"Temperature\"),\n",
    "]\n",
    "print(\"\\nPearson correlations:\")\n",
    "for x,y in corr_pairs:\n",
    "    print(f\"  {x} vs {y}: {df[x].corr(df[y]):.3f}\")\n",
    "\n",
    "print(\" Conclusion: Weak positive correlation between Sales (Net Amount and TC) against Temperature.\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 10) ONE HOT ENCODING FOR WEATHER CODE\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# sample columns\n",
    "df_encoded = pd.get_dummies(df, columns=[\"Weather_Code\"], prefix=\"wc\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in df_encoded.columns:\n",
    "    if col.startswith(\"wc_\"):\n",
    "        r_net = df_encoded[col].corr(df_encoded[\"Net_Amount\"])\n",
    "        r_tc  = df_encoded[col].corr(df_encoded[\"TC\"])\n",
    "        results.append({\n",
    "            \"Weather_Code\": col.replace(\"wc_\", \"\"),\n",
    "            \"Corr_with_Net_Amount\": r_net,\n",
    "            \"Corr_with_TC\": r_tc\n",
    "        })\n",
    "\n",
    "correlation_df = pd.DataFrame(results)\n",
    "print(correlation_df.sort_values(by=\"Corr_with_Net_Amount\", ascending=False))\n",
    "print(\"Conclusion: Weather Code has little to no effect to Sales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2162df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Net_Amount:\n",
      "  Temperature: 1.6682\n",
      "  Hour: 19.9393\n",
      "  intercept: 91.60961312356608\n",
      " RMSE: 246.97286483855052\n",
      "  R²:   0.20509175208723973\n",
      "\n",
      "Model TC:\n",
      "  Temperature: 0.4714\n",
      "  Hour: 1.0518\n",
      "  intercept: -0.20911073247273393\n",
      " RMSE: 18.28180321046031\n",
      "  R²:   0.12867017455804408\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 11) LINEAR REGRESSION MODELS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# drop any rows with missing values\n",
    "df_ml = df.dropna(subset=[\"Net_Amount\",\"TC\",\"Temperature\",\"Hour\"])\n",
    "X = df_ml[[\"Temperature\",\"Hour\"]].values\n",
    "y_net = df_ml[\"Net_Amount\"].values\n",
    "y_tc  = df_ml[\"TC\"].values\n",
    "\n",
    "# split 80/20 - 80 for training, 20 for test\n",
    "Xtr, Xte, ynet_tr, ynet_te, ytc_tr, ytc_te = train_test_split(\n",
    "    X, y_net, y_tc, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model A: Net_Amount\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(Xtr, ynet_tr)\n",
    "p_net = lr1.predict(Xte)\n",
    "print(\"\\nModel Net_Amount:\")\n",
    "\n",
    "coef1 = dict(zip([\"Temperature\", \"Hour\"], lr1.coef_))\n",
    "for feature, value in coef1.items():\n",
    "    print(f\"  {feature}: {value:.4f}\")\n",
    "\n",
    "print(\"  intercept:\", lr1.intercept_)\n",
    "rmse_net_amount = np.sqrt(mean_squared_error(ynet_te, p_net))\n",
    "print(\" RMSE:\", rmse_net_amount)\n",
    "print(\"  R²:  \", r2_score(ynet_te, p_net))\n",
    "\n",
    "# Model B: TC\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(Xtr, ytc_tr)\n",
    "p_tc = lr2.predict(Xte)\n",
    "print(\"\\nModel TC:\")\n",
    "\n",
    "coef2 = dict(zip([\"Temperature\",\"Hour\"], lr2.coef_))\n",
    "for feature, value in coef2.items():\n",
    "    print(f\"  {feature}: {value:.4f}\")\n",
    "\n",
    "print(\"  intercept:\", lr2.intercept_)\n",
    "rmse_tc = np.sqrt(mean_squared_error(ytc_te, p_tc))\n",
    "print(\" RMSE:\", rmse_tc)\n",
    "print(\"  R²:  \", r2_score(ytc_te, p_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d28633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Models for predicting Net Amount\n",
      "            Scaler       Feature_Set             Model        RMSE        R2\n",
      "0       No Scaling              Base  LinearRegression  246.972865  0.205092\n",
      "1       No Scaling              Base             Ridge  246.972865  0.205092\n",
      "2       No Scaling              Base             Lasso  246.973057  0.205091\n",
      "3       No Scaling              Base        ElasticNet  246.974473  0.205081\n",
      "4       No Scaling  With Interaction  LinearRegression  246.961481  0.205165\n",
      "5       No Scaling  With Interaction             Ridge  246.961479  0.205165\n",
      "6       No Scaling  With Interaction             Lasso  246.960390  0.205172\n",
      "7       No Scaling  With Interaction        ElasticNet  246.963610  0.205151\n",
      "8   StandardScaler              Base  LinearRegression  246.972865  0.205092\n",
      "9   StandardScaler              Base             Ridge  246.972862  0.205092\n",
      "10  StandardScaler              Base             Lasso  246.973158  0.205090\n",
      "11  StandardScaler              Base        ElasticNet  250.345260  0.183235\n",
      "12  StandardScaler  With Interaction  LinearRegression  246.961481  0.205165\n",
      "13  StandardScaler  With Interaction             Ridge  246.961430  0.205165\n",
      "14  StandardScaler  With Interaction             Lasso  246.961426  0.205165\n",
      "15  StandardScaler  With Interaction        ElasticNet  248.326059  0.196357\n",
      "\n",
      "Best model: Lasso (No Scaling, With Interaction) with RMSE = 246.960 and R² = 0.205\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 12) LINEAR, RIDGE, LASSO, ELASTICNET REGRESSION MODELS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# — Assumes df_ml already exists with 'Temperature', 'Hour', 'Net_Amount' —\n",
    "# Create the interaction term\n",
    "df_ml['Temp_Hour'] = df_ml['Temperature'] * df_ml['Hour']\n",
    "\n",
    "# Define target variable\n",
    "y = df_ml['Net_Amount'].values\n",
    "\n",
    "# Define feature sets\n",
    "feature_sets = {\n",
    "    'Base': ['Temperature', 'Hour'],\n",
    "    'With Interaction': ['Temperature', 'Hour', 'Temp_Hour']\n",
    "}\n",
    "\n",
    "# Define models including ElasticNet\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0), # Penalises large weights\n",
    "    'Lasso': Lasso(alpha=0.1), # Can zero out irrelevant features\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)  # adjust alpha and l1_ratio as needed, basically Ridge + Lasso\n",
    "}\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for scaler_name, use_scaling in [('No Scaling', False), ('StandardScaler', True)]:\n",
    "    for fs_name, features in feature_sets.items():\n",
    "        X = df_ml[features].values\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        if use_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            Xtr = scaler.fit_transform(Xtr)\n",
    "            Xte = scaler.transform(Xte)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # fit & evaluate\n",
    "            model.fit(Xtr, ytr)\n",
    "            ypred = model.predict(Xte)\n",
    "            rmse = np.sqrt(mean_squared_error(yte, ypred))\n",
    "            r2  = r2_score(yte, ypred)\n",
    "            \n",
    "            results.append({\n",
    "                'Scaler':      scaler_name,\n",
    "                'Feature_Set': fs_name,\n",
    "                'Model':       model_name,\n",
    "                'RMSE':        rmse,\n",
    "                'R2':          r2\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Linear Models for predicting Net Amount\")\n",
    "print(df_results)\n",
    "\n",
    "# Determine and print the best model (lowest RMSE, highest R² as tiebreaker)\n",
    "best_idx = df_results['RMSE'].idxmin()\n",
    "best = df_results.loc[best_idx]\n",
    "print(f\"\\nBest model: {best['Model']} ({best['Scaler']}, {best['Feature_Set']}) \"\n",
    "      f\"with RMSE = {best['RMSE']:.3f} and R² = {best['R2']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65ef181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Models for predicting TC\n",
      "            Scaler       Feature_Set             Model       RMSE        R2\n",
      "0       No Scaling              Base  LinearRegression  18.281803  0.128670\n",
      "1       No Scaling              Base             Ridge  18.281803  0.128670\n",
      "2       No Scaling              Base             Lasso  18.281927  0.128658\n",
      "3       No Scaling              Base        ElasticNet  18.284498  0.128413\n",
      "4       No Scaling  With Interaction  LinearRegression  18.279244  0.128914\n",
      "5       No Scaling  With Interaction             Ridge  18.279244  0.128914\n",
      "6       No Scaling  With Interaction             Lasso  18.279148  0.128923\n",
      "7       No Scaling  With Interaction        ElasticNet  18.279649  0.128876\n",
      "8   StandardScaler              Base  LinearRegression  18.281803  0.128670\n",
      "9   StandardScaler              Base             Ridge  18.281803  0.128670\n",
      "10  StandardScaler              Base             Lasso  18.282480  0.128606\n",
      "11  StandardScaler              Base        ElasticNet  18.466008  0.111023\n",
      "12  StandardScaler  With Interaction  LinearRegression  18.279244  0.128914\n",
      "13  StandardScaler  With Interaction             Ridge  18.279244  0.128914\n",
      "14  StandardScaler  With Interaction             Lasso  18.279661  0.128874\n",
      "15  StandardScaler  With Interaction        ElasticNet  18.368697  0.120368\n",
      "\n",
      "Best model: Lasso (No Scaling, With Interaction) with RMSE = 18.279 and R² = 0.129\n"
     ]
    }
   ],
   "source": [
    "# — Assumes df_ml already exists with 'Temperature', 'Hour', 'Net_Amount' —\n",
    "# Create the interaction term\n",
    "df_ml['Temp_Hour'] = df_ml['Temperature'] * df_ml['Hour']\n",
    "\n",
    "# Define target variable\n",
    "y = df_ml['TC'].values\n",
    "\n",
    "# Define feature sets\n",
    "feature_sets = {\n",
    "    'Base': ['Temperature', 'Hour'],\n",
    "    'With Interaction': ['Temperature', 'Hour', 'Temp_Hour']\n",
    "}\n",
    "\n",
    "# Define models including ElasticNet\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0), # Penalises large weights\n",
    "    'Lasso': Lasso(alpha=0.1), # Can zero out irrelevant features\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)  # adjust alpha and l1_ratio as needed, basically Ridge + Lasso\n",
    "}\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for scaler_name, use_scaling in [('No Scaling', False), ('StandardScaler', True)]:\n",
    "    for fs_name, features in feature_sets.items():\n",
    "        X = df_ml[features].values\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        if use_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            Xtr = scaler.fit_transform(Xtr)\n",
    "            Xte = scaler.transform(Xte)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # fit & evaluate\n",
    "            model.fit(Xtr, ytr)\n",
    "            ypred = model.predict(Xte)\n",
    "            rmse = np.sqrt(mean_squared_error(yte, ypred))\n",
    "            r2  = r2_score(yte, ypred)\n",
    "            \n",
    "            results.append({\n",
    "                'Scaler':      scaler_name,\n",
    "                'Feature_Set': fs_name,\n",
    "                'Model':       model_name,\n",
    "                'RMSE':        rmse,\n",
    "                'R2':          r2\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Linear Models for predicting TC\")\n",
    "print(df_results)\n",
    "\n",
    "# Determine and print the best model (lowest RMSE, highest R² as tiebreaker)\n",
    "best_idx = df_results['RMSE'].idxmin()\n",
    "best = df_results.loc[best_idx]\n",
    "print(f\"\\nBest model: {best['Model']} ({best['Scaler']}, {best['Feature_Set']}) \"\n",
    "      f\"with RMSE = {best['RMSE']:.3f} and R² = {best['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504c08a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 235.06\n",
      "XGBoost R²:   0.2799\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From this Feature Importance, we can see that Hour is the strongest predictor - time of day drives sales (expected).\n",
      "We can also see that the interaction term is important - sales spike when hot at certain times.\n",
      "Temperature is not very useful on its own. Good case where interaction term is impactful.\n",
      "Train RMSE: 234.09  | Test RMSE: 235.06\n",
      "Train R²:   0.2826  | Test R²:   0.2799\n",
      "R² is < 0.3, model is weak - does not capture a lot of data. We want to find a model that ideally has R² of > 0.5.\n",
      "RMSE: 235.06\n",
      "Standard Deviation of Net_Amount: 277.01\n",
      "RMSE is roughly 85% of Standard Deviation. This means that it is useful but not particularly strong/impactful. We want to find a model that ideally has RMSE to SD ratio of lower than 0.75.\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 13) XGBOOST draft 1\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Imports\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Prepare your data (based on your previous work)\n",
    "df_ml = df.dropna(subset=[\"Net_Amount\", \"Temperature\", \"Hour\"])\n",
    "\n",
    "# Optional: Create interaction features\n",
    "df_ml[\"Temp_Hour\"] = df_ml[\"Temperature\"] * df_ml[\"Hour\"]\n",
    "\n",
    "# Define features and target\n",
    "features = [\"Temperature\", \"Hour\", \"Temp_Hour\"]\n",
    "X = df_ml[features].values\n",
    "y = df_ml[\"Net_Amount\"].values\n",
    "\n",
    "# 3. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train XGBoost Regressor\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict & Evaluate\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost RMSE: {rmse:.2f}\")\n",
    "print(f\"XGBoost R²:   {r2:.4f}\")\n",
    "\n",
    "# 6. Check Feature Importance\n",
    "plt.figure(figsize=(8, 5))\n",
    "xgb.feature_importances_\n",
    "plt.bar(features, xgb.feature_importances_)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "print(\"From this Feature Importance, we can see that Hour is the strongest predictor - time of day drives sales (expected).\")\n",
    "print(\"We can also see that the interaction term is important - sales spike when hot at certain times.\")\n",
    "print(\"Temperature is not very useful on its own. Good case where interaction term is impactful.\")\n",
    "\n",
    "# 7. Compare train vs test to assess over/underfitting\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, xgb.predict(X_train)))\n",
    "train_r2 = r2_score(y_train, xgb.predict(X_train))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}  | Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Train R²:   {train_r2:.4f}  | Test R²:   {r2:.4f}\")\n",
    "print(\"R² is < 0.3, model is weak - does not capture a lot of data. We want to find a model that ideally has R² of > 0.5.\")\n",
    "\n",
    "# Standard deviation of the true target\n",
    "std_net = np.std(y_test)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"Standard Deviation of Net_Amount: {std_net:.2f}\")\n",
    "print(\"RMSE is roughly 85% of Standard Deviation. This means that it is useful but not particularly strong/impactful. \" \n",
    "      \"We want to find a model that ideally has RMSE to SD ratio of lower than 0.75.\")\n",
    "\n",
    "#tune hyperparameters using gridsearchcv / optuna. Compare against linear/ridge/lasso models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf8b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best parameters:\n",
      "{'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 10, 'subsample': 1.0}\n",
      "\n",
      "Best CV RMSE: 234.03\n",
      "\n",
      "Test RMSE: 234.74\n",
      "Test R²: 0.2819\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 13) XGBOOST improved using GridSearchCV\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Prepare your dataset\n",
    "# ----------------------------\n",
    "df_ml = df.dropna(subset=[\"Net_Amount\", \"Temperature\", \"Hour\"])\n",
    "\n",
    "# Optional interaction feature\n",
    "df_ml[\"Temp_Hour\"] = df_ml[\"Temperature\"] * df_ml[\"Hour\"]\n",
    "\n",
    "# Define features and target\n",
    "features = [\"Temperature\", \"Hour\", \"Temp_Hour\"]\n",
    "X = df_ml[features].values\n",
    "y = df_ml[\"Net_Amount\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Define parameter grid\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],      # L1 regularization (sparsity)\n",
    "    'reg_lambda': [1, 5, 10]       # L2 regularization (shrinkage)\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Set up the model and GridSearch\n",
    "# ----------------------------\n",
    "model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE (but negative for minimization)\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Print the best parameters and score\n",
    "# ----------------------------\n",
    "print(\"Best parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nBest CV RMSE: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Evaluate on test set\n",
    "# ----------------------------\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest RMSE: {rmse:.2f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4804efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tasks ranked by priority: \n",
    "# 1. adjusting parameters of LinearRegression (in scikit learn) to attain a better linear regression (esp RMSE abit high for net amount one) - tried my best\n",
    "# 2. explore lazypredict and find best models + tweak parameters (gets stuck at 26/42 already ran for more than 1 hour) \n",
    "# 3. attempt 4 different types of more advanced models: xgboost regressor, lightgbm, svr, neural network\n",
    "# 4. can try find correlation of vending machine sales to weather - realised don't have vending machine location data so can't\n",
    "# 5. request data for store type, e.g airport, store, gas station... and do clustering then find correlation against weather based on store type\n",
    "# 6. look into holidays esp public holiday and correlation with weather, then if possible can look for school holidays, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
