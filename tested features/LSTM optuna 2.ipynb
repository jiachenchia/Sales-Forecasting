{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4ffe9a",
   "metadata": {},
   "source": [
    "Running Optuna on params below (taking too long so did 2 of this)\n",
    "{'window': 7, 'n_layers': 4, 'lstm_units': 128, 'dropout_rate': 0.2, 'static_dense': 128, 'learning_rate': 0.0005728715834481826, 'batch_size': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Loading libraries and datasets, and set up data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random           # ← Added this line\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics       import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers    import (\n",
    "    Input, LSTM, Embedding, Flatten, Dense,\n",
    "    Concatenate, SpatialDropout1D,\n",
    "    BatchNormalization, Dropout\n",
    ")\n",
    "from tensorflow.keras.models    import Model\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  (Unchanged) Read & clean\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\" ... csv\")\n",
    "\n",
    "time_varying_categorical_cols = ['Rain?','Name','Puasa','Public Holiday','Day','Month']\n",
    "static_categorical_cols       = ['Store_No','State','CODE (subcluster 1)',\n",
    "                                 'CODE FY26 1 (subcluster 2)','CODE FY26 2 (subcluster 3)']\n",
    "categorical_cols = time_varying_categorical_cols + static_categorical_cols\n",
    "numeric_cols     = ['Net_Amount','TC','Days_after_Opening','Average Daily Temperature (°C)']\n",
    "\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].fillna('blank').replace('', 'blank')\n",
    "df['Name']                = df['Name'].fillna('no PH').replace('', 'no PH')\n",
    "df['Puasa']               = df['Puasa'].fillna(0).replace('', 0)\n",
    "df['Public Holiday']      = df['Public Holiday'].fillna(0).replace('', 0)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  (Unchanged) Encode categoricals\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "embed_cols = []\n",
    "for col in categorical_cols:\n",
    "    n = df[col].nunique()\n",
    "    if col in static_categorical_cols or n >= 7:\n",
    "        embed_cols.append(col)\n",
    "\n",
    "encoders = {}\n",
    "for col in embed_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_enc'] = le.fit_transform(df[col])\n",
    "    encoders[col]  = le\n",
    "\n",
    "df['Rain?'] = df['Rain?'].map({'Yes':1, ' No':0})\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  (Unchanged) Scale continuous features\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  (New) Time-aware train/val split helper for Optuna\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "time_numeric_cols = [\n",
    "    'Net_Amount',\n",
    "    'TC',\n",
    "    'Days_after_Opening',\n",
    "    'Average Daily Temperature (°C)',\n",
    "    'Rain?',          # make sure this is 0/1\n",
    "    'Puasa',          # 0/1\n",
    "    'Public Holiday'  # 0/1\n",
    "]\n",
    "\n",
    "def time_aware_train_val_split(df, window,\n",
    "        time_numeric_cols, static_cols_enc, val_frac=0.2):\n",
    "    \"\"\"\n",
    "    For each store, slide windows of length `window`.\n",
    "    First (1 - val_frac) → train, last val_frac → val.\n",
    "    Returns X_num_tr, X_num_va, X_name_tr, X_name_va, X_day_tr, X_day_va,\n",
    "            X_month_tr, X_month_va, X_stat_tr, X_stat_va, y_tr, y_va\n",
    "    \"\"\"\n",
    "    X_num_tr, X_num_va = [], []\n",
    "    X_name_tr, X_name_va = [], []\n",
    "    X_day_tr,  X_day_va  = [], []\n",
    "    X_month_tr,X_month_va= [], []\n",
    "    X_stat_tr, X_stat_va = [], []\n",
    "    y_tr,      y_va      = [], []\n",
    "\n",
    "    for _, grp in df.groupby('Store_No'):\n",
    "        grp = grp.sort_values('Date')\n",
    "        T = len(grp)\n",
    "        n_windows = T - window\n",
    "        if n_windows <= 0:\n",
    "            continue\n",
    "\n",
    "        split_i = int((1 - val_frac) * n_windows)\n",
    "        arr_num   = grp[time_numeric_cols].values\n",
    "        arr_name  = grp['Name_enc' ].values\n",
    "        arr_day   = grp['Day_enc'  ].values\n",
    "        arr_month = grp['Month_enc'].values\n",
    "        arr_stat  = grp[static_cols_enc].iloc[window:].values\n",
    "        arr_tgt   = grp[['Net_Amount','TC']].values[window:]\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            seq_num   = arr_num[i : i+window]\n",
    "            seq_name  = arr_name[i : i+window]\n",
    "            seq_day   = arr_day[i : i+window]\n",
    "            seq_month = arr_month[i : i+window]\n",
    "            stat_vec  = arr_stat[i]\n",
    "            target    = arr_tgt[i]\n",
    "\n",
    "            if i < split_i:\n",
    "                X_num_tr .append(seq_num)\n",
    "                X_name_tr.append(seq_name)\n",
    "                X_day_tr .append(seq_day)\n",
    "                X_month_tr.append(seq_month)\n",
    "                X_stat_tr.append(stat_vec)\n",
    "                y_tr     .append(target)\n",
    "            else:\n",
    "                X_num_va .append(seq_num)\n",
    "                X_name_va.append(seq_name)\n",
    "                X_day_va .append(seq_day)\n",
    "                X_month_va.append(seq_month)\n",
    "                X_stat_va.append(stat_vec)\n",
    "                y_va     .append(target)\n",
    "\n",
    "    def _stack(lst): return np.stack(lst) if lst else np.array(lst)\n",
    "    return (\n",
    "        _stack(X_num_tr),  _stack(X_num_va),\n",
    "        _stack(X_name_tr), _stack(X_name_va),\n",
    "        _stack(X_day_tr),  _stack(X_day_va),\n",
    "        _stack(X_month_tr),_stack(X_month_va),\n",
    "        _stack(X_stat_tr), _stack(X_stat_va),\n",
    "        _stack(y_tr),      _stack(y_va)\n",
    "    )\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6.  (Modified) Build Sales LSTM with variable depth\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def build_sales_lstm(\n",
    "        W, F,\n",
    "        time_cardinalities, static_cardinalities,\n",
    "        lstm_units   = 48,\n",
    "        dropout_rate = 0.2,\n",
    "        static_dense = 64,\n",
    "        learning_rate= 1e-3,\n",
    "        n_layers     = 1             # <<< NEW\n",
    "    ):\n",
    "    # — time-series inputs & embeddings —\n",
    "    num_in       = Input((W, F), name='num_in')\n",
    "    name_in      = Input((W,),   dtype='int32', name='name_seq_in')\n",
    "    day_in       = Input((W,),   dtype='int32', name='day_seq_in')\n",
    "    month_in     = Input((W,),   dtype='int32', name='month_seq_in')\n",
    "\n",
    "    dim = lambda n: min(50, n//2 + 1)\n",
    "    emb_name  = SpatialDropout1D(dropout_rate)(\n",
    "                    Embedding(time_cardinalities['name'],  dim(time_cardinalities['name']))(name_in))\n",
    "    emb_day   = SpatialDropout1D(dropout_rate)(\n",
    "                    Embedding(time_cardinalities['day'],   dim(time_cardinalities['day'])) (day_in))\n",
    "    emb_month = SpatialDropout1D(dropout_rate)(\n",
    "                    Embedding(time_cardinalities['month'], dim(time_cardinalities['month']))(month_in))\n",
    "\n",
    "    x = Concatenate(axis=-1)([num_in, emb_name, emb_day, emb_month])\n",
    "\n",
    "    # — stacked LSTM layers —\n",
    "    for l in range(n_layers):\n",
    "        x = LSTM(lstm_units,\n",
    "                 dropout=0.0,\n",
    "                 return_sequences=(l < n_layers-1)\n",
    "                )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # — static branch embeddings & dense —\n",
    "    static_inputs, static_vecs = [], []\n",
    "    for base, vocab in static_cardinalities:\n",
    "        s_in  = Input((1,), dtype='int32', name=f\"{base}_in\")\n",
    "        s_emb = Embedding(vocab, dim(vocab))(s_in)\n",
    "        s_emb = Flatten()(s_emb)\n",
    "        s_emb = BatchNormalization()(s_emb)\n",
    "        s_emb = Dropout(dropout_rate)(s_emb)\n",
    "        static_inputs.append(s_in)\n",
    "        static_vecs  .append(s_emb)\n",
    "\n",
    "    s_cat = Concatenate()(static_vecs)\n",
    "    s_cat = Dense(static_dense, activation='relu')(s_cat)\n",
    "    s_cat = BatchNormalization()(s_cat)\n",
    "    s_cat = Dropout(dropout_rate)(s_cat)\n",
    "\n",
    "    out = Dense(2, activation='linear', name='sales_out')(\n",
    "              Concatenate()([x, s_cat])\n",
    "          )\n",
    "\n",
    "    model = Model([num_in, name_in, day_in, month_in] + static_inputs, out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mse', metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7.  Prepare cardinalities & static cols list (unchanged)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "time_cardinalities = {\n",
    "    'name' : df['Name_enc'].nunique(),\n",
    "    'day'  : df['Day_enc'].nunique(),\n",
    "    'month': df['Month_enc'].nunique()\n",
    "}\n",
    "static_cols_enc       = [c + '_enc' for c in static_categorical_cols if c in embed_cols]\n",
    "static_cardinalities = [(col.replace('_enc',''), df[col].nunique()) for col in static_cols_enc]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8.  Optuna hyperparameter search\n",
    "#       tuning: window • n_layers • lstm_units • dropout_rate • static_dense • learning_rate\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def make_input_dict(X_num, X_name, X_day, X_month, X_stat):\n",
    "    d = {\n",
    "        'num_in':       X_num.astype('float32'),\n",
    "        'name_seq_in':  X_name.astype('int32'),\n",
    "        'day_seq_in':   X_day.astype('int32'),\n",
    "        'month_seq_in': X_month.astype('int32'),\n",
    "    }\n",
    "    for i,(base,_) in enumerate(static_cardinalities):\n",
    "        d[f'{base}_in'] = X_stat[:,i].reshape(-1,1).astype('int32')\n",
    "    return d\n",
    "\n",
    "def objective(trial):\n",
    "    # ─ sample hyperparameters ─\n",
    "    window        = trial.suggest_int(\"window\",         7, 28, step=7)\n",
    "    n_layers      = trial.suggest_int(\"n_layers\",       1, 4)\n",
    "    lstm_units    = trial.suggest_int(\"lstm_units\",    32,256,step=32)\n",
    "    dropout_rate  = trial.suggest_float(\"dropout_rate\", 0.0,0.5,step=0.1)\n",
    "    static_dense  = trial.suggest_int(\"static_dense\",   16,128,step=16)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5,1e-2)\n",
    "    batch_size    = trial.suggest_categorical(\"batch_size\",[32,64,128])\n",
    "\n",
    "    # ─ re-split train/val on-the-fly ─\n",
    "    (X_num_tr, X_num_va,\n",
    "     X_name_tr,X_name_va,\n",
    "     X_day_tr, X_day_va,\n",
    "     X_month_tr,X_month_va,\n",
    "     X_stat_tr,X_stat_va,\n",
    "     y_tr,     y_va) = time_aware_train_val_split(\n",
    "                        df, window,\n",
    "                        time_numeric_cols, static_cols_enc,\n",
    "                        val_frac=0.2\n",
    "                      )\n",
    "\n",
    "    train_inputs = make_input_dict(\n",
    "        X_num_tr, X_name_tr, X_day_tr, X_month_tr, X_stat_tr)\n",
    "    val_inputs   = make_input_dict(\n",
    "        X_num_va, X_name_va, X_day_va, X_month_va, X_stat_va)\n",
    "\n",
    "    # ─ build & compile model ─\n",
    "    model = build_sales_lstm(\n",
    "        window, X_num_tr.shape[-1],\n",
    "        time_cardinalities, static_cardinalities,\n",
    "        lstm_units    = lstm_units,\n",
    "        dropout_rate  = dropout_rate,\n",
    "        static_dense  = static_dense,\n",
    "        learning_rate = learning_rate,\n",
    "        n_layers      = n_layers\n",
    "    )\n",
    "\n",
    "    # ─ train silently ─\n",
    "    es = EarlyStopping(\"val_loss\", patience=3, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        train_inputs, y_tr,\n",
    "        validation_data=(val_inputs,y_va),\n",
    "        epochs=50, batch_size=batch_size,\n",
    "        callbacks=[es], verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history[\"val_loss\"])\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner()\n",
    ")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n✅ Best hyperparameters:\")\n",
    "for k,v in study.best_params.items():\n",
    "    print(f\"  {k:<14s}: {v}\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 9.  (Optional) Retrain & evaluate final model with best params\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "best = study.best_params\n",
    "window = best['window']\n",
    "\n",
    "(X_num_tr, X_num_va, X_num_te,\n",
    " X_name_tr, X_name_va, X_name_te,\n",
    " X_day_tr,  X_day_va,  X_day_te,\n",
    " X_month_tr,X_month_va,X_month_te,\n",
    " X_stat_tr, X_stat_va, X_stat_te,\n",
    " y_tr,      y_va,      y_te) = time_aware_train_val_split(\n",
    "    df, window, time_numeric_cols, static_cols_enc, val_frac=0.15\n",
    ")\n",
    "\n",
    "# (Here you would rebuild train/val/test dicts, retrain final model, and evaluate…)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
