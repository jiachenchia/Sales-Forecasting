{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cf886a",
   "metadata": {},
   "source": [
    "Using values from Optuna 2 to test how good this model is.\n",
    "{'window': 7, 'n_layers': 4, 'lstm_units': 128, 'dropout_rate': 0.2, 'static_dense': 128, 'learning_rate': 0.0005728715834481826, 'batch_size': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f45979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Loading libraries and datasets, and set up data\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (Input, LSTM, Embedding, Flatten, Dense, Concatenate, SpatialDropout1D, BatchNormalization, Dropout, Add, RepeatVector)\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Cluster, PH, Weather, Sales data\n",
    "df = pd.read_csv(r\" ... csv\")\n",
    "\n",
    "# Feature columns\n",
    "time_varying_categorical_cols = ['Rain?','Name','Puasa','Public Holiday','Day','Month']  # varies with time\n",
    "static_categorical_cols = ['Store_No','State','CODE (subcluster 1)','CODE FY26 1 (subcluster 2)','CODE FY26 2 (subcluster 3)']  # won't change depending on time\n",
    "categorical_cols = time_varying_categorical_cols + static_categorical_cols\n",
    "numeric_cols = ['Net_Amount','TC','Days_after_Opening','Average Daily Temperature (Â°C)']\n",
    "\n",
    "# Filling in spaces to be encoded: 'CODE (subcluster 1)': blank, Name: no PH, Puasa: 0, Public Holiday: 0\n",
    "# Filling empty spaces\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].fillna('blank')\n",
    "df['Name']               = df['Name'].fillna('no PH')\n",
    "df['Puasa']              = df['Puasa'].fillna(0)\n",
    "df['Public Holiday']     = df['Public Holiday'].fillna(0)\n",
    "\n",
    "# Replace any emptyâ€string entries (''), if they exist\n",
    "df['CODE (subcluster 1)'] = df['CODE (subcluster 1)'].replace('', 'blank')\n",
    "df['Name']               = df['Name'].replace('', 'no PH')\n",
    "df['Puasa']              = df['Puasa'].replace('', 0)\n",
    "df['Public Holiday']     = df['Public Holiday'].replace('', 0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Encoding categorical columns\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Splitting categorical columns into 2 types - to be embedded or be binary\n",
    "embed_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_uniques = df[col].nunique()\n",
    "    if col in static_categorical_cols:\n",
    "        embed_cols.append(col)\n",
    "    elif n_uniques < 3:\n",
    "        pass\n",
    "    elif n_uniques >= 7:\n",
    "        embed_cols.append(col)\n",
    "    else:\n",
    "        print(f\"Column {col} has {n_uniques} categories: choose manually.\")\n",
    "\n",
    "# Encoding embed columns\n",
    "encoders = {}\n",
    "for col in embed_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_enc'] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Binary columns - Puasa and Public Holiday are already in 0s and 1s\n",
    "df['Rain?'] = df['Rain?'].map({'Yes': 1, ' No': 0})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Scaling continuous features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Building sequences for LSTM\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# purely numeric or binary (already scaled / mapped 0/1)\n",
    "time_numeric_cols = [\n",
    "    'Net_Amount',\n",
    "    'TC',\n",
    "    'Days_after_Opening',\n",
    "    'Average Daily Temperature (Â°C)',\n",
    "    'Rain?',          # make sure this is 0/1\n",
    "    'Puasa',          # 0/1\n",
    "    'Public Holiday'  # 0/1\n",
    "]\n",
    "\n",
    "# select window size 7-30 days\n",
    "window = 7\n",
    "\n",
    "X_num, X_name, X_day, X_month, X_stat, y = [], [], [], [], [], []\n",
    "\n",
    "static_cols = [col + '_enc' for col in static_categorical_cols if col in embed_cols]\n",
    "\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T   = len(grp)\n",
    "    if T <= window: \n",
    "        continue\n",
    "\n",
    "    # raw arrays\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(T - window):\n",
    "        X_num.append(   arr_num[i : i+window]     )  # â†’ (window, n_num_feats)\n",
    "        X_name.append(  arr_name[i : i+window]    )  # â†’ (window,)\n",
    "        X_day.append(   arr_day[i : i+window]     )\n",
    "        X_month.append(arr_month[i : i+window]    )\n",
    "        X_stat.append(  arr_static[i]             )  # â†’ (n_static_feats,)\n",
    "        y.append(       arr_target[i]             )  # â†’ scalar\n",
    "\n",
    "# stack into arrays\n",
    "X_num    = np.stack(X_num)    # (N, window, n_num_feats)\n",
    "X_name   = np.stack(X_name)   # (N, window)\n",
    "X_day    = np.stack(X_day)    # (N, window)\n",
    "X_month  = np.stack(X_month)  # (N, window)\n",
    "X_stat   = np.stack(X_stat)   # (N, n_static_feats)\n",
    "y        = np.array(y)        # (N,)\n",
    "\n",
    "print(X_num.shape, X_name.shape, X_day.shape, X_month.shape, X_stat.shape, y.shape)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Time-aware train-validation data split\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Initialize empty Python lists to hold your training vs. validation sequences:\n",
    "X_num_train, X_num_val, \\\n",
    "X_name_train, X_name_val, \\\n",
    "X_day_train,  X_day_val,  \\\n",
    "X_month_train,X_month_val,\\\n",
    "X_stat_train, X_stat_val, \\\n",
    "y_train,      y_val = ([] for _ in range(12))\n",
    "\n",
    "window = 7\n",
    "for store_id, grp in df.groupby('Store_No'):\n",
    "    grp = grp.sort_values('Date')\n",
    "    T = len(grp)\n",
    "    n_windows = T - window\n",
    "    if n_windows <= 0: continue\n",
    "\n",
    "    # how many windows to train on for this store\n",
    "    split_store = int(0.8 * n_windows)\n",
    "\n",
    "    arr_num    = grp[time_numeric_cols].values        # (T, n_num_feats)\n",
    "    arr_name   = grp['Name_enc'].values               # (T,)\n",
    "    arr_day    = grp['Day_enc'].values                # (T,)\n",
    "    arr_month  = grp['Month_enc'].values              # (T,)\n",
    "    arr_static = grp[static_cols].iloc[window:].values# (T-window, n_static_feats)\n",
    "    arr_target = grp[['Net_Amount','TC']].values[window:]    # (T-window,)\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        num_seq  = arr_num[i : i+window]\n",
    "        name_seq = arr_name[i : i+window]\n",
    "        day_seq  = arr_day[i : i+window]\n",
    "        month_seq = arr_month[i : i+window]\n",
    "        static = arr_static[i]\n",
    "        target   = arr_target[i]\n",
    "        if i < split_store:\n",
    "            X_num_train.append(num_seq)\n",
    "            X_name_train.append(name_seq)\n",
    "            X_day_train.append(day_seq)\n",
    "            X_month_train.append(month_seq)\n",
    "            X_stat_train.append(static)\n",
    "            y_train.append(target)\n",
    "        else:\n",
    "            X_num_val.append(num_seq)\n",
    "            X_name_val.append(name_seq)\n",
    "            X_day_val.append(day_seq)\n",
    "            X_month_val.append(month_seq)\n",
    "            X_stat_val.append(static)\n",
    "            y_val.append(target)\n",
    "\n",
    "# finally stack\n",
    "X_num_train = np.stack(X_num_train)\n",
    "X_name_train = np.stack(X_name_train)\n",
    "X_day_train = np.stack(X_day_train)\n",
    "X_month_train = np.stack(X_month_train)\n",
    "X_stat_train = np.stack(X_stat_train)\n",
    "y_train     = np.stack(y_train)\n",
    "\n",
    "X_stat_train = np.array(X_stat_train)\n",
    "X_stat_val   = np.array(X_stat_val)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. Build, sanityâ€‘check & train the Keras model (with embeddings)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "{'window': 7, 'n_layers': 4, 'lstm_units': 128, 'dropout_rate': 0.2, 'static_dense': 128, 'learning_rate': 0.0005728715834481826, 'batch_size': 64}\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Building the model\n",
    "def build_sales_lstm(\n",
    "        W, F,\n",
    "        time_cardinalities,        # dict: {'name':â€¦, 'day':â€¦, 'month':â€¦}\n",
    "        static_cardinalities,      # list of (col_name, vocab_size)\n",
    "        lstm_units    = 128,\n",
    "        n_layers      = 4,         # â† NEW\n",
    "        dropout_rate  = 0.2,\n",
    "        static_dense  = 128,\n",
    "        learning_rate = 0.0018917900209603875\n",
    "    ):\n",
    "\n",
    "    # â€” time-series inputs â€”\n",
    "    num_in       = Input(shape=(W, F), name='num_in')\n",
    "    name_seq_in  = Input(shape=(W,), dtype='int32', name='name_seq_in')\n",
    "    day_seq_in   = Input(shape=(W,), dtype='int32', name='day_seq_in')\n",
    "    month_seq_in = Input(shape=(W,), dtype='int32', name='month_seq_in')\n",
    "\n",
    "    # â€” time-varying embeddings â€”\n",
    "    n_name, n_day, n_month = (\n",
    "        time_cardinalities['name'],\n",
    "        time_cardinalities['day'],\n",
    "        time_cardinalities['month'],\n",
    "    )\n",
    "    dim = lambda n: min(100, n // 2 + 5)\n",
    "\n",
    "    name_emb  = SpatialDropout1D(dropout_rate)(Embedding(n_name,  dim(n_name))(name_seq_in))\n",
    "    day_emb   = SpatialDropout1D(dropout_rate)(Embedding(n_day,   dim(n_day))(day_seq_in))\n",
    "    month_emb = SpatialDropout1D(dropout_rate)(Embedding(n_month, dim(n_month))(month_seq_in))\n",
    "\n",
    "    # concatenate all time-varying features\n",
    "    time_concat = Concatenate(axis=-1)([num_in, name_emb, day_emb, month_emb])  # (batch, W, â€¦)\n",
    "\n",
    "    # â€” static inputs + embeddings â€”\n",
    "    static_inputs, static_vecs = [], []\n",
    "    for base, vocab in static_cardinalities:\n",
    "        inp = Input(shape=(1,), dtype='int32', name=f\"{base}_in\")\n",
    "        emb = Embedding(vocab, dim(vocab))(inp)\n",
    "        emb = Flatten()(emb)\n",
    "        emb = BatchNormalization()(emb)\n",
    "        emb = Dropout(dropout_rate)(emb)\n",
    "        static_inputs.append(inp)\n",
    "        static_vecs.append(emb)\n",
    "\n",
    "    static_cat = Concatenate()(static_vecs)\n",
    "    static_cat = Dense(128, activation='relu')(static_cat)\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    static_cat = Dense(static_dense, activation='relu')(static_cat)\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    # residual block\n",
    "    skip = static_cat\n",
    "    static_cat = Dense(static_dense, activation='relu')(static_cat)\n",
    "    static_cat = Add()([static_cat, skip])\n",
    "    static_cat = BatchNormalization()(static_cat)\n",
    "    static_cat = Dropout(0.3)(static_cat)\n",
    "\n",
    "    # â€” tile static features across time â€”\n",
    "    static_tile = RepeatVector(W)(static_cat)  # (batch, W, static_dense)\n",
    "    time_concat = Concatenate(axis=-1)([time_concat, static_tile])\n",
    "\n",
    "    # â€” generate initial LSTM states from static embedding â€”\n",
    "    init_h = Dense(lstm_units, activation='tanh', name='init_h')(static_cat)\n",
    "    init_c = Dense(lstm_units, activation='tanh', name='init_c')(static_cat)\n",
    "\n",
    "    # â€” stacked LSTM(s) â€”\n",
    "    x = time_concat\n",
    "    for i in range(n_layers):\n",
    "        return_seq = i < n_layers - 1        # keep sequences until the last layer\n",
    "        layer_name = f'lstm_{i+1}'\n",
    "        if i == 0:\n",
    "            x = LSTM(\n",
    "                lstm_units,\n",
    "                dropout=0.0,\n",
    "                return_sequences=return_seq,\n",
    "                name=layer_name\n",
    "            )(x, initial_state=[init_h, init_c])\n",
    "        else:\n",
    "            x = LSTM(\n",
    "                lstm_units,\n",
    "                dropout=0.0,\n",
    "                return_sequences=return_seq,\n",
    "                name=layer_name\n",
    "            )(x)\n",
    "        # optional norm/dropout after each layer\n",
    "        x = BatchNormalization(name=f'bn_{i+1}')(x)\n",
    "        x = Dropout(dropout_rate, name=f'drop_{i+1}')(x)\n",
    "\n",
    "    # â€” combine final LSTM output with static features â€”\n",
    "    merged = Concatenate()([x, static_cat])\n",
    "    out = Dense(2, activation='linear', name='sales_out')(merged)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[num_in, name_seq_in, day_seq_in, month_seq_in] + static_inputs,\n",
    "        outputs=out,\n",
    "        name='Sales_LSTM_StaticInit'\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Prepare cardinalities for embeddings\n",
    "n_name  = df['Name_enc' ].nunique()\n",
    "n_day   = df['Day_enc'  ].nunique()\n",
    "n_month = df['Month_enc'].nunique()\n",
    "time_cardinalities = {'name': n_name, 'day': n_day, 'month': n_month}\n",
    "\n",
    "static_cardinalities = []\n",
    "for enc_col in static_cols:                      # e.g. \"Store_No_enc\"\n",
    "    base  = enc_col.replace('_enc','')           # \"Store_No\"\n",
    "    vocab = df[enc_col].nunique()\n",
    "    static_cardinalities.append((base, vocab))\n",
    "\n",
    "# window & feature count\n",
    "_, W, F = X_num.shape\n",
    "\n",
    "# Cast *all* arrays to proper dtype and shape\n",
    "X_num_train   = np.asarray(X_num_train,   dtype=np.float32)\n",
    "X_num_val     = np.asarray(X_num_val,     dtype=np.float32)\n",
    "\n",
    "def to_int2d(arr): return np.asarray(arr, dtype=np.int32)\n",
    "X_name_train  = to_int2d(X_name_train)\n",
    "X_day_train   = to_int2d(X_day_train)\n",
    "X_month_train = to_int2d(X_month_train)\n",
    "X_name_val    = to_int2d(X_name_val)\n",
    "X_day_val     = to_int2d(X_day_val)\n",
    "X_month_val   = to_int2d(X_month_val)\n",
    "\n",
    "X_stat_train  = np.asarray(X_stat_train, dtype=np.int32)\n",
    "X_stat_val    = np.asarray(X_stat_val,   dtype=np.int32)\n",
    "y_train       = np.asarray(y_train,      dtype=np.float32)\n",
    "y_val         = np.asarray(y_val,        dtype=np.float32)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Fit Model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Build input dicts (by name â”€ safest)\n",
    "train_inputs = {\n",
    "    'num_in':       X_num_train,\n",
    "    'name_seq_in':  X_name_train,\n",
    "    'day_seq_in':   X_day_train,\n",
    "    'month_seq_in': X_month_train,\n",
    "}\n",
    "val_inputs   = {\n",
    "    'num_in':       X_num_val,\n",
    "    'name_seq_in':  X_name_val,\n",
    "    'day_seq_in':   X_day_val,\n",
    "    'month_seq_in': X_month_val,\n",
    "}\n",
    "\n",
    "# add static columns (need reshapeÂ (N,1))\n",
    "for i, (base, _) in enumerate(static_cardinalities):\n",
    "    train_inputs[f'{base}_in'] = X_stat_train[:, i].reshape(-1,1)\n",
    "    val_inputs  [f'{base}_in'] = X_stat_val[:,   i].reshape(-1,1)\n",
    "\n",
    "# Sanity check: print expected vs actual - make sure ValueError: Functional() don't pop up\n",
    "print(\"\\nâ”€â”€ EXPECTED  vs  ACTUAL perâ€‘sample shapes â”€â”€\")\n",
    "tmp_model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "for inp in tmp_model.inputs:\n",
    "    k   = inp.name.split(':')[0]\n",
    "    exp = tuple(inp.shape[1:])\n",
    "    act = tuple(train_inputs[k].shape[1:])\n",
    "    ok  = \"âœ…\" if exp == act else \"âŒ\"\n",
    "    print(f\"{k:25s} expected={str(exp):12s} actual={str(act):12s} {ok}\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")\n",
    "\n",
    "# Instantiate final model & train\n",
    "model = build_sales_lstm(W, F, time_cardinalities, static_cardinalities)\n",
    "es   = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=12,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, rlrp],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.plot(history.history['loss'],     label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8.  Evaluate on a holdâ€‘out test set and generate forecasts\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "# Build a fresh 70/15/15 split Train/Val/Test (timeâ€‘aware per store) \n",
    "def time_aware_split_by_store(df, window, time_numeric_cols, static_cols):\n",
    "    X_num_tr, X_num_va, X_num_te = [], [], []\n",
    "    X_name_tr, X_name_va, X_name_te = [], [], []\n",
    "    X_day_tr,  X_day_va,  X_day_te  = [], [], []\n",
    "    X_month_tr,X_month_va,X_month_te= [], [], []\n",
    "    X_stat_tr, X_stat_va, X_stat_te = [], [], []\n",
    "    y_tr,      y_va,      y_te      = [], [], []\n",
    "\n",
    "    for _, grp in df.groupby('Store_No'):\n",
    "        grp = grp.sort_values('Date')\n",
    "        T = len(grp)\n",
    "        n_windows = T - window\n",
    "        if n_windows <= 0: continue\n",
    "\n",
    "        n_train = int(0.70 * n_windows)\n",
    "        n_val   = int(0.15 * n_windows)\n",
    "        # remainder â†’ test\n",
    "        n_test  = n_windows - n_train - n_val\n",
    "\n",
    "        arr_num    = grp[time_numeric_cols].values\n",
    "        arr_name   = grp['Name_enc' ].values\n",
    "        arr_day    = grp['Day_enc'  ].values\n",
    "        arr_month  = grp['Month_enc'].values\n",
    "        arr_static = grp[static_cols].iloc[window:].values\n",
    "        arr_target = grp[['Net_Amount','TC']].values[window:]\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            seq_num   = arr_num[i:i+window]\n",
    "            seq_name  = arr_name[i:i+window]\n",
    "            seq_day   = arr_day[i:i+window]\n",
    "            seq_month = arr_month[i:i+window]\n",
    "            stat_vec  = arr_static[i]\n",
    "            target    = arr_target[i]\n",
    "\n",
    "            # buckets\n",
    "            if i < n_train:\n",
    "                bucket = (X_num_tr,  X_name_tr,  X_day_tr,  X_month_tr,  X_stat_tr,  y_tr)\n",
    "            elif i < n_train + n_val:\n",
    "                bucket = (X_num_va,  X_name_va,  X_day_va,  X_month_va,  X_stat_va,  y_va)\n",
    "            else:\n",
    "                bucket = (X_num_te,  X_name_te,  X_day_te,  X_month_te,  X_stat_te,  y_te)\n",
    "\n",
    "            bucket[0].append(seq_num)\n",
    "            bucket[1].append(seq_name)\n",
    "            bucket[2].append(seq_day)\n",
    "            bucket[3].append(seq_month)\n",
    "            bucket[4].append(stat_vec)\n",
    "            bucket[5].append(target)\n",
    "\n",
    "    # stack to np arrays\n",
    "    def _stack(lst): return np.stack(lst) if lst and isinstance(lst[0], np.ndarray) else np.array(lst)\n",
    "    return tuple(map(_stack,\n",
    "        [X_num_tr,X_num_va,X_num_te,\n",
    "         X_name_tr,X_name_va,X_name_te,\n",
    "         X_day_tr,X_day_va,X_day_te,\n",
    "         X_month_tr,X_month_va,X_month_te,\n",
    "         X_stat_tr,X_stat_va,X_stat_te,\n",
    "         y_tr,y_va,y_te]))\n",
    "\n",
    "# build split\n",
    "(split_X_num_tr, split_X_num_va, split_X_num_te,\n",
    " split_X_name_tr, split_X_name_va, split_X_name_te,\n",
    " split_X_day_tr,  split_X_day_va,  split_X_day_te,\n",
    " split_X_month_tr,split_X_month_va,split_X_month_te,\n",
    " split_X_stat_tr, split_X_stat_va, split_X_stat_te,\n",
    " split_y_tr,      split_y_va,      split_y_te) = time_aware_split_by_store(\n",
    "    df, window, time_numeric_cols, static_cols\n",
    ")\n",
    "\n",
    "# Assemble TEST input dict \n",
    "def build_input_dict(X_num,X_name,X_day,X_month,X_stat):\n",
    "    d = {\n",
    "        'num_in':       X_num.astype(np.float32),\n",
    "        'name_seq_in':  X_name.astype(np.int32),\n",
    "        'day_seq_in':   X_day.astype(np.int32),\n",
    "        'month_seq_in': X_month.astype(np.int32)\n",
    "    }\n",
    "    for i,(base,_) in enumerate(static_cardinalities):\n",
    "        d[f'{base}_in'] = X_stat[:,i].reshape(-1,1).astype(np.int32)\n",
    "    return d\n",
    "\n",
    "test_inputs = build_input_dict(\n",
    "    split_X_num_te, split_X_name_te, split_X_day_te,\n",
    "    split_X_month_te, split_X_stat_te)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(test_inputs, verbose=0)\n",
    "\n",
    "# Inverseâ€‘scale Net_Amount & TC\n",
    "# remember scaler was fit on numeric_cols, where Net_Amount is col 0 and TC is col 1\n",
    "# weâ€™ll invert only those two columns\n",
    "net_idx = numeric_cols.index('Net_Amount')\n",
    "tc_idx  = numeric_cols.index('TC')\n",
    "\n",
    "def inverse_scale(scaled_vec, col_idx):\n",
    "    tmp      = np.zeros((len(scaled_vec), len(numeric_cols)))\n",
    "    tmp[:,col_idx] = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:,col_idx]\n",
    "\n",
    "net_pred = inverse_scale(y_pred_scaled[:,0], net_idx)\n",
    "tc_pred  = inverse_scale(y_pred_scaled[:,1], tc_idx)\n",
    "\n",
    "net_true = inverse_scale(split_y_te[:,0], net_idx)\n",
    "tc_true  = inverse_scale(split_y_te[:,1], tc_idx)\n",
    "\n",
    "# Metrics & Interpretation\n",
    "def interpret(true, pred, name):\n",
    "    # core metrics\n",
    "    mae  = mean_absolute_error(true, pred)\n",
    "    rmse = math.sqrt(mean_squared_error(true, pred))\n",
    "    r2   = r2_score(true, pred)\n",
    "\n",
    "    # reference stats\n",
    "    mean_y = np.mean(true)\n",
    "    std_y  = np.std(true)\n",
    "\n",
    "    # relative ratios\n",
    "    mae_ratio  = mae  / mean_y  if mean_y else float('nan')\n",
    "    rmse_ratio = rmse / std_y   if std_y  else float('nan')\n",
    "\n",
    "    # header\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\" MAE : {mae:8.2f}   (mean = {mean_y:8.2f}, MAE/mean = {mae_ratio:.2f})\")\n",
    "    print(f\" RMSE: {rmse:8.2f}   (std  = {std_y:8.2f}, RMSE/std  = {rmse_ratio:.2f})\")\n",
    "    print(f\" RÂ²  : {r2:6.3f}\")\n",
    "\n",
    "    # interpret MAEâ€‘toâ€‘mean\n",
    "    if mae_ratio < 0.10:\n",
    "        print(\"   ğŸ”µ Excellent MAE (<10% of mean)\")\n",
    "    elif mae_ratio < 0.20:\n",
    "        print(\"   ğŸŸ¢ Good MAE (<20% of mean)\")\n",
    "    elif mae_ratio < 0.30:\n",
    "        print(\"   ğŸŸ¡ Acceptable MAE (<30% of mean)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Poor MAE (>30% of mean)\")\n",
    "\n",
    "    # interpret RMSEâ€‘toâ€‘std\n",
    "    if rmse_ratio < 0.50:\n",
    "        print(\"   ğŸ”µ Excellent RMSE (<0.5â€¯Ïƒ)\")\n",
    "    elif rmse_ratio < 0.75:\n",
    "        print(\"   ğŸŸ¢ Good RMSE (<0.75â€¯Ïƒ)\")\n",
    "    elif rmse_ratio < 1.00:\n",
    "        print(\"   ğŸŸ¡ Acceptable RMSE (<1.0â€¯Ïƒ)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Poor RMSE (>1.0â€¯Ïƒ)\")\n",
    "\n",
    "    # interpret RÂ²\n",
    "    if r2 >= 0.90:\n",
    "        print(\"   ğŸ”µ Excellent RÂ² (â‰¥0.90)\")\n",
    "    elif r2 >= 0.75:\n",
    "        print(\"   ğŸŸ¢ Good RÂ² (0.75â€“0.90)\")\n",
    "    elif r2 >= 0.50:\n",
    "        print(\"   ğŸŸ¡ Acceptable RÂ² (0.50â€“0.75)\")\n",
    "    else:\n",
    "        print(\"   ğŸ”´ Weak RÂ² (<0.50)\")\n",
    "\n",
    "# run it for both targets\n",
    "interpret(net_true, net_pred, \"Net_Amount\")\n",
    "interpret(tc_true,  tc_pred,  \"TC\")\n",
    "\n",
    "# Visual check for Net_Amount\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(net_true, net_pred, alpha=.4)\n",
    "ax[0].plot([net_true.min(), net_true.max()],\n",
    "           [net_true.min(), net_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True Net_Amount\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"Net_Amount â€“ scatter\")\n",
    "\n",
    "ax[1].plot(net_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(net_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"Net_Amount â€“ first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Visual check for TC\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "ax[0].scatter(tc_true, tc_pred, alpha=.4)\n",
    "ax[0].plot([tc_true.min(), tc_true.max()],\n",
    "           [tc_true.min(), tc_true.max()], 'k--')\n",
    "ax[0].set_xlabel(\"True TC\"); ax[0].set_ylabel(\"Predicted\")\n",
    "ax[0].set_title(\"TC â€“ scatter\")\n",
    "\n",
    "ax[1].plot(tc_true[:200], label='True')    # first 200 horizon windows\n",
    "ax[1].plot(tc_pred[:200], label='Pred', alpha=.7)\n",
    "ax[1].set_title(\"TC â€“ first 200 forecast windows\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Helper: inverse-scale a single column (we already defined earlier)\n",
    "def inverse_scale(scaled_vec, col_idx):\n",
    "    tmp              = np.zeros((len(scaled_vec), len(numeric_cols)))\n",
    "    tmp[:, col_idx]  = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:, col_idx]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Baseline error on Net_Amount (choose whichever target you care about)\n",
    "base_pred = model.predict(test_inputs, verbose=0)[:, 0]      # column 0 = Net_Amount\n",
    "base_pred = inverse_scale(base_pred, numeric_cols.index('Net_Amount'))\n",
    "base_true = inverse_scale(split_y_te[:, 0], numeric_cols.index('Net_Amount'))\n",
    "base_mse  = mean_squared_error(base_true, base_pred)\n",
    "\n",
    "print(f\"Baseline RMSE = {np.sqrt(base_mse):.2f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Define groups of columns that belong to one logical feature\n",
    "# build a dict: { \"display name\": list[ key(s) in test_inputs ] }\n",
    "groups = OrderedDict({\n",
    "    # time-series numeric (whole sequence counts as one logical feature)\n",
    "    \"Net_Amount_lags\" : [\"num_in\"],   # but we'll permute only the Net column inside num_in\n",
    "    \"TC_lags\"         : [\"num_in\"],\n",
    "    \"Days_after_Opening\": [\"num_in\"],\n",
    "    \"Avg_Temp_lags\"   : [\"num_in\"],\n",
    "    \"Rain?\"           : [\"num_in\"],\n",
    "    \"Puasa\"           : [\"num_in\"],\n",
    "    \"Public_Holiday\"  : [\"num_in\"],\n",
    "\n",
    "    # time-categorical\n",
    "    \"Name_enc_lags\"   : [\"name_seq_in\"],\n",
    "    \"Day_enc_lags\"    : [\"day_seq_in\"],\n",
    "    \"Month_enc_lags\"  : [\"month_seq_in\"],\n",
    "\n",
    "    # static categoricals â€“ each has its own input tensor\n",
    "})\n",
    "for base, _ in static_cardinalities:\n",
    "    groups[base] = [f\"{base}_in\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Permutation importance loop\n",
    "importances = OrderedDict()\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for gname, keys in groups.items():\n",
    "    # make a deep copy of the original dict\n",
    "    perturbed = {k: v.copy() for k, v in test_inputs.items()}\n",
    "\n",
    "    for k in keys:\n",
    "        arr = perturbed[k]\n",
    "        # permute **independently for each timestep** if 3-D, else shuffle axis-0\n",
    "        if arr.ndim == 3:          # (N, window, features)\n",
    "            flat = arr.reshape(arr.shape[0], -1)\n",
    "            rng.shuffle(flat, axis=0)\n",
    "            perturbed[k] = flat.reshape(arr.shape)\n",
    "        else:                      # (N,1) or (N,window)\n",
    "            rng.shuffle(arr, axis=0)\n",
    "            perturbed[k] = arr\n",
    "\n",
    "    # predict with the corrupted feature(s)\n",
    "    y_perm = model.predict(perturbed, verbose=0)[:, 0]\n",
    "    y_perm = inverse_scale(y_perm, numeric_cols.index('Net_Amount'))\n",
    "    perm_mse = mean_squared_error(base_true, y_perm)\n",
    "\n",
    "    importances[gname] = np.sqrt(perm_mse) - np.sqrt(base_mse)  # Î” RMSE\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Plot\n",
    "names  = list(importances.keys())\n",
    "scores = np.array(list(importances.values()))\n",
    "order  = np.argsort(scores)[::-1]         # highest impact first\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(names)[order], scores[order])\n",
    "plt.xlabel(\"Increase in RMSE when feature is permuted\")\n",
    "plt.title(\"Permutation Feature Importance â€“ Net_Amount\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6b. Permutation importance for TC (column 1)\n",
    "# ------------------------------------------------------------\n",
    "# 6b.1  Baseline TC predictions & RMSE\n",
    "base_pred_tc = model.predict(test_inputs, verbose=0)[:, 1]  # pick the TC column\n",
    "base_pred_tc = inverse_scale(base_pred_tc, numeric_cols.index('TC'))\n",
    "base_true_tc = inverse_scale(split_y_te[:, 1], numeric_cols.index('TC'))\n",
    "base_mse_tc  = mean_squared_error(base_true_tc, base_pred_tc)\n",
    "print(f\"Baseline TC RMSE = {np.sqrt(base_mse_tc):.2f}\")\n",
    "\n",
    "# 6b.2  Same groups dictionary from before\n",
    "#         (we assume 'groups' is still in scope from your Net_Amount run)\n",
    "\n",
    "importances_tc = OrderedDict()\n",
    "for gname, keys in groups.items():\n",
    "    perturbed = {k: v.copy() for k, v in test_inputs.items()}\n",
    "    for k in keys:\n",
    "        arr = perturbed[k]\n",
    "        if arr.ndim == 3:\n",
    "            flat = arr.reshape(arr.shape[0], -1)\n",
    "            rng.shuffle(flat, axis=0)\n",
    "            perturbed[k] = flat.reshape(arr.shape)\n",
    "        else:\n",
    "            rng.shuffle(arr, axis=0)\n",
    "            perturbed[k] = arr\n",
    "\n",
    "    # predict on permuted data, then inverse-scale TC\n",
    "    y_p_tc = model.predict(perturbed, verbose=0)[:, 1]\n",
    "    y_p_tc = inverse_scale(y_p_tc, numeric_cols.index('TC'))\n",
    "    imp_mse = mean_squared_error(base_true_tc, y_p_tc)\n",
    "\n",
    "    importances_tc[gname] = np.sqrt(imp_mse) - np.sqrt(base_mse_tc)\n",
    "\n",
    "# 6b.3  Plot TC importances\n",
    "names_tc  = list(importances_tc.keys())\n",
    "scores_tc = np.array(list(importances_tc.values()))\n",
    "order_tc  = np.argsort(scores_tc)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(names_tc)[order_tc], scores_tc[order_tc])\n",
    "plt.xlabel(\"Increase in TC-RMSE when feature is permuted\")\n",
    "plt.title(\"Permutation Feature Importance â€“ TC\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f741cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9.  200-step autoregressive forecast \n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 9.0 Ensure your df has full-day and month columns -----------------------\n",
    "df['Date']  = pd.to_datetime(df['Date'])\n",
    "df['Day']   = df['Date'].dt.day_name()            # 'Monday', 'Tuesday', â€¦\n",
    "df['Month'] = df['Date'].dt.month.astype(str)     # '1' â€¦ '12'\n",
    "\n",
    "# --- 9.0b Fit (or re-fit) your Day/Month encoders ---------------------------\n",
    "encoders['Day'] = LabelEncoder().fit([\n",
    "    'Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'\n",
    "])\n",
    "encoders['Month'] = LabelEncoder().fit([str(i) for i in range(1,13)])\n",
    "\n",
    "# (Re-)attach the encoded columns to df\n",
    "df['Day_enc']   = encoders['Day'].transform(df['Day'])\n",
    "df['Month_enc'] = encoders['Month'].transform(df['Month'])\n",
    "\n",
    "# --- 9.1 Prep the last window -----------------------------------------------\n",
    "future_horizon = 200\n",
    "last_date      = df['Date'].max()\n",
    "\n",
    "tail_df   = df.sort_values('Date').tail(window).copy()\n",
    "seq_num   = tail_df[time_numeric_cols].values.astype(np.float32)\n",
    "seq_name  = tail_df['Name_enc'].values.astype(np.int32)\n",
    "seq_day   = tail_df['Day_enc'].values.astype(np.int32)\n",
    "seq_month = tail_df['Month_enc'].values.astype(np.int32)\n",
    "\n",
    "static_vec = tail_df.iloc[-1][static_cols].values.astype(np.int32)\n",
    "\n",
    "dates_f, net_f, tc_f = [], [], []\n",
    "\n",
    "# --- 9.2 Autoregressive loop -----------------------------------------------\n",
    "for step in range(future_horizon):\n",
    "    # build one-step input dict\n",
    "    sample_inputs = {\n",
    "        'num_in':       seq_num[np.newaxis, ...],\n",
    "        'name_seq_in':  seq_name[np.newaxis, ...],\n",
    "        'day_seq_in':   seq_day[np.newaxis, ...],\n",
    "        'month_seq_in': seq_month[np.newaxis, ...],\n",
    "    }\n",
    "    for i, (base, _) in enumerate(static_cardinalities):\n",
    "        sample_inputs[f'{base}_in'] = static_vec[[i]].reshape(1,1)\n",
    "\n",
    "    # predict & inverse-scale\n",
    "    yhat_s = model.predict(sample_inputs, verbose=0)[0]\n",
    "    net_s, tc_s = yhat_s\n",
    "    net_v = inverse_scale([net_s], numeric_cols.index('Net_Amount'))[0]\n",
    "    tc_v  = inverse_scale([tc_s],  numeric_cols.index('TC'))[0]\n",
    "\n",
    "    # record the date + values\n",
    "    curr_date = last_date + pd.Timedelta(days=step+1)\n",
    "    dates_f.append(curr_date)\n",
    "    net_f.append(net_v)\n",
    "    tc_f.append(tc_v)\n",
    "\n",
    "    # advance day & month encodings for the *next* step\n",
    "    next_day_str   = curr_date.day_name()    # full weekday name\n",
    "    next_month_str = str(curr_date.month)    # '1'â€“'12'\n",
    "\n",
    "    seq_day = np.roll(seq_day, -1)\n",
    "    seq_day[-1] = encoders['Day'].transform([next_day_str])[0]\n",
    "\n",
    "    seq_month = np.roll(seq_month, -1)\n",
    "    seq_month[-1] = encoders['Month'].transform([next_month_str])[0]\n",
    "\n",
    "    # roll the numeric window\n",
    "    new_row = seq_num[-1].copy()\n",
    "    new_row[numeric_cols.index('Net_Amount')] = net_s\n",
    "    new_row[numeric_cols.index('TC')]         = tc_s\n",
    "    seq_num = np.vstack([seq_num[1:], new_row])\n",
    "\n",
    "# --- 9.3 Build forecast DF & plot ------------------------------------------\n",
    "df_forecast = pd.DataFrame({\n",
    "    'Date':       dates_f,\n",
    "    'Net_Amount': net_f,\n",
    "    'TC':         tc_f,\n",
    "})\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True,\n",
    "    subplot_titles=('Net_Amount â€“ 200-step Forecast', 'TC â€“ 200-step Forecast'),\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "# Net_Amount trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_forecast['Date'],\n",
    "        y=df_forecast['Net_Amount'],\n",
    "        mode='lines+markers',\n",
    "        name='Net Amount',\n",
    "        hovertemplate=\n",
    "            '<b>Date</b>: %{x}<br>'+\n",
    "            '<b>Net Amount</b>: %{y:.2f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# TC trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_forecast['Date'],\n",
    "        y=df_forecast['TC'],\n",
    "        mode='lines+markers',\n",
    "        name='TC',\n",
    "        hovertemplate=\n",
    "            '<b>Date</b>: %{x}<br>'+\n",
    "            '<b>Transactions</b>: %{y:.0f}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    height=600, width=900,\n",
    "    hovermode='x unified',           # vertical line at cursor\n",
    "    title_text='200-Step Autoregressive Forecast',\n",
    "    margin=dict(t=80, b=40)\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "fig.update_xaxes(title_text='Date', row=2, col=1)\n",
    "fig.update_yaxes(title_text='RM',           row=1, col=1)\n",
    "fig.update_yaxes(title_text='Transactions', row=2, col=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd56c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0.  DEFINE scale_cols & new inverse_scale\n",
    "# ---------------------------------------------------------------------------\n",
    "scale_cols = ['Net_Amount', 'TC', \n",
    "              'Days_after_Opening', 'Average Daily Temperature (Â°C)']\n",
    "\n",
    "def inverse_scale(scaled_vec, col_name):\n",
    "    \"\"\"\n",
    "    scaled_vec: list or array of length N\n",
    "    col_name : one of scale_cols\n",
    "    \"\"\"\n",
    "    # how many features our scaler was actually fit on?\n",
    "    n_feats = scaler.min_.shape[0]           # e.g. 4\n",
    "    tmp     = np.zeros((len(scaled_vec), n_feats))\n",
    "    idx     = scale_cols.index(col_name)\n",
    "    tmp[:, idx] = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:, idx]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Build actuals DF (blue line)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_actual = (\n",
    "    df[['Date','Store_No','Net_Amount','TC']].copy()\n",
    "      .assign(Date=lambda d: pd.to_datetime(d['Date']))\n",
    "      .query(\"Date >= '2025-06-17'\")\n",
    "      .sort_values(['Store_No','Date'])\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Forecast each store for 21 days (orange line)\n",
    "# ---------------------------------------------------------------------------\n",
    "all_forecasts = []\n",
    "future_horizon = 21\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "for store_id in df['Store_No'].unique():\n",
    "    one = df[df['Store_No']==store_id].sort_values('Date')\n",
    "    if len(one) < window:\n",
    "        continue\n",
    "\n",
    "    tail_df   = one.tail(window).copy()\n",
    "    last_date = tail_df['Date'].max()\n",
    "\n",
    "    # timeâ€series features (7 dims) â†’ seq_num\n",
    "    seq_num   = tail_df[time_numeric_cols].values.astype(np.float32)\n",
    "    seq_name  = tail_df['Name_enc'].values.astype(np.int32)\n",
    "    seq_day   = tail_df['Day_enc'].values.astype(np.int32)\n",
    "    seq_month = tail_df['Month_enc'].values.astype(np.int32)\n",
    "    static_vec= tail_df.iloc[-1][static_cols].values.astype(np.int32)\n",
    "\n",
    "    dates_f, net_f, tc_f = [], [], []\n",
    "\n",
    "    for step in range(future_horizon):\n",
    "        # build input dict\n",
    "        inp = {\n",
    "            'num_in'      : seq_num[np.newaxis],\n",
    "            'name_seq_in' : seq_name[np.newaxis],\n",
    "            'day_seq_in'  : seq_day[np.newaxis],\n",
    "            'month_seq_in': seq_month[np.newaxis],\n",
    "        }\n",
    "        for i,(base,_) in enumerate(static_cardinalities):\n",
    "            inp[f'{base}_in'] = static_vec[[i]].reshape(1,1)\n",
    "\n",
    "        # predict (scaled) & inverseâ€scale using the NEW function\n",
    "        y_s       = model.predict(inp, verbose=0)[0]\n",
    "        net_s, tc_s = y_s\n",
    "        net_v = inverse_scale([net_s], 'Net_Amount')[0]\n",
    "        tc_v  = inverse_scale([tc_s],  'TC'        )[0]\n",
    "\n",
    "        # record date + trueâ€scale values\n",
    "        curr_date = last_date + pd.Timedelta(days=step+1)\n",
    "        dates_f.append(curr_date)\n",
    "        net_f.append(net_v)\n",
    "        tc_f .append(tc_v)\n",
    "\n",
    "        # roll day/month encoding\n",
    "        day_str   = curr_date.day_name()\n",
    "        month_str = str(curr_date.month)\n",
    "        seq_day   = np.roll(seq_day,   -1)\n",
    "        seq_day[-1]   = encoders['Day'  ].transform([day_str])[0]\n",
    "        seq_month = np.roll(seq_month, -1)\n",
    "        seq_month[-1] = encoders['Month'].transform([month_str])[0]\n",
    "\n",
    "        # roll the numeric window (uses time_numeric_cols indexes)\n",
    "        new_row = seq_num[-1].copy()\n",
    "        j0 = time_numeric_cols.index('Net_Amount')\n",
    "        j1 = time_numeric_cols.index('TC')\n",
    "        new_row[j0] = net_s\n",
    "        new_row[j1] = tc_s\n",
    "        seq_num = np.vstack([seq_num[1:], new_row])\n",
    "\n",
    "    # assemble DF for this store\n",
    "    df_fc = pd.DataFrame({\n",
    "        'Date'      : dates_f,\n",
    "        'Net_Amount': net_f,\n",
    "        'TC'        : tc_f,\n",
    "        'Store_No'  : store_id\n",
    "    })\n",
    "    all_forecasts.append(df_fc)\n",
    "\n",
    "df_all = pd.concat(all_forecasts, ignore_index=True)\n",
    "\n",
    "# out = widgets.interactive_output(make_fig, {'store_id': store_widget})\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "df_actual = pd.read_csv(r\" ... csv\")\n",
    "df_actual['Date'] = pd.to_datetime(df_actual['Date'])\n",
    "\n",
    "# Step 2: Rename columns to match forecast dataframe\n",
    "df_actual = df_actual.rename(columns={\n",
    "    'Store No'   : 'Store_No',\n",
    "    'Net Amount' : 'Net_Amount'\n",
    "})\n",
    "\n",
    "# Step 3: Match dtype for merging or plotting later (optional)\n",
    "df_actual['Store_No'] = df_actual['Store_No'].astype(df_all['Store_No'].dtype)\n",
    "\n",
    "# Step 4: Negate if necessary\n",
    "df_actual['Net_Amount'] = df_actual['Net_Amount'] * -1\n",
    "\n",
    "# Step 5: Sort by date\n",
    "df_actual = df_actual.sort_values(['Store_No', 'Date'])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Interactive figure:  \"True\"  (blue)  vs  \"Predicted\"  (orange)\n",
    "# ------------------------------------------------------------------\n",
    "def make_fig(store_id):\n",
    "    act = df_actual.query(\"Store_No == @store_id\").copy()\n",
    "    fc  = df_all   .query(\"Store_No == @store_id\").copy()\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1, shared_xaxes=True,\n",
    "        subplot_titles=(\"Net_Amount\", \"TC\"),\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "\n",
    "    # -------- Net_Amount --------\n",
    "    # Predicted Net (orange) â€” leave as-is\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fc['Date'], y=fc['Net_Amount'],\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Net',\n",
    "            line=dict(color='orange'),\n",
    "            legendgroup='pred',\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # True Net (now with markers)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=act['Date'], y=act['Net_Amount'],\n",
    "            mode='lines+markers',              # <<â€” add +markers\n",
    "            name='True Net',\n",
    "            line=dict(color='blue'),\n",
    "            marker=dict(color='blue'),         # ensure markers are blue\n",
    "            legendgroup='true',\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # -------- TC --------\n",
    "    # Predicted TC (orange) â€” leave as-is\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fc['Date'], y=fc['TC'],\n",
    "            mode='lines+markers',\n",
    "            name='Predicted TC',\n",
    "            line=dict(color='orange'),\n",
    "            legendgroup='pred',\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    # True TC (now with markers)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=act['Date'], y=act['TC'],\n",
    "            mode='lines+markers',              # <<â€” add +markers\n",
    "            name='True TC',\n",
    "            line=dict(color='blue'),\n",
    "            marker=dict(color='blue'),\n",
    "            legendgroup='true',\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "        # ---- cosmetics ----\n",
    "    fig.update_layout(\n",
    "            height=600, width=900,\n",
    "            hovermode='x unified',\n",
    "            title=f\"Store {store_id}: True vs Predicted (21-Day Horizon)\",\n",
    "            legend_title_text=''          # cleaner legend\n",
    "        )\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"RM\",           row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Transactions\", row=2, col=1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Widget â€“ same as before\n",
    "# ------------------------------------------------------------------\n",
    "store_widget = widgets.Dropdown(\n",
    "    options=sorted(df_all['Store_No'].unique()),\n",
    "    description='Select Store_No:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "out = widgets.interactive_output(make_fig, {'store_id': store_widget})\n",
    "display(store_widget, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0.  DEFINE scale_cols & new inverse_scale\n",
    "# ---------------------------------------------------------------------------\n",
    "scale_cols = ['Net_Amount', 'TC', \n",
    "              'Days_after_Opening', 'Average Daily Temperature (Â°C)']\n",
    "\n",
    "def inverse_scale(scaled_vec, col_name):\n",
    "    \"\"\"\n",
    "    scaled_vec: list or array of length N\n",
    "    col_name : one of scale_cols\n",
    "    \"\"\"\n",
    "    # how many features our scaler was actually fit on?\n",
    "    n_feats = scaler.min_.shape[0]           # e.g. 4\n",
    "    tmp     = np.zeros((len(scaled_vec), n_feats))\n",
    "    idx     = scale_cols.index(col_name)\n",
    "    tmp[:, idx] = scaled_vec\n",
    "    return scaler.inverse_transform(tmp)[:, idx]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Build actuals DF (blue line)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_actual = (\n",
    "    df[['Date','Store_No','Net_Amount','TC']].copy()\n",
    "      .assign(Date=lambda d: pd.to_datetime(d['Date']))\n",
    "      .query(\"Date >= '2025-06-17'\")\n",
    "      .sort_values(['Store_No','Date'])\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Forecast each store for 21 days (orange line)\n",
    "# ---------------------------------------------------------------------------\n",
    "all_forecasts = []\n",
    "future_horizon = 21\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "for store_id in df['Store_No'].unique():\n",
    "    one = df[df['Store_No']==store_id].sort_values('Date')\n",
    "    if len(one) < window:\n",
    "        continue\n",
    "\n",
    "    tail_df   = one.tail(window).copy()\n",
    "    last_date = tail_df['Date'].max()\n",
    "\n",
    "    # timeâ€series features (7 dims) â†’ seq_num\n",
    "    seq_num   = tail_df[time_numeric_cols].values.astype(np.float32)\n",
    "    seq_name  = tail_df['Name_enc'].values.astype(np.int32)\n",
    "    seq_day   = tail_df['Day_enc'].values.astype(np.int32)\n",
    "    seq_month = tail_df['Month_enc'].values.astype(np.int32)\n",
    "    static_vec= tail_df.iloc[-1][static_cols].values.astype(np.int32)\n",
    "\n",
    "    dates_f, net_f, tc_f = [], [], []\n",
    "\n",
    "    for step in range(future_horizon):\n",
    "        # build input dict\n",
    "        inp = {\n",
    "            'num_in'      : seq_num[np.newaxis],\n",
    "            'name_seq_in' : seq_name[np.newaxis],\n",
    "            'day_seq_in'  : seq_day[np.newaxis],\n",
    "            'month_seq_in': seq_month[np.newaxis],\n",
    "        }\n",
    "        for i,(base,_) in enumerate(static_cardinalities):\n",
    "            inp[f'{base}_in'] = static_vec[[i]].reshape(1,1)\n",
    "\n",
    "        # predict (scaled) & inverseâ€scale using the NEW function\n",
    "        y_s       = model.predict(inp, verbose=0)[0]\n",
    "        net_s, tc_s = y_s\n",
    "        net_v = inverse_scale([net_s], 'Net_Amount')[0]\n",
    "        tc_v  = inverse_scale([tc_s],  'TC'        )[0]\n",
    "\n",
    "        # record date + trueâ€scale values\n",
    "        curr_date = last_date + pd.Timedelta(days=step+1)\n",
    "        dates_f.append(curr_date)\n",
    "        net_f.append(net_v)\n",
    "        tc_f .append(tc_v)\n",
    "\n",
    "        # roll day/month encoding\n",
    "        day_str   = curr_date.day_name()\n",
    "        month_str = str(curr_date.month)\n",
    "        seq_day   = np.roll(seq_day,   -1)\n",
    "        seq_day[-1]   = encoders['Day'  ].transform([day_str])[0]\n",
    "        seq_month = np.roll(seq_month, -1)\n",
    "        seq_month[-1] = encoders['Month'].transform([month_str])[0]\n",
    "\n",
    "        # roll the numeric window (uses time_numeric_cols indexes)\n",
    "        new_row = seq_num[-1].copy()\n",
    "        j0 = time_numeric_cols.index('Net_Amount')\n",
    "        j1 = time_numeric_cols.index('TC')\n",
    "        new_row[j0] = net_s\n",
    "        new_row[j1] = tc_s\n",
    "        seq_num = np.vstack([seq_num[1:], new_row])\n",
    "\n",
    "    # assemble DF for this store\n",
    "    df_fc = pd.DataFrame({\n",
    "        'Date'      : dates_f,\n",
    "        'Net_Amount': net_f,\n",
    "        'TC'        : tc_f,\n",
    "        'Store_No'  : store_id\n",
    "    })\n",
    "    all_forecasts.append(df_fc)\n",
    "\n",
    "df_all = pd.concat(all_forecasts, ignore_index=True)\n",
    "\n",
    "out = widgets.interactive_output(make_fig, {'store_id': store_widget})\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "df_actual = pd.read_csv(r\" ... csv\")\n",
    "df_actual['Date'] = pd.to_datetime(df_actual['Date'])\n",
    "\n",
    "# Step 2: Rename columns to match forecast dataframe\n",
    "df_actual = df_actual.rename(columns={\n",
    "    'Store No'   : 'Store_No',\n",
    "    'Net Amount' : 'Net_Amount'\n",
    "})\n",
    "\n",
    "# Step 3: Match dtype for merging or plotting later (optional)\n",
    "df_actual['Store_No'] = df_actual['Store_No'].astype(df_all['Store_No'].dtype)\n",
    "\n",
    "# Step 4: Negate if necessary\n",
    "df_actual['Net_Amount'] = df_actual['Net_Amount'] * -1\n",
    "\n",
    "# Step 5: Sort by date\n",
    "df_actual = df_actual.sort_values(['Store_No', 'Date'])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) Merge your actuals & predictions over the holdâ€out so you can compute residuals\n",
    "df_merged = (\n",
    "    df_actual\n",
    "      .rename(columns={\"Net_Amount\":\"Net_Amount_act\",\"TC\":\"TC_act\"})\n",
    "      .merge(\n",
    "         df_all.rename(columns={\"Net_Amount\":\"Net_Amount_pred\",\"TC\":\"TC_pred\"}),\n",
    "         on=[\"Store_No\",\"Date\"],\n",
    "         how=\"inner\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# 2) Compute residual stdâ€dev across all stores & days\n",
    "sigma_net = np.std(df_merged[\"Net_Amount_act\"] - df_merged[\"Net_Amount_pred\"], ddof=1)\n",
    "sigma_tc  = np.std(df_merged[\"TC_act\"]         - df_merged[\"TC_pred\"],        ddof=1)\n",
    "\n",
    "# 3) 95% halfâ€widths\n",
    "ci_half_net = 1.96 * sigma_net\n",
    "ci_half_tc  = 1.96 * sigma_tc\n",
    "\n",
    "print(f\"Global 95% CI half-widths:  Net={ci_half_net:.0f},  TC={ci_half_tc:.1f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Interactive figure:  \"True\"  (blue)  vs  \"Predicted\"  (orange)\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np   # add at top of your file\n",
    "\n",
    "def make_fig(store_id):\n",
    "    # just grab your preds & actuals for the display\n",
    "    act = df_actual.query(\"Store_No == @store_id\")\n",
    "    fc  = df_all   .query(\"Store_No == @store_id\")\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1, shared_xaxes=True,\n",
    "        subplot_titles=(\"Net_Amount\", \"TC\"),\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "\n",
    "    # --- Net_Amount with *constant* CI ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fc['Date'], y=fc['Net_Amount'],\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Net',\n",
    "            line=dict(color='orange'),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=[ci_half_net]*len(fc),  # constant half-width\n",
    "                visible=True,\n",
    "                thickness=1.5,\n",
    "                width=4,\n",
    "            )\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=act['Date'], y=act['Net_Amount'],\n",
    "            mode='lines+markers',\n",
    "            name='True Net',\n",
    "            line=dict(color='blue'),\n",
    "            marker=dict(color='blue'),\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # --- TC with *constant* CI ---\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fc['Date'], y=fc['TC'],\n",
    "            mode='lines+markers',\n",
    "            name='Predicted TC',\n",
    "            line=dict(color='orange'),\n",
    "            showlegend=False,\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=[ci_half_tc]*len(fc),\n",
    "                visible=True,\n",
    "                thickness=1.5,\n",
    "                width=4,\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=act['Date'], y=act['TC'],\n",
    "            mode='lines+markers',\n",
    "            name='True TC',\n",
    "            line=dict(color='blue'),\n",
    "            marker=dict(color='blue'),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # layout tweaks\n",
    "    fig.update_layout(\n",
    "        height=600, width=900,\n",
    "        hovermode='x unified',\n",
    "        title=f\"Store {store_id}: True vs Predicted (21-Day Horizon)\",\n",
    "        legend_title_text=''\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"RM\",           row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Transactions\", row=2, col=1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Widget â€“ same as before\n",
    "# ------------------------------------------------------------------\n",
    "store_widget = widgets.Dropdown(\n",
    "    options=sorted(df_all['Store_No'].unique()),\n",
    "    description='Select Store_No:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "out = widgets.interactive_output(make_fig, {'store_id': store_widget})\n",
    "display(store_widget, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
